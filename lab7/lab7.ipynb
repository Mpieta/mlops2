{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74c987e3-c3cf-42b5-b868-2e8287652b80",
   "metadata": {},
   "source": [
    "# Lab 7 - Model Optimization for Inference\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, we will focus on optimizing neural network models for faster inference. \n",
    "There are many techniques available in PyTorch, including: \n",
    "\n",
    "* switching the model to **evaluation** mode and disabling gradient computation\n",
    "* various strategies for GPU speedup, e.g. optimized tensor placement, pinning memory,\n",
    "  lower precision calculations\n",
    "* using the `torch.compile()` function for automatic model compilation\n",
    "* **model quantization** to reduce size and speed up computations\n",
    "* exporting the model to ONNX format and ONNX Runtime optimization\n",
    "\n",
    "These techniques allow you to **speed up** the inference and **reduce resource usage**,\n",
    "which are crucial when deploying ML models to production systems. They are particularly\n",
    "useful for low-latency applications (e.g. online services, streaming ML), as well as for\n",
    "mobile and edge deployments with limited resources.\n",
    "\n",
    "### Environment note\n",
    "\n",
    "We recommend using a local Python environment managed with `uv`. If you encounter problems\n",
    "or do not have a CUDA-compatible GPU (e.g. on macOS), you can use Google Colab. In that case,\n",
    "remember to enable the GPU accelerator in the runtime settings.\n",
    "\n",
    "In the following exercises, we will use a pretrained Sentence Transformer model,\n",
    "`sentence-transformers/multi-qa-mpnet-base-cos-v1`. It embeds sentences as 768-dimensional vectors.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d390ec-012b-4bfd-974d-60313fe92bc8",
   "metadata": {},
   "source": [
    "## 1. Evaluation mode\n",
    "\n",
    "When using PyTorch for inference, there are several optimizations that can be applied to reduce the overhead of the model.\n",
    "They include:\n",
    "\n",
    "1. Model evaluation (eval) mode - it disables layers used only during training (e.g. dropout, batch normalization).\n",
    "   Used with `model.eval()` method.\n",
    "2. Disabling gradients - during inference, gradients are not needed, so it omits tracking them and allocating memory\n",
    "   for them. Used with `torch.no_grad()` context manager, or preferably with a more recently added and more performant\n",
    "   `torch.inference_mode()`.\n",
    "3. Inference loop optimization - avoid unnecessary repetition of operations, e.g. move model to a device beforehand,\n",
    "   pre-allocate memory.\n",
    "\n",
    "For differences between `no_grad()` and `inference_mode()`, see:\n",
    "- [this StackOverflow answer](https://stackoverflow.com/a/74197846/9472066)\n",
    "- [PyTorch forum discussion](https://discuss.pytorch.org/t/pytorch-torch-no-grad-vs-torch-inference-mode/134099)\n",
    "- [PyTorch docs on grad modes](https://docs.pytorch.org/docs/stable/notes/autograd.html#grad-modes)\n",
    "\n",
    "### Exercise 1 (3 points)\n",
    "\n",
    "1. Load the `sentence-transformers/multi-qa-mpnet-base-cos-v1` model and tokenizer. Use the `AutoModel` and\n",
    "   `AutoTokenizer` classes from `tranformers` library.\n",
    "2. Create a sample input text and tokenize it (padding, truncation, `return_tensors=\"pt\"`).\n",
    "3. Measure the inference time of the model in various inference modes (average time over 100 runs):\n",
    "   - no optimizations (simple PyTorch)\n",
    "   - `model.eval()`\n",
    "   - `model.eval()` and `no_grad()`\n",
    "   - `model.eval()` and `inference_mode()`\n",
    "4. Compare the speedup of options 2, 3, and 4 over the pure PyTorch. To calculate speedup, divide the\n",
    "   PyTorch time by the current time. \n",
    "\n",
    "In general, the time should decrease for subsequent options. If `inference_mode()` is slower than `no_grad()`, \n",
    "it may be due some not supported operations in the model, so `no_grad()` is preferred in such cases. \n",
    "But when models contain many operations and overhead with autograd is significant, `inference_mode()` should be faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a0544a4e-ff8e-4f8d-8be3-987497675c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/multi-qa-mpnet-base-cos-v1\")\n",
    "model = model.to('cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/multi-qa-mpnet-base-cos-v1\")\n",
    "\n",
    "\n",
    "prompt = \"One two three four five six seven eight nine\"\n",
    "inputs = tokenizer(prompt, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "inputs = inputs.to('cuda')\n",
    "print(model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ae5ae3-0468-4f98-a1cf-a09aeb4cf435",
   "metadata": {},
   "source": [
    "- tokenization + transfer to gpu time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "644186d1-93fe-448f-a935-3691ea701c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284 μs ± 30 μs per loop (mean ± std. dev. of 100 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1000 -r 100 -v timeit_transfer\n",
    "prompt = \"One two three four five six seven eight nine\"\n",
    "inp1 = tokenizer(prompt, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "inp1 = inp1.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e066a2-ec5b-4a07-a54c-9c7e120cbecf",
   "metadata": {},
   "source": [
    "- no optimizations (simple PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7003c95a-e455-4ad1-b59f-366029e00b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "timeit_pytorch = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1164dcc1-afaf-4139-8876-5a56e9520e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.16 ms ± 274 μs per loop (mean ± std. dev. of 10 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100 -r 10 -v timeit_pytorch\n",
    "out1 = model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778d29fc-94d8-43ba-9728-65274d118beb",
   "metadata": {},
   "source": [
    "- `model.eval()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "95ce72e6-ed19-477b-9f32-fec972b35db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "timeit_eval = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a21d03b5-e2a4-441a-a052-b8394d09be24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.71 ms ± 180 μs per loop (mean ± std. dev. of 10 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100 -r 10 -v timeit_eval\n",
    "out1 = model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8d0169-8b4e-46b2-84f5-fb1acc7c37a8",
   "metadata": {},
   "source": [
    "- `model.eval()` and `no_grad()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0cfb962a-315f-4e7a-a318-3008d0c6fd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "timeit_eval_nograd = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fbd3f4dc-427f-4557-b7ce-d3151569573e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5 ms ± 244 μs per loop (mean ± std. dev. of 10 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100 -r 10 -v timeit_eval_nograd\n",
    "with torch.no_grad():\n",
    "    out1 = model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda5e975-cfd9-4982-8a82-0a87d2abc34a",
   "metadata": {},
   "source": [
    "- `model.eval()` and `inference_mode()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "92b7f271-4cc9-47db-a58a-c2f3feda1ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "timeit_eval_inference = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "691d5314-5df5-4389-b777-55c7ed0c4c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.54 ms ± 341 μs per loop (mean ± std. dev. of 10 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100 -r 10 -v timeit_eval_inference\n",
    "with torch.inference_mode():\n",
    "    out1 = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "20de711b-958b-42ba-a67a-129f42fdb675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval speedup relative to pytorch: 1.058\n",
      "eval and no_grad speedup relative to pytorch: 1.088\n",
      "eval and inference mode speedup relative to pytorch: 1.082\n"
     ]
    }
   ],
   "source": [
    "times = [timeit_pytorch, timeit_eval, timeit_eval_nograd, timeit_eval_inference]\n",
    "names = [\"pytorch\", \"eval\", \"eval and no_grad\", \"eval and inference mode\"]\n",
    "\n",
    "for time, name in zip(times[1:], names[1:]):\n",
    "    print(f\"{name} speedup relative to pytorch: {times[0].average / time.average:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ed49e6-662e-4af2-b5c2-4db9e3b20de5",
   "metadata": {},
   "source": [
    "## 2. PyTorch model compilation\n",
    "\n",
    "PyTorch 2.0 introduced a new functionality, model compilation, which automatically optimizes model execution\n",
    "via `torch.compile()` function.\n",
    "\n",
    "This mechanism uses modules such as **TorchDynamo** and **TorchInductor** under the hood to capture the model\n",
    "computation graph and generate optimized low-level code. The default backend (TorchInductor) can generate\n",
    "optimized CUDA kernels on GPU, and optimized vectorized code on CPU. It can also fuse operations together and\n",
    "bypass the overhead of memory transfers and Python interpreter.\n",
    "\n",
    "Note that `torch.compile()` is a lossless model optimization technique, changing only its physical execution.\n",
    "You should call it after setting the model to evaluation mode, so that the computation graph contains only the\n",
    "final inference operations.\n",
    "\n",
    "Example usage:\n",
    "\n",
    "```python\n",
    "model.eval()\n",
    "compiled_model = torch.compile(model)\n",
    "```\n",
    "\n",
    "The above line returns a compiled version of the model that can be used just like the original model.\n",
    "During the first inference call, the model execution operations are traced and its computation graph is optimized,\n",
    "which incurs an overhead, which can be quite significant. Further calls will use the generated optimized code,\n",
    "which should be significantly faster.\n",
    "\n",
    "### Exercise 2 (2 points)\n",
    "\n",
    "In this exercise, we will verify the gains from model compilation with `torch.compile()`.\n",
    "\n",
    "1. Compile the model using `torch.compile()` after switching it to evaluation mode, and warm-up the model\n",
    "   by running a single inference call. Measure this compilation + warm-up time (just once).\n",
    "2. Measure the inference time (average of 100 runs) of the compiled model in inference mode.\n",
    "3. Calculate the speedup, and compare results with those from the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83273860-745a-494f-86e6-5cd154934932",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit_compiled_warmup = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8f21162-b1f3-4368-8089-16f286f6740f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.29 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 1 -v timeit_compiled_warmup\n",
    "\n",
    "model.eval()\n",
    "model_compiled = torch.compile(model)\n",
    "_ = model_compiled(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e73c1c-9075-4cc7-a8f7-9cd452319058",
   "metadata": {},
   "source": [
    "- no optimizations (simple PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b645599-5b51-48e2-a457-beeeed1fda98",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model_compiled = torch.compile(model)\n",
    "_ = model_compiled(**inputs)\n",
    "\n",
    "model_compiled.train()\n",
    "timeit_compiled_pytorch = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e49266f1-9632-4896-934c-494bf19b4ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 8.62 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "11.4 ms ± 14.3 ms per loop (mean ± std. dev. of 10 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100 -r 10 -v timeit_compiled_pytorch\n",
    "out1 = model_compiled(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac26833b-7a24-4858-a785-9174e2ecd321",
   "metadata": {},
   "source": [
    "- `model.eval()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edd4a1d2-afc7-4392-a6b8-3cc79701c746",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_compiled.eval()\n",
    "timeit_compiled_eval = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e799eb5-6cff-45f4-b971-90003547c13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.38 ms ± 114 μs per loop (mean ± std. dev. of 10 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100 -r 10 -v timeit_compiled_eval\n",
    "out1 = model_compiled(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58689ab4-70de-44b8-ab1d-2b3abd0c4447",
   "metadata": {},
   "source": [
    "- `model.eval()` and `no_grad()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c8659c2-834e-4a26-aa65-54f215c32c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_compiled.eval()\n",
    "timeit_compiled_eval_nograd = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52f85d00-0a81-437e-826c-5a6ef84df582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.37 ms ± 2.81 ms per loop (mean ± std. dev. of 10 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100 -r 10 -v timeit_compiled_eval_nograd\n",
    "with torch.no_grad():\n",
    "    out1 = model_compiled(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d82fd6-f841-4240-8f60-604302596abf",
   "metadata": {},
   "source": [
    "- `model.eval()` and `inference_mode()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76f23777-058b-49b9-a19b-d6f7f91e81da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_compiled.eval()\n",
    "timeit_compiled_eval_inference = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec789e4a-0d16-4b60-9e0b-03c3cc91ea8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.27 ms ± 2.69 ms per loop (mean ± std. dev. of 10 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100 -r 10 -v timeit_compiled_eval_inference\n",
    "with torch.inference_mode():\n",
    "    out1 = model_compiled(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d95a6252-1f9f-4cc6-a907-d920ad59b80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch compiled speedup relative to not compiled: 0.723\n",
      "eval compiled speedup relative to not compiled: 1.263\n",
      "eval and no_grad compiled speedup relative to not compiled: 1.056\n",
      "eval and inference mode compiled speedup relative to not compiled: 1.019\n"
     ]
    }
   ],
   "source": [
    "times_compiled = [timeit_compiled_pytorch, timeit_compiled_eval, timeit_compiled_eval_nograd, timeit_compiled_eval_inference]\n",
    "names = [\"pytorch\", \"eval\", \"eval and no_grad\", \"eval and inference mode\"]\n",
    "\n",
    "for time, time_compiled, name in zip(times, times_compiled, names):\n",
    "    print(f\"{name} compiled speedup relative to not compiled: {time.average / time_compiled.average:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a21ce1-45d9-4558-b6c4-e74570c9d81c",
   "metadata": {},
   "source": [
    "I'm not sure what was happening with default compile but ignoring the slowest run it's faster than non-compiled version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8908d27a-6c55-4ca8-a959-06e5c3680bf8",
   "metadata": {},
   "source": [
    "## 3. Quantization\n",
    "\n",
    "Another way to optimize a model is to **quantize** its weights, reducing its size, but also the precision.\n",
    "Quantization means representing parameters (weights, and optionally also activations) with lower precision\n",
    "than the standard 32 bits. Most often, this means switching to 8-bit integers, i.e. dtype `int8`.\n",
    "\n",
    "PyTorch provides built-in tools for both **dynamic** and **static quantization**.\n",
    "\n",
    "**Dynamic quantization:**\n",
    "- convert weights `fp32 -> int8`, while activations remain in `float32` and are quantized dynamically during\n",
    "  model execution\n",
    "- does not require any post-training calibration\n",
    "- slower and more complex than static quantization, but also more precise\n",
    "- most effective and popular on CPU, which widely support integer operations\n",
    "- GPU usage requires specialized software & hardware (supporting `int8` operations)\n",
    "\n",
    "**Static quantization:**\n",
    "- quantize both weights and activations to `int8`\n",
    "- typically requires calibration, i.e. passing data through the model to estimate value ranges to know\n",
    "  how to quantize\n",
    "- faster and simpler to execute, but may be less precise (due to rounding activations)\n",
    "- more frequently used in production, particularly because the saved model files are smaller in this mode\n",
    "\n",
    "### Exercise 3 (3 points)\n",
    "\n",
    "We will perform a dynamic quantization for our model, which is very simple operationally to use with PyTorch.\n",
    "It provides the `torch.ao.quantization.quantize_dynamic()` function, to which we pass the model and a \n",
    "list of layer types that we want to quantize. In the case of transformers, those are primarily the linear\n",
    "layers, which contain the majority of weights and perform most computations.\n",
    "\n",
    "1. Ensure the model is on the CPU.\n",
    "2. Quantize the model with `torch.ao.quantization.quantize_dynamic()`, setting the target weight to `torch.qint8` and\n",
    "   layers to a single-element set with `nn.Linear`.\n",
    "3. Save the model to a new variable (e.g. `model_quantized`), and print it to verify that linear layers have been\n",
    "   quantized properly (i.e. `DynamicQuantizedLinear` instead of `Linear`).\n",
    "4. Save both models to disk (`state_dict` for both) and compare the file sizes (e.g. `os.path.getsize()`).\n",
    "5. Compare the inference speed and speedup on CPU for original and quantized models (again, average of 100 runs).\n",
    "6. Display the comparison. Do you think that quantization is helpful in this case?\n",
    "\n",
    "Typically, we would observe the reduction in model size up to 4x and speedup of 1.5-2x, depending on the model type\n",
    "and what parameters exactly are quantized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b967563e-621d-47e4-b31f-97e5220676a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.to('cpu')\n",
    "inputs = inputs.to('cpu')\n",
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2c276d8-6eec-46a4-be95-bfc5fcfeefae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maps3\\AppData\\Local\\Temp\\ipykernel_13440\\2528016189.py:1: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  model_quantized = torch.ao.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MPNetModel(\n",
       "  (embeddings): MPNetEmbeddings(\n",
       "    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): MPNetEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (k): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (v): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (o): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (relative_attention_bias): Embedding(32, 12)\n",
       "  )\n",
       "  (pooler): MPNetPooler(\n",
       "    (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_quantized = torch.ao.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
    "model_quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "572d2fc4-ddc2-4c92-93ab-00b282db62a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 model size:      417.730 MB\n",
      "Quantized model size: 173.097 MB\n",
      "Size ratio:   2.413x smaller\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "torch.save(model.state_dict(), \"model_fp32.pth\")\n",
    "torch.save(model_quantized.state_dict(), \"model_quantized.pth\")\n",
    "\n",
    "size_fp32 = os.path.getsize(\"model_fp32.pth\") / (1024 * 1024)\n",
    "size_q = os.path.getsize(\"model_quantized.pth\") / (1024 * 1024)\n",
    "\n",
    "print(f\"FP32 model size:      {size_fp32:.3f} MB\")\n",
    "print(f\"Quantized model size: {size_q:.3f} MB\")\n",
    "print(f\"Size ratio:   {size_fp32 / size_q:.3f}x smaller\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2760d3c7-0cd1-40b9-99bc-f41e1e1bc153",
   "metadata": {},
   "source": [
    "- no optimizations (simple PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "99e4d20f-b556-4f2e-807c-42f05d4663ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "model_quantized.train()\n",
    "timeit_pytorch = None\n",
    "timeit_pytorch_quantized = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f0e7d9e-76d1-481c-b08d-4910960e800a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "628 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100 -r 1 -v timeit_pytorch_cpu\n",
    "out1 = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f81b6164-8319-4c0a-a48a-c500c7d2c606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "497 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100 -r 1 -v timeit_pytorch_cpu_quantized\n",
    "out1 = model_quantized(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "de3ebe51-c115-46c6-be73-46c3ca0bd44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch quantized speedup relative to unquantized: 1.264\n"
     ]
    }
   ],
   "source": [
    "print(f\"pytorch quantized speedup relative to unquantized: {timeit_pytorch_cpu.average / timeit_pytorch_cpu_quantized.average:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940be000-8160-495b-aa19-46c6f3f5672e",
   "metadata": {},
   "source": [
    "- `model.eval()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fc21b6dc-b068-4f22-98ea-e9e5571c0cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model_quantized.eval()\n",
    "timeit_pytorch_eval = None\n",
    "timeit_pytorch_quantized_eval = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e60e4923-10c7-4b2b-bcc4-fbe4120d747e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100 -r 1 -v timeit_pytorch_cpu_eval\n",
    "out1 = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b028d299-b868-4678-8a1d-ec0353ad5704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.2 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100 -r 1 -v timeit_pytorch_cpu_quantized_eval\n",
    "out1 = model_quantized(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c0ac1e92-122a-4713-af7d-59fa01ecbdb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch quantized speedup relative to unquantized: 2.442\n"
     ]
    }
   ],
   "source": [
    "print(f\"pytorch quantized speedup relative to unquantized: {timeit_pytorch_cpu_eval.average / timeit_pytorch_cpu_quantized_eval.average:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a8a633-2f08-4073-8a29-9fa56ccedd4f",
   "metadata": {},
   "source": [
    "- `model.eval()` and `no_grad()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d00d8407-be4c-4353-a75a-5baf20f7a373",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model_quantized.eval()\n",
    "timeit_pytorch_eval_nograd = None\n",
    "timeit_pytorch_quantized_eval_nograd = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "17585483-bbbb-4181-b95f-65d150038078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100 -r 1 -v timeit_pytorch_cpu_eval_nograd\n",
    "with torch.no_grad():\n",
    "    out1 = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f8b9398b-d6e5-440a-9550-fcdb587b64ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.8 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100 -r 1 -v timeit_pytorch_cpu_quantized_eval_nograd\n",
    "with torch.no_grad():\n",
    "    out1 = model_quantized(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "771e4f7d-077d-4a86-8c91-c7acc9b57096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch quantized speedup relative to unquantized: 2.840\n"
     ]
    }
   ],
   "source": [
    "print(f\"pytorch quantized speedup relative to unquantized: {timeit_pytorch_cpu_eval_nograd.average / timeit_pytorch_cpu_quantized_eval_nograd.average:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03898d72-52ab-44b9-a116-fb55ba4a4682",
   "metadata": {},
   "source": [
    "- `model.eval()` and `inference_mode()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8396cda6-031b-4fa3-9a83-6a98dc4851e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model_quantized.eval()\n",
    "timeit_pytorch_eval_inference = None\n",
    "timeit_pytorch_quantized_eval_inference = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "05a4efa5-e3ea-484a-8f2b-b5b78ac27f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100 -r 1 -v timeit_pytorch_cpu_eval_inference\n",
    "with torch.inference_mode():\n",
    "    out1 = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "550c858d-0aa5-406d-b398-1c40265da2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.5 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100 -r 1 -v timeit_pytorch_cpu_quantized_eval_inference\n",
    "with torch.inference_mode():\n",
    "    out1 = model_quantized(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "904a0398-817b-4036-8772-e1f18866ae45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch quantized speedup relative to unquantized: 2.906\n"
     ]
    }
   ],
   "source": [
    "print(f\"pytorch quantized speedup relative to unquantized: {timeit_pytorch_cpu_eval_inference.average / timeit_pytorch_cpu_quantized_eval_inference.average:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f23a96-9e97-4058-871c-94cbe4871375",
   "metadata": {},
   "source": [
    "Based on times and model size alone it's definitely worth it to use quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19771ca-2515-421e-ac22-c28986be7037",
   "metadata": {},
   "source": [
    "## 4. GPU optimization strategies\n",
    "\n",
    "### GPU inference\n",
    "\n",
    "The most straightforward way to speed up inference is to run the model on a GPU if you have a suitable card\n",
    "and can afford that in the production environment. Deep models typically run much faster on GPU than on CPU,\n",
    "especially for larger batches.\n",
    "\n",
    "For example:\n",
    "```python\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "inputs_gpu = {k: v.to(device) for k, v in inputs.items()}\n",
    "with torch.inference_mode():\n",
    "    outputs = model(**inputs_gpu)\n",
    "```\n",
    "\n",
    "Transferring data to the GPU involves additional overhead, so it's done explicitly in PyTorch as above.\n",
    "Due to this overhead, it may not be beneficial for tiny models and single inputs, so this should be\n",
    "kept in mind for inference.\n",
    "\n",
    "After transferring data to the GPU, it is also worth considering the use of `torch.compile()` on the model \n",
    "to gain additional acceleration through operator fusion and generation of optimized CUDA code. It works\n",
    "similarly to CPU compilation that we tried before.\n",
    "\n",
    "![torch_compile_1](images/with_torch_compile.png)\n",
    "\n",
    "### CUDA Graphs\n",
    "\n",
    "Launching individual GPU kernels for single operations incurs a significant overhead for many operations.\n",
    "Each one requires memory allocation, memory transfer, and synchronization. Instead, we can combine them\n",
    "in a **CUDA Graph**, replacing a sequence of kernels with a single, efficient operation.\n",
    "\n",
    "```python\n",
    "# Enable CUDA Graphs for maximum throughput\n",
    "compiled_model_with_cudagraphs = torch.compile(model, mode=\"max-autotune\")\n",
    "```\n",
    "\n",
    "![torch_compile_2](images/with_torch_compile_with_cuda_graphs.png)\n",
    "\n",
    "The `max-autotune` mode of PyTorch compilation can generate entirely new operations on the fly. In this mode,\n",
    "PyTorch creates several Triton kernel implementations for each operation, benchmarks their performance, and\n",
    "selects the fastest one.\n",
    "\n",
    "![torch_compile_3](images/with_torch_compile_with_cuda_kernels.png)\n",
    "\n",
    "These automatically generated kernels often outperform naive operations, or even handwritten generic \n",
    "implementations, because they are tailored for a given model. For example, tensor shapes are known and\n",
    "constant, and memory access patterns are predictable.\n",
    "\n",
    "However, CUDA Graphs are **static** by design - they record a fixed sequence of operations with predefined\n",
    "tensor shapes. This is problematic for models handling dynamic input sizes, e.g. variable-length sentences\n",
    "in transformers or images with different size in CNNs. CUDA Graphs become invalid when input dimensions\n",
    "change.\n",
    "\n",
    "The `max-autotune-no-cudagraphs` mode addresses this limitation. It still creates custom Triton kernels,\n",
    "optimized computation graphs, and fused operations, but allows the model to handle dynamic inputs without\n",
    "recompilation. This is relevant to many production environments with unpredictable input sizes, providing\n",
    "both flexibility and high performance.\n",
    "\n",
    "```python\n",
    "# Enable max-autotune without CUDA Graphs for dynamic input shapes\n",
    "compiled_model_dynamic = torch.compile(model, mode=\"max-autotune-no-cudagraphs\")\n",
    "```\n",
    "\n",
    "### Pinning GPU memory\n",
    "\n",
    "When transferring data from CPU to GPU, using **pinned (page-locked) memory** can speed up the transfer process.\n",
    "By default, PyTorch allocates tensors in pageable memory, which can be slower to transfer to GPU.\n",
    "To allocate pinned memory, use the `pin_memory=True` argument when creating tensors or DataLoader.\n",
    "\n",
    "Examples:\n",
    "\n",
    "```python\n",
    "inputs = tokenizer(sample_text, padding=True, truncation=True, return_tensors=\"pt\", pin_memory=True)\n",
    "```\n",
    "\n",
    "```python\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32, pin_memory=True)\n",
    "```\n",
    "\n",
    "When transferring to GPU, pinned memory allows for faster transfers, improving overall throughput.\n",
    "\n",
    "### Exercise 4 (2 points)\n",
    "\n",
    "1. Compare inference time of:\n",
    "   - `torch.compile()` with default settings\n",
    "   - `torch.compile()` with `mode=\"max-autotune\"`\n",
    "   - `torch.compile()` with `mode=\"max-autotune-no-cudagraphs\"`\n",
    "2. Report the average time of 100 runs and speedup of the latter two modes.\n",
    "\n",
    "Check a few different text input sizes. What happens in the latter two modes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "34219a71-f568-47cf-99cf-d0e9989c0a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch._dynamo\n",
    "torch._dynamo.reset()\n",
    "model = model.to('cuda')\n",
    "inputs = inputs.to('cuda')\n",
    "\n",
    "model_default = torch.compile(model)\n",
    "model_autotune = torch.compile(model, mode=\"max-autotune\")\n",
    "model_no_cg = torch.compile(model, mode=\"max-autotune-no-cudagraphs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee12c80-4a05-4ea7-9ae6-a950b4321f5e",
   "metadata": {},
   "source": [
    "- default compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cedb1386-91d7-4f00-93e0-3e8a8fba57d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_default.eval()\n",
    "model_default.device\n",
    "with torch.inference_mode():\n",
    "    model_default(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5de79582-636c-4a1d-928c-888ecd7a5d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.55 ms ± 262 μs per loop (mean ± std. dev. of 10 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100 -r 10 -v timeit_default\n",
    "with torch.inference_mode():\n",
    "    out1 = model_default(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80baaef-3469-4902-a6e9-7d7c64eb74d6",
   "metadata": {},
   "source": [
    "- autotune\n",
    "\n",
    "\n",
    "This is the first time i had any sort of problem when using windows on these labs, whenever i tried running anything after compiling with max-autotune i got 'int too large for C' error, found an issue describing this problem here https://github.com/pytorch/pytorch/issues/166886 and managed to get it to work after editing the dll file as described in https://github.com/pytorch/pytorch/issues/162430#issuecomment-3289054096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cf4c25f7-1586-475d-8efb-bfd8d9167e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_autotune.eval()\n",
    "model_autotune.device\n",
    "with torch.inference_mode():\n",
    "    model_autotune(**inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f4dcc837-aaeb-4514-b2e7-7c6889989d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.35 ms ± 166 μs per loop (mean ± std. dev. of 10 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100 -r 10 -v timeit_autotune\n",
    "with torch.inference_mode():\n",
    "    out1 = model_autotune(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f605165b-8534-4cbc-86a9-73f6d665b7cc",
   "metadata": {},
   "source": [
    "- autotune no cudagraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "49f931ef-cef8-4c79-b133-dc9b897ca5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_no_cg.eval()\n",
    "model_no_cg.device\n",
    "with torch.inference_mode():\n",
    "    model_no_cg(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "edf2e063-5cde-4b4c-87ff-7d6b33dc2766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.54 ms ± 183 μs per loop (mean ± std. dev. of 10 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100 -r 10 -v timeit_no_cg\n",
    "with torch.inference_mode():\n",
    "    out1 = model_no_cg(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326e6475-af91-4a2e-baf7-f381771cb2c0",
   "metadata": {},
   "source": [
    "## 4. Changing numerical precision\n",
    "\n",
    "Most modern CPU and GPU hardware can perform operations on 16-bit numbers (`float16` / `fp16`)\n",
    "much faster than on 32-bit numbers (`float32` / `fp32`). This is because we can pack twice the\n",
    "number of vectors into the same amount of memory, theoretically doubling the throughput. This is\n",
    "also known as half-precision computation.\n",
    "\n",
    "If your application can tolerate a minimal drop in accuracy, this kind of quantization (or precision\n",
    "reduction, depending on definition) is really useful for inference. Since this is equal to just \n",
    "cutting particular bits, this can be done on the fly easily, and some frameworks support doing \n",
    "this on model loading for weights.\n",
    "\n",
    "There are also other dedicated formats for neural networks. Newer NVidia GPUs also support `bfloat16`\n",
    "type, which retains value range and only cuts precision bits, which typically works better for neural\n",
    "networks. Further, we can use mixed precision, i.e. perform less sensitive operations in `fp16`\n",
    "(e.g. convolution), and more precise ones in `fp32` (e.g. weights updates).\n",
    "\n",
    "PyTorch also supports simplified automated casting to reduced precision types with `autocast`, see:\n",
    "- [torch.amp documentation](https://docs.pytorch.org/docs/stable/amp.html)\n",
    "- [torch.amp autocasting docs](https://docs.pytorch.org/docs/stable/amp.html#autocasting)\n",
    "- [automated mixed precision PyTorch tutorial](https://docs.pytorch.org/tutorials/recipes/recipes/amp_recipe.html)\n",
    "\n",
    "However, if your hardware does not support those types and fast operations, they probably will not\n",
    "provide any speedup, or this may even slow down execution due to type casts.\n",
    "\n",
    "You can check if your NVidia GPU supports fast float16 (via Tensor Cores) using the following code:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "capability = torch.cuda.get_device_capability()\n",
    "print(f\"CUDA device capability: {capability}\")\n",
    "\n",
    "# Tensor Cores are available on NVidia GPUs with CUDA >= 7 (e.g. Volta, Turing, Ampere, Hopper)\n",
    "if capability >= (7, 0):\n",
    "    print(\"Tensor Cores available: fast float16 supported.\")\n",
    "else:\n",
    "    print(\"Tensor Cores not available: float16 may be slow or unsupported.\")\n",
    "```\n",
    "\n",
    "Casting model weights and inputs to half-precision works as follows:\n",
    "\n",
    "```python\n",
    "model_half = model.half().to('cuda')\n",
    "outputs = model_half(input_ids.to('cuda').half(), attention_mask.to('cuda').half())\n",
    "```\n",
    "\n",
    "You can also verify it by running:\n",
    "\n",
    "```python\n",
    "model_fp32 = torch.nn.Linear(10, 1)\n",
    "data_fp32 = torch.randn(100, 10)\n",
    "labels_fp32 = torch.randn(100, 1)\n",
    "\n",
    "print(f\"Data type of model_fp32 parameters: {model_fp32.weight.dtype}\")\n",
    "print(f\"Data type of data_fp32: {data_fp32.dtype}\")\n",
    "print(f\"Data type of labels_fp32: {labels_fp32.dtype}\")\n",
    "\n",
    "output_fp32 = model_fp32(data_fp32)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "loss_fp32 = loss_fn(output_fp32, labels_fp32)\n",
    "\n",
    "print(f\"Loss fp32: {loss_fp32.item()}\")\n",
    "```\n",
    "\n",
    "```python\n",
    "model_fp16 = model_fp32.half()\n",
    "data_fp16 = data_fp32.half()\n",
    "labels_fp16 = labels_fp32.half()\n",
    "\n",
    "print(f\"Data type of model_fp16 parameters: {model_fp16.weight.dtype}\")\n",
    "print(f\"Data type of data_fp16: {data_fp16.dtype}\")\n",
    "print(f\"Data type of labels_fp16: {labels_fp16.dtype}\")\n",
    "\n",
    "output_fp16 = model_fp16(data_fp16)\n",
    "loss_fp16 = loss_fn(output_fp16.float(), labels_fp16.float())\n",
    "\n",
    "print(f\"Loss fp16: {loss_fp16.item()}\")\n",
    "```\n",
    "\n",
    "### Exercise 5 (2 points)\n",
    "\n",
    "1. Check if your GPU supports Tensor Cores (capability >= (7,0)). If not, switch to Google Colab with GPU runtime.\n",
    "2. Measure inference time with:\n",
    "   - full precision (`float32`)\n",
    "   - manual half-precision (`float16`)\n",
    "   - automatic mixed precision (`torch.autocast`)\n",
    "3. Compare time and speedup. Which variant would you use in practice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e02409d6-15e9-44d4-82ee-dca52270301c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA device capability: (12, 0)\n",
      "Tensor Cores available: fast float16 supported.\n"
     ]
    }
   ],
   "source": [
    "capability = torch.cuda.get_device_capability()\n",
    "print(f\"CUDA device capability: {capability}\")\n",
    "\n",
    "# Tensor Cores are available on NVidia GPUs with CUDA >= 7 (e.g. Volta, Turing, Ampere, Hopper)\n",
    "if capability >= (7, 0):\n",
    "    print(\"Tensor Cores available: fast float16 supported.\")\n",
    "else:\n",
    "    print(\"Tensor Cores not available: float16 may be slow or unsupported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0c6526e3-9801-4246-85b1-0cedbb6b77fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"One two three four five six seven eight nine\"*32\n",
    "inputs = tokenizer(prompt, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "inputs = inputs.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e2b9ca-c07f-4398-b1b7-130f5ec2e770",
   "metadata": {},
   "source": [
    "- fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ef9dc181-f4e3-492a-8f0a-47c69c590b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "fp32_time = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f5be090b-450d-486d-a2b9-400ed61c9ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.95 ms ± 227 μs per loop (mean ± std. dev. of 10 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100 -r 10 -v fp32_time\n",
    "with torch.inference_mode():\n",
    "    out1 = model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8d77f9-53ac-4216-86f1-3a4b9738fce7",
   "metadata": {},
   "source": [
    "- fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3a8cf5aa-061d-481d-b231-2703dcbd4520",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_half = model.half().to(\"cuda\")\n",
    "model_half.eval()\n",
    "fp16_time = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "878a5be1-fbd8-4e4a-a8ba-de0b5a56eae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.77 ms ± 139 μs per loop (mean ± std. dev. of 10 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100 -r 10 -v fp16_time\n",
    "with torch.inference_mode():\n",
    "    out1 = model_half(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "79e5d2f4-c623-4d3a-b5e4-c6de1fb54fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fp32 to fp16 speedup: 1.030\n"
     ]
    }
   ],
   "source": [
    "print(f\"fp32 to fp16 speedup: {fp32_time.average / fp16_time.average:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b889976-d1bd-4a7e-ad38-f6a9ddeaf2fa",
   "metadata": {},
   "source": [
    "- autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5375427f-0cc3-4fff-804e-70ae3331ad4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "autocast_time = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f03c0a20-d8c6-4d3d-9234-27f17e22fda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.22 ms ± 223 μs per loop (mean ± std. dev. of 10 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100 -r 10 -v autocast_time\n",
    "with torch.inference_mode(), torch.autocast('cuda', dtype=torch.float16):\n",
    "    out1 = model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6b0318-7764-4a7d-b9af-6cd95a48996e",
   "metadata": {},
   "source": [
    "Any speedup when using fp16 vs fp32 is negligible for this model. This could be because the model is too small for my gpu to use the benefits of fp16 acceleration in any meaningful way. Using autocast likely adds a slight overhead due to dynamically choosing which operations to run in fp16 vs fp32, but in my case this is largely irrelevant so autocast is worst in terms of execution time (5.95 ms vs 5.77 ms for fp32 vs fp16, and 7.22 ms with autocast).\n",
    "\n",
    "I also tried increasing input length (`prompt = \"One two three four five six seven eight nine\"*32`) in hope to make the difference a little more visible, which it did, but it also somehow made the inference run faster, which i have no explanation for :] (~7-8ms at start of the notebook vs <6ms here)\n",
    "\n",
    "### To better see the difference i took some random larger model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8ce4bead-4d56-4d32-ad75-17a31f6d2338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a2fc853918846769c130d08edda76e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model1 = AutoModel.from_pretrained(\"Alibaba-NLP/gme-Qwen2-VL-2B-Instruct\").to(\"cuda\")\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(\"Alibaba-NLP/gme-Qwen2-VL-2B-Instruct\")\n",
    "prompt1 = \"One two three four five six seven eight nine\"*32\n",
    "inputs1 = tokenizer(prompt, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "inputs1 = inputs.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3caf17-041c-4fd1-bc90-627fb47b0b40",
   "metadata": {},
   "source": [
    "- fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fbd44114-93e6-4f39-b2ba-ad0e828e4ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.eval()\n",
    "fp32_time1 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "284db82a-394c-45b8-9bb9-656943c9785d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.5 ms ± 658 μs per loop (mean ± std. dev. of 10 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100 -r 10 -v fp32_time\n",
    "with torch.inference_mode():\n",
    "    out1 = model1(**inputs1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b60828-ee12-40e6-a057-494f5a52e4c4",
   "metadata": {},
   "source": [
    "- fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a1c5f442-5a4d-452e-a4eb-cde9130cae0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.synchronize()\n",
    "model_half1 = model1.half().to(\"cuda\")\n",
    "model_half1.eval()\n",
    "fp16_time = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c200851a-c477-4b4a-85b0-d954094a9d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.1 ms ± 413 μs per loop (mean ± std. dev. of 10 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100 -r 10 -v fp16_time\n",
    "with torch.inference_mode():\n",
    "    out1 = model_half1(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d808af5f-1023-49bb-9705-dc332e47cb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fp32 to fp16 speedup: 2.459\n"
     ]
    }
   ],
   "source": [
    "print(f\"fp32 to fp16 speedup: {fp32_time.average / fp16_time.average:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54cb9de-b4a2-4f8f-8da2-190db350a4e0",
   "metadata": {},
   "source": [
    "- autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "438b942c-d19f-4d65-836f-6737aca365ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del model_half1\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6b658e13-5c99-4bb0-bf8d-9fe215993f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.4 ms ± 729 μs per loop (mean ± std. dev. of 10 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100 -r 10 -v autocast_time\n",
    "with torch.inference_mode(), torch.amp.autocast('cuda', dtype=torch.float16):\n",
    "    out1 = model1(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c2dd5871-81d3-42a8-874b-8b8158d4411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del model1\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b580b5-99bd-45b0-ac26-7e5d6fc9db51",
   "metadata": {},
   "source": [
    "Here speedup of fp16 is much more visible, autocast also performs better than fp32 :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41982b8c-4830-4e77-afc1-d381e7a017cd",
   "metadata": {},
   "source": [
    "## 5. ONNX\n",
    "\n",
    "ONNX (Open Neural Network eXchange) is a standardized format for representing neural networks. It abstracts\n",
    "operations, turning the framework-specific code into an **execution graph** built from standardized operators.\n",
    "It describes operations, input/output shapes, and model parameters in a hardware- and framework-agnostic way.\n",
    "Then, it can be run with via ONNX Runtime (ORT), which can execute the code with kernels and optimizations from\n",
    "specialized providers, like Intel OpenVINO or NVidia TensorRT.\n",
    "\n",
    "ONNX and ONNX Runtime have considerable advantages:\n",
    "1. Framework- and language-agnostic - ONNX runs on any framework and programming language, e.g. you can export\n",
    "   PyTorch model in Python, and then run it in a Java application.\n",
    "2. Execution graph optimization - ONNX Runtime provides a series of optimizations for the execution graph,\n",
    "   including hardware-specific operators provided by manufacturers.\n",
    "3. Lightweight deployment - ONNX & ORT have much smaller package size than the whole PyTorch (even CPU-only wheels),\n",
    "   reducing sizes of dependencies and Docker containers, and accelerating loading.\n",
    "\n",
    "In practice, `torch.compile()` works well for PyTorch optimization, but ONNX is preferable for deploying models,\n",
    "particularly for lightweight or mobile runtimes. It also supports GPU inference via NVidia TensorRT provider.\n",
    "\n",
    "Exporting to ONNX produces a raw computation graph in `.onnx` format. This file is:\n",
    "- a static description of operators, weights, and I/O tensors\n",
    "- a general graph - no hardware-specific rewrites happen during ONNX export\n",
    "- hardware-agnostic - it does not contain CUDA/CPU kernels or provider information\n",
    "\n",
    "We will export a Transformer model with dynamic batch size and dynamic sequence length.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.onnx\n",
    "\n",
    "# Put the model in eval mode and move to CPU\n",
    "model_cpu = model.eval().cpu()\n",
    "\n",
    "# Example input for tracking (for onnx export)\n",
    "sample_input = tokenizer(\n",
    "    \"This is a sample input text for ONNX export.\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "# Export to ONNX format\n",
    "torch.onnx.export(\n",
    "    model_cpu,\n",
    "    (sample_input[\"input_ids\"], sample_input[\"attention_mask\"]),\n",
    "    \"model.onnx\",\n",
    "    opset_version=17,\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\n",
    "        \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"output\": {0: \"batch_size\"},\n",
    "    },\n",
    ")\n",
    "```\n",
    "\n",
    "We export on CPU in `eval()` mode to get deterministic behavior.\n",
    "\n",
    "Look at how we marked dynamic axes in `dynamic_axes`:\n",
    "1. For `input_ids` and `attention_mask`, we marked axes 0 (batch size) and 1 (sequence length) as dynamic,\n",
    "   since they can vary during inference.\n",
    "   - axis 0 - batch size, depends on number of inputs\n",
    "   - axis 1 - sequence length, depends on text length\n",
    "   - axis 2 - embedding size, fixed and constant (768), so we don't mark it\n",
    "2. For `output`, we marked only axis 0 (batch size) as dynamic, since the output will have the same number\n",
    "   of rows as the input batch size.\n",
    "\n",
    "The exported `model.onnx` is a raw graph, not yet optimized. It can be changed during InferenceSession\n",
    "creation in ONNX Runtime or when we explicitly run offline optimizations.\n",
    "\n",
    "### Optimization & inference with ONNX Runtime\n",
    "\n",
    "First, we run inference using ONNX Runtime with default settings. By default, all optimizations are applied.\n",
    "\n",
    "```python\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "# Load the model\n",
    "ort_session = ort.InferenceSession(\"model.onnx\")\n",
    "\n",
    "# Prepare input data\n",
    "sample_input = tokenizer(\n",
    "    \"This is a sample input text for ONNX inference.\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"np\",\n",
    ")\n",
    "\n",
    "\n",
    "# Create input dictionary, in same format as during export\n",
    "inputs_onnx = {\n",
    "    \"input_ids\": sample_input[\"input_ids\"],\n",
    "    \"attention_mask\": sample_input[\"attention_mask\"],\n",
    "}\n",
    "\n",
    "# Run inference\n",
    "outputs_onnx = ort_session.run(None, inputs_onnx)\n",
    "```\n",
    "\n",
    "The raw ONNX is parsed, optimized (default level is `ORT_ENABLE_ALL`), and executed using the default\n",
    "execution provider (generally generic CPU by default).\n",
    "\n",
    "We did not specify a provider in this example to keep the code short. ONNX Runtime internally chooses \n",
    "providers based on how it was built (for example, CPU only, or CPU + CUDA). For production use, you \n",
    "should specify providers explicitly. We will do that in the next section.\n",
    "\n",
    "### Graph optimization settings\n",
    "\n",
    "ONNX Runtime groups graph optimizations into levels. Each level builds on the previous one:\n",
    "\n",
    "1. **Basic graph optimizations** - semantics-preserving rewrites that remove redundant work. \n",
    "   They run before graph partitioning, so they apply to nodes regardless of the target execution provider.\n",
    "2. **Extended graph optimizations** - They run after graph partitioning and are applied only to nodes \n",
    "   assigned to selected providers (CPU, CUDA, ROCm).\n",
    "3. **Layout optimizations** - change layout from NHCW to NCHWc for CPU provider.\n",
    "\n",
    "All optimizations are enabled by default. You can control them using the `GraphOptimizationLevel` enum:\n",
    "* `ORT_DISABLE_ALL` – disable all optimizations\n",
    "* `ORT_ENABLE_BASIC` – only basic\n",
    "* `ORT_ENABLE_EXTENDED` – basic and extended\n",
    "* `ORT_ENABLE_ALL` – basic + extended + layout optimizations (default)\n",
    "\n",
    "### Online mode (load-time optimization)\n",
    "\n",
    "In online mode, optimizations are applied each time you create an `InferenceSession`.\n",
    "This happens when you create it:\n",
    "```python\n",
    "ort_session = ort.InferenceSession(\"model.onnx\")\n",
    "```\n",
    "We can control the optimization level using `SessionOptions`:\n",
    "\n",
    "```python\n",
    "options = ort.SessionOptions()\n",
    "options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "\n",
    "ort_session = ort.InferenceSession(\n",
    "    \"model.onnx\", sess_options=options, providers=[\"CPUExecutionProvider\"]\n",
    ")\n",
    "```\n",
    "\n",
    "Online mode is most convenient for:\n",
    "- development and experimentation - you can quickly try out different settings\n",
    "- dynamic environments - when running on different hardware or deployments, depending on settings\n",
    "\n",
    "The cost of online mode is that optimization work is repeated each time a session is created, which\n",
    "may be noticeable for large models. When you deploy to a known target each time, offline mode is\n",
    "a better choice.\n",
    "\n",
    "### Offline mode (ahead-of-time optimization)\n",
    "\n",
    "In offline mode, optimizations are applied once, and the optimized model is saved to a new ONNX file. \n",
    "This can significantly reduce startup time in production environments. The key element is setting the\n",
    "`SessionOptions.optimized_model_filepath`, which specifies where to save the optimized model.\n",
    "When enabled, ONNX Runtime runs graph optimizations according to `graph_optimization_level`, and saves\n",
    "the optimized model to the file.\n",
    "\n",
    "```python\n",
    "import onnxruntime as ort\n",
    "\n",
    "sess_options = ort.SessionOptions()\n",
    "\n",
    "# Choose the optimization level for the offline pass\n",
    "sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_EXTENDED\n",
    "\n",
    "# Save the optimized model to this path\n",
    "sess_options.optimized_model_filepath = \"model_optimized.onnx\"\n",
    "\n",
    "# Create InferenceSession, which will perform offline optimization and save the optimized model\n",
    "ort.InferenceSession(\"model.onnx\", sess_options)\n",
    "```\n",
    "\n",
    "After you can load this file and disable optimizations to avoid re-optimizing:\n",
    "\n",
    "```python\n",
    "# Load the optimized model without re-optimizing\n",
    "sess_options = ort.SessionOptions()\n",
    "sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_DISABLE_ALL\n",
    "\n",
    "ort_session_optimized = ort.InferenceSession(\n",
    "    \"model_optimized.onnx\", \n",
    "    sess_options=sess_options, \n",
    "    providers=['CPUExecutionProvider']\n",
    ")\n",
    "```\n",
    "\n",
    "Offline mode is best suited for:\n",
    "- production deployments - startup time is important, and the model changes only during training\n",
    "- limited resource environments - repeated optimization is costly\n",
    "- static hardware setups - when we know the hardware configuration, there is no need for re-optimization\n",
    "\n",
    "### Executions Providers\n",
    "\n",
    "Execution providers decide how and where the nodes of the ONNX graph are executed. They are not an\n",
    "extra optimization pass on top of the graph. Instead, they are backends that provide concrete kernel\n",
    "implementations for operators such as `MatMul`, `Conv`, `LayerNorm`, and so on.\n",
    "\n",
    "Typical providers include:\n",
    "\n",
    "* `CPUExecutionProvider`\n",
    "* `CUDAExecutionProvider`\n",
    "* `TensorrtExecutionProvider`\n",
    "* `OpenVINOExecutionProvider`\n",
    "\n",
    "The ONNX file itself is always hardware-agnostic. It does not contain any provider information.\n",
    "Providers come into play only when you create an `InferenceSession`. Provider is responsible for:\n",
    "\n",
    "* mapping ONNX operations to actual kernels, e.g. CPU BLAS vs cuBLAS vs TensorRT engines\n",
    "* deciding which fused patterns it can execute efficiently for extended optimizations\n",
    "* executing its part of the graph on the target hardware\n",
    "\n",
    "So why do we need to care about providers? In production, it is better to be explicit, so that the behavior\n",
    "does not change when you move the same model to a different environment.\n",
    "\n",
    "```python\n",
    "import onnxruntime as ort\n",
    "\n",
    "options = ort.SessionOptions()\n",
    "options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "\n",
    "# Force CPU only\n",
    "session_cpu = ort.InferenceSession(\n",
    "    \"model.onnx\", sess_options=options, providers=[\"CPUExecutionProvider\"]\n",
    ")\n",
    "\n",
    "# Prefer CUDA, fall back to CPU if CUDA is not available\n",
    "session_cuda = ort.InferenceSession(\n",
    "    \"model.onnx\",\n",
    "    sess_options=options,\n",
    "    providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"],\n",
    ")\n",
    "```\n",
    "\n",
    "For more information about providers, see the official [Execution Providers section](https://iot-robotics.github.io/ONNXRuntime/docs/execution-providers/).\n",
    "\n",
    "### Exercise 6 (3 points)\n",
    "\n",
    "1. Measure cold start time (including session creation) of the ONNX model using online and offline optimization modes\n",
    "   on CPU.\n",
    "2. Measure inference time of the ONNX model on CPU using both optimization modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5e2e570b-8243-4f44-830f-453bc70d7736",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maps3\\AppData\\Local\\Temp\\ipykernel_13440\\2026979277.py:16: UserWarning: # 'dynamic_axes' is not recommended when dynamo=True, and may lead to 'torch._dynamo.exc.UserError: Constraints violated.' Supply the 'dynamic_shapes' argument instead if export is unsuccessful.\n",
      "  torch.onnx.export(\n",
      "W1126 19:51:58.406000 13440 Lib\\site-packages\\torch\\onnx\\_internal\\exporter\\_compat.py:114] Setting ONNX exporter to use operator set version 18 because the requested opset_version 17 is a lower version than we have implementations for. Automatic version conversion will be performed, which may not be successful at converting to the requested version. If version conversion is unsuccessful, the opset version of the exported model will be kept at 18. Please consider setting opset_version >=18 to leverage latest ONNX features\n",
      "W1126 19:51:58.767000 13440 Lib\\site-packages\\torch\\onnx\\_internal\\exporter\\_registration.py:107] torchvision is not installed. Skipping torchvision::nms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `MPNetModel([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `MPNetModel([...]` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model version conversion is not supported by the onnxscript version converter and fallback is enabled. The model will be converted using the onnx C API (target version: 17).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "Applied 83 of general pattern rewrite rules.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ONNXProgram(\n",
       "    model=\n",
       "        <\n",
       "            ir_version=10,\n",
       "            opset_imports={'': 17},\n",
       "            producer_name='pytorch',\n",
       "            producer_version='2.9.1+cu130',\n",
       "            domain=None,\n",
       "            model_version=None,\n",
       "        >\n",
       "        graph(\n",
       "            name=main_graph,\n",
       "            inputs=(\n",
       "                %\"input_ids\"<INT64,[s43,s53]>,\n",
       "                %\"attention_mask\"<INT64,[s43,s53]>\n",
       "            ),\n",
       "            outputs=(\n",
       "                %\"output\"<FLOAT16,[1,s53,768]>,\n",
       "                %\"tanh\"<FLOAT16,[1,768]>\n",
       "            ),\n",
       "            initializers=(\n",
       "                %\"embeddings.LayerNorm.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"embeddings.LayerNorm.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.0.attention.attn.q.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.0.attention.attn.k.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.0.attention.attn.v.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.0.attention.attn.o.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.0.attention.LayerNorm.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.0.attention.LayerNorm.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.0.output.dense.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.0.output.LayerNorm.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.0.output.LayerNorm.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.1.attention.attn.q.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.1.attention.attn.k.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.1.attention.attn.v.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.1.attention.attn.o.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.1.attention.LayerNorm.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.1.attention.LayerNorm.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.1.output.dense.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.1.output.LayerNorm.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.1.output.LayerNorm.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.2.attention.attn.q.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.2.attention.attn.k.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.2.attention.attn.v.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.2.attention.attn.o.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.2.attention.LayerNorm.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.2.attention.LayerNorm.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.2.output.dense.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.2.output.LayerNorm.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.2.output.LayerNorm.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.3.attention.attn.q.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.3.attention.attn.k.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.3.attention.attn.v.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.3.attention.attn.o.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.3.attention.LayerNorm.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.3.attention.LayerNorm.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.3.output.dense.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.3.output.LayerNorm.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.3.output.LayerNorm.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.4.attention.attn.q.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.4.attention.attn.k.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.4.attention.attn.v.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.4.attention.attn.o.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.4.attention.LayerNorm.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.4.attention.LayerNorm.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.4.output.dense.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.4.output.LayerNorm.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.4.output.LayerNorm.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.5.attention.attn.q.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.5.attention.attn.k.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.5.attention.attn.v.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.5.attention.attn.o.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.5.attention.LayerNorm.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.5.attention.LayerNorm.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.5.output.dense.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.5.output.LayerNorm.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.5.output.LayerNorm.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.6.attention.attn.q.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.6.attention.attn.k.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.6.attention.attn.v.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.6.attention.attn.o.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.6.attention.LayerNorm.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.6.attention.LayerNorm.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.6.output.dense.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.6.output.LayerNorm.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.6.output.LayerNorm.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.7.attention.attn.q.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.7.attention.attn.k.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.7.attention.attn.v.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.7.attention.attn.o.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.7.attention.LayerNorm.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.7.attention.LayerNorm.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.7.output.dense.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.7.output.LayerNorm.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.7.output.LayerNorm.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.8.attention.attn.q.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.8.attention.attn.k.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.8.attention.attn.v.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.8.attention.attn.o.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.8.attention.LayerNorm.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.8.attention.LayerNorm.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.8.output.dense.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.8.output.LayerNorm.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.8.output.LayerNorm.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.9.attention.attn.q.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.9.attention.attn.k.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.9.attention.attn.v.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.9.attention.attn.o.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.9.attention.LayerNorm.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.9.attention.LayerNorm.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.9.output.dense.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.9.output.LayerNorm.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.9.output.LayerNorm.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.10.attention.attn.q.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.10.attention.attn.k.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.10.attention.attn.v.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.10.attention.attn.o.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.10.attention.LayerNorm.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.10.attention.LayerNorm.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.10.output.dense.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.10.output.LayerNorm.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.10.output.LayerNorm.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.11.attention.attn.q.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.11.attention.attn.k.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.11.attention.attn.v.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.11.attention.attn.o.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.11.attention.LayerNorm.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.11.attention.LayerNorm.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.11.output.dense.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.11.output.LayerNorm.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.11.output.LayerNorm.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.relative_attention_bias.weight\"<FLOAT16,[32,12]>{TorchTensor(...)},\n",
       "                %\"embeddings.word_embeddings.weight\"<FLOAT16,[30527,768]>{TorchTensor(...)},\n",
       "                %\"embeddings.position_embeddings.weight\"<FLOAT16,[514,768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.0.intermediate.dense.bias\"<FLOAT16,[3072]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.1.intermediate.dense.bias\"<FLOAT16,[3072]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.2.intermediate.dense.bias\"<FLOAT16,[3072]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.3.intermediate.dense.bias\"<FLOAT16,[3072]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.4.intermediate.dense.bias\"<FLOAT16,[3072]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.5.intermediate.dense.bias\"<FLOAT16,[3072]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.6.intermediate.dense.bias\"<FLOAT16,[3072]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.7.intermediate.dense.bias\"<FLOAT16,[3072]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.8.intermediate.dense.bias\"<FLOAT16,[3072]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.9.intermediate.dense.bias\"<FLOAT16,[3072]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.10.intermediate.dense.bias\"<FLOAT16,[3072]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.11.intermediate.dense.bias\"<FLOAT16,[3072]>{TorchTensor(...)},\n",
       "                %\"pooler.dense.weight\"<FLOAT16,[768,768]>{TorchTensor(...)},\n",
       "                %\"scalar_tensor_default\"<FLOAT16,[]>{Tensor<FLOAT16,[]>(array(1., dtype=float16), name='scalar_tensor_default')},\n",
       "                %\"scalar_tensor_default_1\"<FLOAT16,[]>{Tensor<FLOAT16,[]>(array(-6.55e+04, dtype=float16), name='scalar_tensor_default_1')},\n",
       "                %\"val_34\"<INT64,[]>{Tensor<INT64,[]>(array(0), name='val_34')},\n",
       "                %\"val_35\"<INT64,[]>{Tensor<INT64,[]>(array(1), name='val_35')},\n",
       "                %\"scalar_tensor_default_2\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(8., dtype=float32), name='scalar_tensor_default_2')},\n",
       "                %\"val_64\"<INT64,[]>{Tensor<INT64,[]>(array(15), name='val_64')},\n",
       "                %\"val_72\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_80\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_88\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"scalar_tensor_default_4\"<FLOAT16,[]>{Tensor<FLOAT16,[]>(array(8., dtype=float16), name='scalar_tensor_default_4')},\n",
       "                %\"val_102\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_106\"<FLOAT16,[768,3072]>{Tensor(...)},\n",
       "                %\"val_115\"<FLOAT16,[3072,768]>{Tensor(...)},\n",
       "                %\"val_119\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_127\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_135\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_148\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_152\"<FLOAT16,[768,3072]>{Tensor(...)},\n",
       "                %\"val_161\"<FLOAT16,[3072,768]>{Tensor(...)},\n",
       "                %\"val_165\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_173\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_181\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_194\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_198\"<FLOAT16,[768,3072]>{Tensor(...)},\n",
       "                %\"val_207\"<FLOAT16,[3072,768]>{Tensor(...)},\n",
       "                %\"val_211\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_219\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_227\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_240\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_244\"<FLOAT16,[768,3072]>{Tensor(...)},\n",
       "                %\"val_253\"<FLOAT16,[3072,768]>{Tensor(...)},\n",
       "                %\"val_257\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_265\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_273\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_286\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_290\"<FLOAT16,[768,3072]>{Tensor(...)},\n",
       "                %\"val_299\"<FLOAT16,[3072,768]>{Tensor(...)},\n",
       "                %\"val_303\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_311\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_319\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_332\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_336\"<FLOAT16,[768,3072]>{Tensor(...)},\n",
       "                %\"val_345\"<FLOAT16,[3072,768]>{Tensor(...)},\n",
       "                %\"val_349\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_357\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_365\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_378\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_382\"<FLOAT16,[768,3072]>{Tensor(...)},\n",
       "                %\"val_391\"<FLOAT16,[3072,768]>{Tensor(...)},\n",
       "                %\"val_395\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_403\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_411\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_424\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_428\"<FLOAT16,[768,3072]>{Tensor(...)},\n",
       "                %\"val_437\"<FLOAT16,[3072,768]>{Tensor(...)},\n",
       "                %\"val_441\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_449\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_457\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_470\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_474\"<FLOAT16,[768,3072]>{Tensor(...)},\n",
       "                %\"val_483\"<FLOAT16,[3072,768]>{Tensor(...)},\n",
       "                %\"val_487\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_495\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_503\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_516\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_520\"<FLOAT16,[768,3072]>{Tensor(...)},\n",
       "                %\"val_529\"<FLOAT16,[3072,768]>{Tensor(...)},\n",
       "                %\"val_533\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_541\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_549\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_562\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_566\"<FLOAT16,[768,3072]>{Tensor(...)},\n",
       "                %\"val_575\"<FLOAT16,[3072,768]>{Tensor(...)},\n",
       "                %\"val_579\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_587\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_595\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_608\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_612\"<FLOAT16,[768,3072]>{Tensor(...)},\n",
       "                %\"val_621\"<FLOAT16,[3072,768]>{Tensor(...)},\n",
       "                %\"val_14\"<INT64,[1]>{TensorProtoTensor<INT64,[1]>(array([1]), name='val_14')},\n",
       "                %\"val_635\"<INT64,[2]>{Tensor<INT64,[2]>(array([1, 2]), name='val_635')},\n",
       "                %\"val_49\"<INT64,[1]>{TensorProtoTensor<INT64,[1]>(array([0]), name='val_49')},\n",
       "                %\"val_60\"<INT64,[]>{TensorProtoTensor<INT64,[]>(array(16), name='val_60')},\n",
       "                %\"val_61\"<INT64,[]>{TensorProtoTensor<INT64,[]>(array(8), name='val_61')},\n",
       "                %\"val_62\"<FLOAT,[]>{TensorProtoTensor<FLOAT,[]>(array(2.7725887, dtype=float32), name='val_62')},\n",
       "                %\"val_76\"<INT64,[1]>{Tensor<INT64,[1]>(array([-1]), name='val_76')},\n",
       "                %\"val_77\"<INT64,[1]>{Tensor<INT64,[1]>(array([12]), name='val_77')},\n",
       "                %\"val_78\"<INT64,[1]>{Tensor<INT64,[1]>(array([64]), name='val_78')},\n",
       "                %\"val_100\"<INT64,[1]>{Tensor<INT64,[1]>(array([768]), name='val_100')},\n",
       "                %\"val_108\"<FLOAT16,[]>{TensorProtoTensor<FLOAT16,[]>(array(1.414, dtype=float16), name='val_108')},\n",
       "                %\"val_113\"<FLOAT16,[]>{TensorProtoTensor<FLOAT16,[]>(array(0.5, dtype=float16), name='val_113')}\n",
       "            ),\n",
       "        ) {\n",
       "              0 |  # node_Shape_0\n",
       "                   %\"val_0\"<INT64,[1]> ⬅️ ::Shape(%\"attention_mask\") {start=0, end=1}\n",
       "              1 |  # node_Shape_1\n",
       "                   %\"val_1\"<INT64,[1]> ⬅️ ::Shape(%\"attention_mask\") {start=1, end=2}\n",
       "              2 |  # node_sym_size_int_86\n",
       "                   %\"sym_size_int_86\"<INT64,[]> ⬅️ ::Squeeze(%\"val_1\")\n",
       "              3 |  # node_Unsqueeze_24\n",
       "                   %\"unsqueeze_1\"<INT64,[s43,1,1,s53]> ⬅️ ::Unsqueeze(%\"attention_mask\", %\"val_635\"{[1, 2]})\n",
       "              4 |  # node__to_copy\n",
       "                   %\"_to_copy\"<FLOAT16,[s43,1,1,s53]> ⬅️ ::Cast(%\"unsqueeze_1\") {to=10}\n",
       "              5 |  # node_sub_10\n",
       "                   %\"sub_10\"<FLOAT16,[s43,1,1,s53]> ⬅️ ::Sub(%\"scalar_tensor_default\"{1.0}, %\"_to_copy\")\n",
       "              6 |  # node_mul_22\n",
       "                   %\"mul_22\"<FLOAT16,[1,1,1,s53]> ⬅️ ::Mul(%\"sub_10\", %\"scalar_tensor_default_1\"{-65504.0})\n",
       "              7 |  # node_Equal_30\n",
       "                   %\"val_30\"<BOOL,[s43,s53]> ⬅️ ::Equal(%\"input_ids\", %\"val_35\"{1})\n",
       "              8 |  # node_ne_3\n",
       "                   %\"ne_3\"<BOOL,[s43,s53]> ⬅️ ::Not(%\"val_30\")\n",
       "              9 |  # node__to_copy_1\n",
       "                   %\"_to_copy_1\"<INT32,[s43,s53]> ⬅️ ::Cast(%\"ne_3\") {to=6}\n",
       "             10 |  # node_convert_element_type_default\n",
       "                   %\"convert_element_type_default\"<INT64,[s43,s53]> ⬅️ ::Cast(%\"_to_copy_1\") {to=7}\n",
       "             11 |  # node_cumsum\n",
       "                   %\"cumsum\"<INT64,[1,s53]> ⬅️ ::CumSum(%\"convert_element_type_default\", %\"val_35\"{1}) {exclusive=0, reverse=0}\n",
       "             12 |  # node_type_as\n",
       "                   %\"type_as\"<INT32,[1,s53]> ⬅️ ::Cast(%\"cumsum\") {to=6}\n",
       "             13 |  # node_mul_34\n",
       "                   %\"mul_34\"<INT32,[s43,s53]> ⬅️ ::Mul(%\"type_as\", %\"_to_copy_1\")\n",
       "             14 |  # node__to_copy_2\n",
       "                   %\"_to_copy_2\"<INT64,[s43,s53]> ⬅️ ::Cast(%\"mul_34\") {to=7}\n",
       "             15 |  # node_add_45\n",
       "                   %\"add_45\"<INT64,[1,s53]> ⬅️ ::Add(%\"_to_copy_2\", %\"val_35\"{1})\n",
       "             16 |  # node_embedding\n",
       "                   %\"embedding\"<FLOAT16,[s43,s53,768]> ⬅️ ::Gather(%\"embeddings.word_embeddings.weight\"{...}, %\"input_ids\") {axis=0}\n",
       "             17 |  # node_embedding_1\n",
       "                   %\"embedding_1\"<FLOAT16,[1,s53,768]> ⬅️ ::Gather(%\"embeddings.position_embeddings.weight\"{...}, %\"add_45\") {axis=0}\n",
       "             18 |  # node_add_55\n",
       "                   %\"add_55\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"embedding\", %\"embedding_1\")\n",
       "             19 |  # node_layer_norm\n",
       "                   %\"layer_norm\"<FLOAT16,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_55\", %\"embeddings.LayerNorm.weight\"{...}, %\"embeddings.LayerNorm.bias\"{...}) {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}\n",
       "             20 |  # node_arange\n",
       "                   %\"arange\"<INT64,[s53]> ⬅️ ::Range(%\"val_34\"{0}, %\"sym_size_int_86\", %\"val_35\"{1})\n",
       "             21 |  # node_unsqueeze_2\n",
       "                   %\"unsqueeze_2\"<INT64,[s53,1]> ⬅️ ::Unsqueeze(%\"arange\", %\"val_14\"{[1]})\n",
       "             22 |  # node_unsqueeze_3\n",
       "                   %\"unsqueeze_3\"<INT64,[1,s53]> ⬅️ ::Unsqueeze(%\"arange\", %\"val_49\"{[0]})\n",
       "             23 |  # node_sub_37\n",
       "                   %\"sub_37\"<INT64,[s53,s53]> ⬅️ ::Sub(%\"unsqueeze_3\", %\"unsqueeze_2\")\n",
       "             24 |  # node_neg\n",
       "                   %\"neg\"<INT64,[s53,s53]> ⬅️ ::Neg(%\"sub_37\")\n",
       "             25 |  # node_lt\n",
       "                   %\"lt\"<BOOL,[s53,s53]> ⬅️ ::Less(%\"neg\", %\"val_34\"{0})\n",
       "             26 |  # node__to_copy_3\n",
       "                   %\"_to_copy_3\"<INT64,[s53,s53]> ⬅️ ::Cast(%\"lt\") {to=7}\n",
       "             27 |  # node_mul_71\n",
       "                   %\"mul_71\"<INT64,[s53,s53]> ⬅️ ::Mul(%\"_to_copy_3\", %\"val_60\"{16})\n",
       "             28 |  # node_abs_1\n",
       "                   %\"abs_1\"<INT64,[s53,s53]> ⬅️ ::Abs(%\"neg\")\n",
       "             29 |  # node_lt_1\n",
       "                   %\"lt_1\"<BOOL,[s53,s53]> ⬅️ ::Less(%\"abs_1\", %\"val_61\"{8})\n",
       "             30 |  # node__to_copy_4\n",
       "                   %\"_to_copy_4\"<FLOAT,[s53,s53]> ⬅️ ::Cast(%\"abs_1\") {to=1}\n",
       "             31 |  # node_div\n",
       "                   %\"div\"<FLOAT,[s53,s53]> ⬅️ ::Div(%\"_to_copy_4\", %\"scalar_tensor_default_2\"{8.0})\n",
       "             32 |  # node_log\n",
       "                   %\"log\"<FLOAT,[s53,s53]> ⬅️ ::Log(%\"div\")\n",
       "             33 |  # node_div_1\n",
       "                   %\"div_1\"<FLOAT,[s53,s53]> ⬅️ ::Div(%\"log\", %\"val_62\"{2.7725887298583984})\n",
       "             34 |  # node_mul_87\n",
       "                   %\"mul_87\"<FLOAT,[s53,s53]> ⬅️ ::Mul(%\"div_1\", %\"scalar_tensor_default_2\"{8.0})\n",
       "             35 |  # node__to_copy_5\n",
       "                   %\"_to_copy_5\"<INT64,[s53,s53]> ⬅️ ::Cast(%\"mul_87\") {to=7}\n",
       "             36 |  # node_add_121\n",
       "                   %\"add_121\"<INT64,[s53,s53]> ⬅️ ::Add(%\"_to_copy_5\", %\"val_61\"{8})\n",
       "             37 |  # node_Shape_63\n",
       "                   %\"val_65\"<INT64,[2]> ⬅️ ::Shape(%\"add_121\") {start=0}\n",
       "             38 |  # node_full_like\n",
       "                   %\"full_like\"<INT64,[s53,s53]> ⬅️ ::Expand(%\"val_64\"{15}, %\"val_65\")\n",
       "             39 |  # node_minimum\n",
       "                   %\"minimum\"<INT64,[s53,s53]> ⬅️ ::Min(%\"add_121\", %\"full_like\")\n",
       "             40 |  # node_where\n",
       "                   %\"where\"<INT64,[s53,s53]> ⬅️ ::Where(%\"lt_1\", %\"abs_1\", %\"minimum\")\n",
       "             41 |  # node_add_140\n",
       "                   %\"add_140\"<INT64,[s53,s53]> ⬅️ ::Add(%\"mul_71\", %\"where\")\n",
       "             42 |  # node_embedding_2\n",
       "                   %\"embedding_2\"<FLOAT16,[s53,s53,12]> ⬅️ ::Gather(%\"encoder.relative_attention_bias.weight\"{...}, %\"add_140\") {axis=0}\n",
       "             43 |  # node_permute\n",
       "                   %\"permute\"<FLOAT16,[12,s53,s53]> ⬅️ ::Transpose(%\"embedding_2\") {perm=(2, 0, 1)}\n",
       "             44 |  # node_unsqueeze_4\n",
       "                   %\"unsqueeze_4\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::Unsqueeze(%\"permute\", %\"val_49\"{[0]})\n",
       "             45 |  # node_Concat_69\n",
       "                   %\"val_71\"<INT64,[4]> ⬅️ ::Concat(%\"val_0\", %\"val_14\"{[1]}, %\"val_1\", %\"val_1\") {axis=0}\n",
       "             46 |  # node_expand\n",
       "                   %\"expand\"<FLOAT16,[s43,12,s53,s53]> ⬅️ ::Expand(%\"unsqueeze_4\", %\"val_71\")\n",
       "             47 |  # node_MatMul_71\n",
       "                   %\"val_73\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm\", %\"val_72\"{...})\n",
       "             48 |  # node_linear\n",
       "                   %\"linear\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_73\", %\"encoder.layer.0.attention.attn.q.bias\"{...})\n",
       "             49 |  # node_Concat_77\n",
       "                   %\"val_79\"<INT64,[4]> ⬅️ ::Concat(%\"val_0\", %\"val_76\"{[-1]}, %\"val_77\"{[12]}, %\"val_78\"{[64]}) {axis=0}\n",
       "             50 |  # node_view\n",
       "                   %\"view\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear\", %\"val_79\") {allowzero=1}\n",
       "             51 |  # node_transpose\n",
       "                   %\"transpose\"<FLOAT16,[1,12,s53,64]> ⬅️ ::Transpose(%\"view\") {perm=(0, 2, 1, 3)}\n",
       "             52 |  # node_MatMul_79\n",
       "                   %\"val_81\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm\", %\"val_80\"{...})\n",
       "             53 |  # node_linear_1\n",
       "                   %\"linear_1\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_81\", %\"encoder.layer.0.attention.attn.k.bias\"{...})\n",
       "             54 |  # node_view_1\n",
       "                   %\"view_1\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_1\", %\"val_79\") {allowzero=1}\n",
       "             55 |  # node_MatMul_87\n",
       "                   %\"val_89\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm\", %\"val_88\"{...})\n",
       "             56 |  # node_linear_2\n",
       "                   %\"linear_2\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_89\", %\"encoder.layer.0.attention.attn.v.bias\"{...})\n",
       "             57 |  # node_view_2\n",
       "                   %\"view_2\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_2\", %\"val_79\") {allowzero=1}\n",
       "             58 |  # node_transpose_2\n",
       "                   %\"transpose_2\"<FLOAT16,[1,12,s53,64]> ⬅️ ::Transpose(%\"view_2\") {perm=(0, 2, 1, 3)}\n",
       "             59 |  # node_Transpose_35\n",
       "                   %\"transpose_3\"<FLOAT16,[1,12,64,s53]> ⬅️ ::Transpose(%\"view_1\") {perm=(0, 2, 3, 1)}\n",
       "             60 |  # node_matmul\n",
       "                   %\"matmul\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::MatMul(%\"transpose\", %\"transpose_3\")\n",
       "             61 |  # node_div_2\n",
       "                   %\"div_2\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::Div(%\"matmul\", %\"scalar_tensor_default_4\"{8.0})\n",
       "             62 |  # node_add_213\n",
       "                   %\"add_213\"<FLOAT16,[s43,12,s53,s53]> ⬅️ ::Add(%\"div_2\", %\"expand\")\n",
       "             63 |  # node_add_219\n",
       "                   %\"add_219\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::Add(%\"add_213\", %\"mul_22\")\n",
       "             64 |  # node_softmax\n",
       "                   %\"softmax\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::Softmax(%\"add_219\") {axis=-1}\n",
       "             65 |  # node_matmul_1\n",
       "                   %\"matmul_1\"<FLOAT16,[1,12,s53,64]> ⬅️ ::MatMul(%\"softmax\", %\"transpose_2\")\n",
       "             66 |  # node_permute_1\n",
       "                   %\"permute_1\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Transpose(%\"matmul_1\") {perm=(0, 2, 1, 3)}\n",
       "             67 |  # node_Concat_99\n",
       "                   %\"val_101\"<INT64,[3]> ⬅️ ::Concat(%\"val_0\", %\"val_1\", %\"val_100\"{[768]}) {axis=0}\n",
       "             68 |  # node_view_3\n",
       "                   %\"view_3\"<FLOAT16,[1,s53,768]> ⬅️ ::Reshape(%\"permute_1\", %\"val_101\") {allowzero=1}\n",
       "             69 |  # node_MatMul_101\n",
       "                   %\"val_103\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"view_3\", %\"val_102\"{...})\n",
       "             70 |  # node_linear_3\n",
       "                   %\"linear_3\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_103\", %\"encoder.layer.0.attention.attn.o.bias\"{...})\n",
       "             71 |  # node_add_253\n",
       "                   %\"add_253\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"linear_3\", %\"layer_norm\")\n",
       "             72 |  # node_layer_norm_1\n",
       "                   %\"layer_norm_1\"<FLOAT16,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_253\", %\"encoder.layer.0.attention.LayerNorm.weight\"{...}, %\"encoder.layer.0.attention.LayerNorm.bias\"{...}) {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}\n",
       "             73 |  # node_MatMul_103\n",
       "                   %\"val_107\"<FLOAT16,[1,s53,3072]> ⬅️ ::MatMul(%\"layer_norm_1\", %\"val_106\"{...})\n",
       "             74 |  # node_linear_4\n",
       "                   %\"linear_4\"<FLOAT16,[1,s53,3072]> ⬅️ ::Add(%\"val_107\", %\"encoder.layer.0.intermediate.dense.bias\"{...})\n",
       "             75 |  # node_Div_105\n",
       "                   %\"val_109\"<FLOAT16,[1,s53,3072]> ⬅️ ::Div(%\"linear_4\", %\"val_108\"{1.4140625})\n",
       "             76 |  # node_Erf_106\n",
       "                   %\"val_110\"<FLOAT16,[1,s53,3072]> ⬅️ ::Erf(%\"val_109\")\n",
       "             77 |  # node_Add_108\n",
       "                   %\"val_112\"<FLOAT16,[1,s53,3072]> ⬅️ ::Add(%\"val_110\", %\"scalar_tensor_default\"{1.0})\n",
       "             78 |  # node_Mul_110\n",
       "                   %\"val_114\"<FLOAT16,[1,s53,3072]> ⬅️ ::Mul(%\"val_113\"{0.5}, %\"val_112\")\n",
       "             79 |  # node_gelu\n",
       "                   %\"gelu\"<FLOAT16,[1,s53,3072]> ⬅️ ::Mul(%\"linear_4\", %\"val_114\")\n",
       "             80 |  # node_MatMul_112\n",
       "                   %\"val_116\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"gelu\", %\"val_115\"{...})\n",
       "             81 |  # node_linear_5\n",
       "                   %\"linear_5\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_116\", %\"encoder.layer.0.output.dense.bias\"{...})\n",
       "             82 |  # node_add_272\n",
       "                   %\"add_272\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"linear_5\", %\"layer_norm_1\")\n",
       "             83 |  # node_layer_norm_2\n",
       "                   %\"layer_norm_2\"<FLOAT16,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_272\", %\"encoder.layer.0.output.LayerNorm.weight\"{...}, %\"encoder.layer.0.output.LayerNorm.bias\"{...}) {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}\n",
       "             84 |  # node_MatMul_114\n",
       "                   %\"val_120\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_2\", %\"val_119\"{...})\n",
       "             85 |  # node_linear_6\n",
       "                   %\"linear_6\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_120\", %\"encoder.layer.1.attention.attn.q.bias\"{...})\n",
       "             86 |  # node_view_4\n",
       "                   %\"view_4\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_6\", %\"val_79\") {allowzero=1}\n",
       "             87 |  # node_transpose_4\n",
       "                   %\"transpose_4\"<FLOAT16,[1,12,s53,64]> ⬅️ ::Transpose(%\"view_4\") {perm=(0, 2, 1, 3)}\n",
       "             88 |  # node_MatMul_122\n",
       "                   %\"val_128\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_2\", %\"val_127\"{...})\n",
       "             89 |  # node_linear_7\n",
       "                   %\"linear_7\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_128\", %\"encoder.layer.1.attention.attn.k.bias\"{...})\n",
       "             90 |  # node_view_5\n",
       "                   %\"view_5\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_7\", %\"val_79\") {allowzero=1}\n",
       "             91 |  # node_MatMul_130\n",
       "                   %\"val_136\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_2\", %\"val_135\"{...})\n",
       "             92 |  # node_linear_8\n",
       "                   %\"linear_8\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_136\", %\"encoder.layer.1.attention.attn.v.bias\"{...})\n",
       "             93 |  # node_view_6\n",
       "                   %\"view_6\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_8\", %\"val_79\") {allowzero=1}\n",
       "             94 |  # node_transpose_6\n",
       "                   %\"transpose_6\"<FLOAT16,[1,12,s53,64]> ⬅️ ::Transpose(%\"view_6\") {perm=(0, 2, 1, 3)}\n",
       "             95 |  # node_Transpose_41\n",
       "                   %\"transpose_7\"<FLOAT16,[1,12,64,s53]> ⬅️ ::Transpose(%\"view_5\") {perm=(0, 2, 3, 1)}\n",
       "             96 |  # node_matmul_2\n",
       "                   %\"matmul_2\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::MatMul(%\"transpose_4\", %\"transpose_7\")\n",
       "             97 |  # node_div_3\n",
       "                   %\"div_3\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::Div(%\"matmul_2\", %\"scalar_tensor_default_4\"{8.0})\n",
       "             98 |  # node_add_328\n",
       "                   %\"add_328\"<FLOAT16,[s43,12,s53,s53]> ⬅️ ::Add(%\"div_3\", %\"expand\")\n",
       "             99 |  # node_add_334\n",
       "                   %\"add_334\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::Add(%\"add_328\", %\"mul_22\")\n",
       "            100 |  # node_softmax_1\n",
       "                   %\"softmax_1\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::Softmax(%\"add_334\") {axis=-1}\n",
       "            101 |  # node_matmul_3\n",
       "                   %\"matmul_3\"<FLOAT16,[1,12,s53,64]> ⬅️ ::MatMul(%\"softmax_1\", %\"transpose_6\")\n",
       "            102 |  # node_permute_2\n",
       "                   %\"permute_2\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Transpose(%\"matmul_3\") {perm=(0, 2, 1, 3)}\n",
       "            103 |  # node_view_7\n",
       "                   %\"view_7\"<FLOAT16,[1,s53,768]> ⬅️ ::Reshape(%\"permute_2\", %\"val_101\") {allowzero=1}\n",
       "            104 |  # node_MatMul_143\n",
       "                   %\"val_149\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"view_7\", %\"val_148\"{...})\n",
       "            105 |  # node_linear_9\n",
       "                   %\"linear_9\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_149\", %\"encoder.layer.1.attention.attn.o.bias\"{...})\n",
       "            106 |  # node_add_368\n",
       "                   %\"add_368\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"linear_9\", %\"layer_norm_2\")\n",
       "            107 |  # node_layer_norm_3\n",
       "                   %\"layer_norm_3\"<FLOAT16,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_368\", %\"encoder.layer.1.attention.LayerNorm.weight\"{...}, %\"encoder.layer.1.attention.LayerNorm.bias\"{...}) {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}\n",
       "            108 |  # node_MatMul_145\n",
       "                   %\"val_153\"<FLOAT16,[1,s53,3072]> ⬅️ ::MatMul(%\"layer_norm_3\", %\"val_152\"{...})\n",
       "            109 |  # node_linear_10\n",
       "                   %\"linear_10\"<FLOAT16,[1,s53,3072]> ⬅️ ::Add(%\"val_153\", %\"encoder.layer.1.intermediate.dense.bias\"{...})\n",
       "            110 |  # node_Div_147\n",
       "                   %\"val_155\"<FLOAT16,[1,s53,3072]> ⬅️ ::Div(%\"linear_10\", %\"val_108\"{1.4140625})\n",
       "            111 |  # node_Erf_148\n",
       "                   %\"val_156\"<FLOAT16,[1,s53,3072]> ⬅️ ::Erf(%\"val_155\")\n",
       "            112 |  # node_Add_150\n",
       "                   %\"val_158\"<FLOAT16,[1,s53,3072]> ⬅️ ::Add(%\"val_156\", %\"scalar_tensor_default\"{1.0})\n",
       "            113 |  # node_Mul_152\n",
       "                   %\"val_160\"<FLOAT16,[1,s53,3072]> ⬅️ ::Mul(%\"val_113\"{0.5}, %\"val_158\")\n",
       "            114 |  # node_gelu_1\n",
       "                   %\"gelu_1\"<FLOAT16,[1,s53,3072]> ⬅️ ::Mul(%\"linear_10\", %\"val_160\")\n",
       "            115 |  # node_MatMul_154\n",
       "                   %\"val_162\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"gelu_1\", %\"val_161\"{...})\n",
       "            116 |  # node_linear_11\n",
       "                   %\"linear_11\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_162\", %\"encoder.layer.1.output.dense.bias\"{...})\n",
       "            117 |  # node_add_387\n",
       "                   %\"add_387\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"linear_11\", %\"layer_norm_3\")\n",
       "            118 |  # node_layer_norm_4\n",
       "                   %\"layer_norm_4\"<FLOAT16,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_387\", %\"encoder.layer.1.output.LayerNorm.weight\"{...}, %\"encoder.layer.1.output.LayerNorm.bias\"{...}) {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}\n",
       "            119 |  # node_MatMul_156\n",
       "                   %\"val_166\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_4\", %\"val_165\"{...})\n",
       "            120 |  # node_linear_12\n",
       "                   %\"linear_12\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_166\", %\"encoder.layer.2.attention.attn.q.bias\"{...})\n",
       "            121 |  # node_view_8\n",
       "                   %\"view_8\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_12\", %\"val_79\") {allowzero=1}\n",
       "            122 |  # node_transpose_8\n",
       "                   %\"transpose_8\"<FLOAT16,[1,12,s53,64]> ⬅️ ::Transpose(%\"view_8\") {perm=(0, 2, 1, 3)}\n",
       "            123 |  # node_MatMul_164\n",
       "                   %\"val_174\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_4\", %\"val_173\"{...})\n",
       "            124 |  # node_linear_13\n",
       "                   %\"linear_13\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_174\", %\"encoder.layer.2.attention.attn.k.bias\"{...})\n",
       "            125 |  # node_view_9\n",
       "                   %\"view_9\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_13\", %\"val_79\") {allowzero=1}\n",
       "            126 |  # node_MatMul_172\n",
       "                   %\"val_182\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_4\", %\"val_181\"{...})\n",
       "            127 |  # node_linear_14\n",
       "                   %\"linear_14\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_182\", %\"encoder.layer.2.attention.attn.v.bias\"{...})\n",
       "            128 |  # node_view_10\n",
       "                   %\"view_10\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_14\", %\"val_79\") {allowzero=1}\n",
       "            129 |  # node_transpose_10\n",
       "                   %\"transpose_10\"<FLOAT16,[1,12,s53,64]> ⬅️ ::Transpose(%\"view_10\") {perm=(0, 2, 1, 3)}\n",
       "            130 |  # node_Transpose_47\n",
       "                   %\"transpose_11\"<FLOAT16,[1,12,64,s53]> ⬅️ ::Transpose(%\"view_9\") {perm=(0, 2, 3, 1)}\n",
       "            131 |  # node_matmul_4\n",
       "                   %\"matmul_4\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::MatMul(%\"transpose_8\", %\"transpose_11\")\n",
       "            132 |  # node_div_4\n",
       "                   %\"div_4\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::Div(%\"matmul_4\", %\"scalar_tensor_default_4\"{8.0})\n",
       "            133 |  # node_add_443\n",
       "                   %\"add_443\"<FLOAT16,[s43,12,s53,s53]> ⬅️ ::Add(%\"div_4\", %\"expand\")\n",
       "            134 |  # node_add_449\n",
       "                   %\"add_449\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::Add(%\"add_443\", %\"mul_22\")\n",
       "            135 |  # node_softmax_2\n",
       "                   %\"softmax_2\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::Softmax(%\"add_449\") {axis=-1}\n",
       "            136 |  # node_matmul_5\n",
       "                   %\"matmul_5\"<FLOAT16,[1,12,s53,64]> ⬅️ ::MatMul(%\"softmax_2\", %\"transpose_10\")\n",
       "            137 |  # node_permute_3\n",
       "                   %\"permute_3\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Transpose(%\"matmul_5\") {perm=(0, 2, 1, 3)}\n",
       "            138 |  # node_view_11\n",
       "                   %\"view_11\"<FLOAT16,[1,s53,768]> ⬅️ ::Reshape(%\"permute_3\", %\"val_101\") {allowzero=1}\n",
       "            139 |  # node_MatMul_185\n",
       "                   %\"val_195\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"view_11\", %\"val_194\"{...})\n",
       "            140 |  # node_linear_15\n",
       "                   %\"linear_15\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_195\", %\"encoder.layer.2.attention.attn.o.bias\"{...})\n",
       "            141 |  # node_add_483\n",
       "                   %\"add_483\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"linear_15\", %\"layer_norm_4\")\n",
       "            142 |  # node_layer_norm_5\n",
       "                   %\"layer_norm_5\"<FLOAT16,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_483\", %\"encoder.layer.2.attention.LayerNorm.weight\"{...}, %\"encoder.layer.2.attention.LayerNorm.bias\"{...}) {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}\n",
       "            143 |  # node_MatMul_187\n",
       "                   %\"val_199\"<FLOAT16,[1,s53,3072]> ⬅️ ::MatMul(%\"layer_norm_5\", %\"val_198\"{...})\n",
       "            144 |  # node_linear_16\n",
       "                   %\"linear_16\"<FLOAT16,[1,s53,3072]> ⬅️ ::Add(%\"val_199\", %\"encoder.layer.2.intermediate.dense.bias\"{...})\n",
       "            145 |  # node_Div_189\n",
       "                   %\"val_201\"<FLOAT16,[1,s53,3072]> ⬅️ ::Div(%\"linear_16\", %\"val_108\"{1.4140625})\n",
       "            146 |  # node_Erf_190\n",
       "                   %\"val_202\"<FLOAT16,[1,s53,3072]> ⬅️ ::Erf(%\"val_201\")\n",
       "            147 |  # node_Add_192\n",
       "                   %\"val_204\"<FLOAT16,[1,s53,3072]> ⬅️ ::Add(%\"val_202\", %\"scalar_tensor_default\"{1.0})\n",
       "            148 |  # node_Mul_194\n",
       "                   %\"val_206\"<FLOAT16,[1,s53,3072]> ⬅️ ::Mul(%\"val_113\"{0.5}, %\"val_204\")\n",
       "            149 |  # node_gelu_2\n",
       "                   %\"gelu_2\"<FLOAT16,[1,s53,3072]> ⬅️ ::Mul(%\"linear_16\", %\"val_206\")\n",
       "            150 |  # node_MatMul_196\n",
       "                   %\"val_208\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"gelu_2\", %\"val_207\"{...})\n",
       "            151 |  # node_linear_17\n",
       "                   %\"linear_17\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_208\", %\"encoder.layer.2.output.dense.bias\"{...})\n",
       "            152 |  # node_add_502\n",
       "                   %\"add_502\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"linear_17\", %\"layer_norm_5\")\n",
       "            153 |  # node_layer_norm_6\n",
       "                   %\"layer_norm_6\"<FLOAT16,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_502\", %\"encoder.layer.2.output.LayerNorm.weight\"{...}, %\"encoder.layer.2.output.LayerNorm.bias\"{...}) {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}\n",
       "            154 |  # node_MatMul_198\n",
       "                   %\"val_212\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_6\", %\"val_211\"{...})\n",
       "            155 |  # node_linear_18\n",
       "                   %\"linear_18\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_212\", %\"encoder.layer.3.attention.attn.q.bias\"{...})\n",
       "            156 |  # node_view_12\n",
       "                   %\"view_12\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_18\", %\"val_79\") {allowzero=1}\n",
       "            157 |  # node_transpose_12\n",
       "                   %\"transpose_12\"<FLOAT16,[1,12,s53,64]> ⬅️ ::Transpose(%\"view_12\") {perm=(0, 2, 1, 3)}\n",
       "            158 |  # node_MatMul_206\n",
       "                   %\"val_220\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_6\", %\"val_219\"{...})\n",
       "            159 |  # node_linear_19\n",
       "                   %\"linear_19\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_220\", %\"encoder.layer.3.attention.attn.k.bias\"{...})\n",
       "            160 |  # node_view_13\n",
       "                   %\"view_13\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_19\", %\"val_79\") {allowzero=1}\n",
       "            161 |  # node_MatMul_214\n",
       "                   %\"val_228\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_6\", %\"val_227\"{...})\n",
       "            162 |  # node_linear_20\n",
       "                   %\"linear_20\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_228\", %\"encoder.layer.3.attention.attn.v.bias\"{...})\n",
       "            163 |  # node_view_14\n",
       "                   %\"view_14\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_20\", %\"val_79\") {allowzero=1}\n",
       "            164 |  # node_transpose_14\n",
       "                   %\"transpose_14\"<FLOAT16,[1,12,s53,64]> ⬅️ ::Transpose(%\"view_14\") {perm=(0, 2, 1, 3)}\n",
       "            165 |  # node_Transpose_53\n",
       "                   %\"transpose_15\"<FLOAT16,[1,12,64,s53]> ⬅️ ::Transpose(%\"view_13\") {perm=(0, 2, 3, 1)}\n",
       "            166 |  # node_matmul_6\n",
       "                   %\"matmul_6\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::MatMul(%\"transpose_12\", %\"transpose_15\")\n",
       "            167 |  # node_div_5\n",
       "                   %\"div_5\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::Div(%\"matmul_6\", %\"scalar_tensor_default_4\"{8.0})\n",
       "            168 |  # node_add_558\n",
       "                   %\"add_558\"<FLOAT16,[s43,12,s53,s53]> ⬅️ ::Add(%\"div_5\", %\"expand\")\n",
       "            169 |  # node_add_564\n",
       "                   %\"add_564\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::Add(%\"add_558\", %\"mul_22\")\n",
       "            170 |  # node_softmax_3\n",
       "                   %\"softmax_3\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::Softmax(%\"add_564\") {axis=-1}\n",
       "            171 |  # node_matmul_7\n",
       "                   %\"matmul_7\"<FLOAT16,[1,12,s53,64]> ⬅️ ::MatMul(%\"softmax_3\", %\"transpose_14\")\n",
       "            172 |  # node_permute_4\n",
       "                   %\"permute_4\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Transpose(%\"matmul_7\") {perm=(0, 2, 1, 3)}\n",
       "            173 |  # node_view_15\n",
       "                   %\"view_15\"<FLOAT16,[1,s53,768]> ⬅️ ::Reshape(%\"permute_4\", %\"val_101\") {allowzero=1}\n",
       "            174 |  # node_MatMul_227\n",
       "                   %\"val_241\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"view_15\", %\"val_240\"{...})\n",
       "            175 |  # node_linear_21\n",
       "                   %\"linear_21\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_241\", %\"encoder.layer.3.attention.attn.o.bias\"{...})\n",
       "            176 |  # node_add_598\n",
       "                   %\"add_598\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"linear_21\", %\"layer_norm_6\")\n",
       "            177 |  # node_layer_norm_7\n",
       "                   %\"layer_norm_7\"<FLOAT16,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_598\", %\"encoder.layer.3.attention.LayerNorm.weight\"{...}, %\"encoder.layer.3.attention.LayerNorm.bias\"{...}) {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}\n",
       "            178 |  # node_MatMul_229\n",
       "                   %\"val_245\"<FLOAT16,[1,s53,3072]> ⬅️ ::MatMul(%\"layer_norm_7\", %\"val_244\"{...})\n",
       "            179 |  # node_linear_22\n",
       "                   %\"linear_22\"<FLOAT16,[1,s53,3072]> ⬅️ ::Add(%\"val_245\", %\"encoder.layer.3.intermediate.dense.bias\"{...})\n",
       "            180 |  # node_Div_231\n",
       "                   %\"val_247\"<FLOAT16,[1,s53,3072]> ⬅️ ::Div(%\"linear_22\", %\"val_108\"{1.4140625})\n",
       "            181 |  # node_Erf_232\n",
       "                   %\"val_248\"<FLOAT16,[1,s53,3072]> ⬅️ ::Erf(%\"val_247\")\n",
       "            182 |  # node_Add_234\n",
       "                   %\"val_250\"<FLOAT16,[1,s53,3072]> ⬅️ ::Add(%\"val_248\", %\"scalar_tensor_default\"{1.0})\n",
       "            183 |  # node_Mul_236\n",
       "                   %\"val_252\"<FLOAT16,[1,s53,3072]> ⬅️ ::Mul(%\"val_113\"{0.5}, %\"val_250\")\n",
       "            184 |  # node_gelu_3\n",
       "                   %\"gelu_3\"<FLOAT16,[1,s53,3072]> ⬅️ ::Mul(%\"linear_22\", %\"val_252\")\n",
       "            185 |  # node_MatMul_238\n",
       "                   %\"val_254\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"gelu_3\", %\"val_253\"{...})\n",
       "            186 |  # node_linear_23\n",
       "                   %\"linear_23\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_254\", %\"encoder.layer.3.output.dense.bias\"{...})\n",
       "            187 |  # node_add_617\n",
       "                   %\"add_617\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"linear_23\", %\"layer_norm_7\")\n",
       "            188 |  # node_layer_norm_8\n",
       "                   %\"layer_norm_8\"<FLOAT16,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_617\", %\"encoder.layer.3.output.LayerNorm.weight\"{...}, %\"encoder.layer.3.output.LayerNorm.bias\"{...}) {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}\n",
       "            189 |  # node_MatMul_240\n",
       "                   %\"val_258\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_8\", %\"val_257\"{...})\n",
       "            190 |  # node_linear_24\n",
       "                   %\"linear_24\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_258\", %\"encoder.layer.4.attention.attn.q.bias\"{...})\n",
       "            191 |  # node_view_16\n",
       "                   %\"view_16\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_24\", %\"val_79\") {allowzero=1}\n",
       "            192 |  # node_transpose_16\n",
       "                   %\"transpose_16\"<FLOAT16,[1,12,s53,64]> ⬅️ ::Transpose(%\"view_16\") {perm=(0, 2, 1, 3)}\n",
       "            193 |  # node_MatMul_248\n",
       "                   %\"val_266\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_8\", %\"val_265\"{...})\n",
       "            194 |  # node_linear_25\n",
       "                   %\"linear_25\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_266\", %\"encoder.layer.4.attention.attn.k.bias\"{...})\n",
       "            195 |  # node_view_17\n",
       "                   %\"view_17\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_25\", %\"val_79\") {allowzero=1}\n",
       "            196 |  # node_MatMul_256\n",
       "                   %\"val_274\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_8\", %\"val_273\"{...})\n",
       "            197 |  # node_linear_26\n",
       "                   %\"linear_26\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_274\", %\"encoder.layer.4.attention.attn.v.bias\"{...})\n",
       "            198 |  # node_view_18\n",
       "                   %\"view_18\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_26\", %\"val_79\") {allowzero=1}\n",
       "            199 |  # node_transpose_18\n",
       "                   %\"transpose_18\"<FLOAT16,[1,12,s53,64]> ⬅️ ::Transpose(%\"view_18\") {perm=(0, 2, 1, 3)}\n",
       "            200 |  # node_Transpose_59\n",
       "                   %\"transpose_19\"<FLOAT16,[1,12,64,s53]> ⬅️ ::Transpose(%\"view_17\") {perm=(0, 2, 3, 1)}\n",
       "            201 |  # node_matmul_8\n",
       "                   %\"matmul_8\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::MatMul(%\"transpose_16\", %\"transpose_19\")\n",
       "            202 |  # node_div_6\n",
       "                   %\"div_6\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::Div(%\"matmul_8\", %\"scalar_tensor_default_4\"{8.0})\n",
       "            203 |  # node_add_673\n",
       "                   %\"add_673\"<FLOAT16,[s43,12,s53,s53]> ⬅️ ::Add(%\"div_6\", %\"expand\")\n",
       "            204 |  # node_add_679\n",
       "                   %\"add_679\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::Add(%\"add_673\", %\"mul_22\")\n",
       "            205 |  # node_softmax_4\n",
       "                   %\"softmax_4\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::Softmax(%\"add_679\") {axis=-1}\n",
       "            206 |  # node_matmul_9\n",
       "                   %\"matmul_9\"<FLOAT16,[1,12,s53,64]> ⬅️ ::MatMul(%\"softmax_4\", %\"transpose_18\")\n",
       "            207 |  # node_permute_5\n",
       "                   %\"permute_5\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Transpose(%\"matmul_9\") {perm=(0, 2, 1, 3)}\n",
       "            208 |  # node_view_19\n",
       "                   %\"view_19\"<FLOAT16,[1,s53,768]> ⬅️ ::Reshape(%\"permute_5\", %\"val_101\") {allowzero=1}\n",
       "            209 |  # node_MatMul_269\n",
       "                   %\"val_287\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"view_19\", %\"val_286\"{...})\n",
       "            210 |  # node_linear_27\n",
       "                   %\"linear_27\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_287\", %\"encoder.layer.4.attention.attn.o.bias\"{...})\n",
       "            211 |  # node_add_713\n",
       "                   %\"add_713\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"linear_27\", %\"layer_norm_8\")\n",
       "            212 |  # node_layer_norm_9\n",
       "                   %\"layer_norm_9\"<FLOAT16,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_713\", %\"encoder.layer.4.attention.LayerNorm.weight\"{...}, %\"encoder.layer.4.attention.LayerNorm.bias\"{...}) {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}\n",
       "            213 |  # node_MatMul_271\n",
       "                   %\"val_291\"<FLOAT16,[1,s53,3072]> ⬅️ ::MatMul(%\"layer_norm_9\", %\"val_290\"{...})\n",
       "            214 |  # node_linear_28\n",
       "                   %\"linear_28\"<FLOAT16,[1,s53,3072]> ⬅️ ::Add(%\"val_291\", %\"encoder.layer.4.intermediate.dense.bias\"{...})\n",
       "            215 |  # node_Div_273\n",
       "                   %\"val_293\"<FLOAT16,[1,s53,3072]> ⬅️ ::Div(%\"linear_28\", %\"val_108\"{1.4140625})\n",
       "            216 |  # node_Erf_274\n",
       "                   %\"val_294\"<FLOAT16,[1,s53,3072]> ⬅️ ::Erf(%\"val_293\")\n",
       "            217 |  # node_Add_276\n",
       "                   %\"val_296\"<FLOAT16,[1,s53,3072]> ⬅️ ::Add(%\"val_294\", %\"scalar_tensor_default\"{1.0})\n",
       "            218 |  # node_Mul_278\n",
       "                   %\"val_298\"<FLOAT16,[1,s53,3072]> ⬅️ ::Mul(%\"val_113\"{0.5}, %\"val_296\")\n",
       "            219 |  # node_gelu_4\n",
       "                   %\"gelu_4\"<FLOAT16,[1,s53,3072]> ⬅️ ::Mul(%\"linear_28\", %\"val_298\")\n",
       "            220 |  # node_MatMul_280\n",
       "                   %\"val_300\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"gelu_4\", %\"val_299\"{...})\n",
       "            221 |  # node_linear_29\n",
       "                   %\"linear_29\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_300\", %\"encoder.layer.4.output.dense.bias\"{...})\n",
       "            222 |  # node_add_732\n",
       "                   %\"add_732\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"linear_29\", %\"layer_norm_9\")\n",
       "            223 |  # node_layer_norm_10\n",
       "                   %\"layer_norm_10\"<FLOAT16,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_732\", %\"encoder.layer.4.output.LayerNorm.weight\"{...}, %\"encoder.layer.4.output.LayerNorm.bias\"{...}) {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}\n",
       "            224 |  # node_MatMul_282\n",
       "                   %\"val_304\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_10\", %\"val_303\"{...})\n",
       "            225 |  # node_linear_30\n",
       "                   %\"linear_30\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_304\", %\"encoder.layer.5.attention.attn.q.bias\"{...})\n",
       "            226 |  # node_view_20\n",
       "                   %\"view_20\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_30\", %\"val_79\") {allowzero=1}\n",
       "            227 |  # node_transpose_20\n",
       "                   %\"transpose_20\"<FLOAT16,[1,12,s53,64]> ⬅️ ::Transpose(%\"view_20\") {perm=(0, 2, 1, 3)}\n",
       "            228 |  # node_MatMul_290\n",
       "                   %\"val_312\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_10\", %\"val_311\"{...})\n",
       "            229 |  # node_linear_31\n",
       "                   %\"linear_31\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_312\", %\"encoder.layer.5.attention.attn.k.bias\"{...})\n",
       "            230 |  # node_view_21\n",
       "                   %\"view_21\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_31\", %\"val_79\") {allowzero=1}\n",
       "            231 |  # node_MatMul_298\n",
       "                   %\"val_320\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_10\", %\"val_319\"{...})\n",
       "            232 |  # node_linear_32\n",
       "                   %\"linear_32\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_320\", %\"encoder.layer.5.attention.attn.v.bias\"{...})\n",
       "            233 |  # node_view_22\n",
       "                   %\"view_22\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_32\", %\"val_79\") {allowzero=1}\n",
       "            234 |  # node_transpose_22\n",
       "                   %\"transpose_22\"<FLOAT16,[1,12,s53,64]> ⬅️ ::Transpose(%\"view_22\") {perm=(0, 2, 1, 3)}\n",
       "            235 |  # node_Transpose_65\n",
       "                   %\"transpose_23\"<FLOAT16,[1,12,64,s53]> ⬅️ ::Transpose(%\"view_21\") {perm=(0, 2, 3, 1)}\n",
       "            236 |  # node_matmul_10\n",
       "                   %\"matmul_10\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::MatMul(%\"transpose_20\", %\"transpose_23\")\n",
       "            237 |  # node_div_7\n",
       "                   %\"div_7\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::Div(%\"matmul_10\", %\"scalar_tensor_default_4\"{8.0})\n",
       "            238 |  # node_add_788\n",
       "                   %\"add_788\"<FLOAT16,[s43,12,s53,s53]> ⬅️ ::Add(%\"div_7\", %\"expand\")\n",
       "            239 |  # node_add_794\n",
       "                   %\"add_794\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::Add(%\"add_788\", %\"mul_22\")\n",
       "            240 |  # node_softmax_5\n",
       "                   %\"softmax_5\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::Softmax(%\"add_794\") {axis=-1}\n",
       "            241 |  # node_matmul_11\n",
       "                   %\"matmul_11\"<FLOAT16,[1,12,s53,64]> ⬅️ ::MatMul(%\"softmax_5\", %\"transpose_22\")\n",
       "            242 |  # node_permute_6\n",
       "                   %\"permute_6\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Transpose(%\"matmul_11\") {perm=(0, 2, 1, 3)}\n",
       "            243 |  # node_view_23\n",
       "                   %\"view_23\"<FLOAT16,[1,s53,768]> ⬅️ ::Reshape(%\"permute_6\", %\"val_101\") {allowzero=1}\n",
       "            244 |  # node_MatMul_311\n",
       "                   %\"val_333\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"view_23\", %\"val_332\"{...})\n",
       "            245 |  # node_linear_33\n",
       "                   %\"linear_33\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_333\", %\"encoder.layer.5.attention.attn.o.bias\"{...})\n",
       "            246 |  # node_add_828\n",
       "                   %\"add_828\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"linear_33\", %\"layer_norm_10\")\n",
       "            247 |  # node_layer_norm_11\n",
       "                   %\"layer_norm_11\"<FLOAT16,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_828\", %\"encoder.layer.5.attention.LayerNorm.weight\"{...}, %\"encoder.layer.5.attention.LayerNorm.bias\"{...}) {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}\n",
       "            248 |  # node_MatMul_313\n",
       "                   %\"val_337\"<FLOAT16,[1,s53,3072]> ⬅️ ::MatMul(%\"layer_norm_11\", %\"val_336\"{...})\n",
       "            249 |  # node_linear_34\n",
       "                   %\"linear_34\"<FLOAT16,[1,s53,3072]> ⬅️ ::Add(%\"val_337\", %\"encoder.layer.5.intermediate.dense.bias\"{...})\n",
       "            250 |  # node_Div_315\n",
       "                   %\"val_339\"<FLOAT16,[1,s53,3072]> ⬅️ ::Div(%\"linear_34\", %\"val_108\"{1.4140625})\n",
       "            251 |  # node_Erf_316\n",
       "                   %\"val_340\"<FLOAT16,[1,s53,3072]> ⬅️ ::Erf(%\"val_339\")\n",
       "            252 |  # node_Add_318\n",
       "                   %\"val_342\"<FLOAT16,[1,s53,3072]> ⬅️ ::Add(%\"val_340\", %\"scalar_tensor_default\"{1.0})\n",
       "            253 |  # node_Mul_320\n",
       "                   %\"val_344\"<FLOAT16,[1,s53,3072]> ⬅️ ::Mul(%\"val_113\"{0.5}, %\"val_342\")\n",
       "            254 |  # node_gelu_5\n",
       "                   %\"gelu_5\"<FLOAT16,[1,s53,3072]> ⬅️ ::Mul(%\"linear_34\", %\"val_344\")\n",
       "            255 |  # node_MatMul_322\n",
       "                   %\"val_346\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"gelu_5\", %\"val_345\"{...})\n",
       "            256 |  # node_linear_35\n",
       "                   %\"linear_35\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_346\", %\"encoder.layer.5.output.dense.bias\"{...})\n",
       "            257 |  # node_add_847\n",
       "                   %\"add_847\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"linear_35\", %\"layer_norm_11\")\n",
       "            258 |  # node_layer_norm_12\n",
       "                   %\"layer_norm_12\"<FLOAT16,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_847\", %\"encoder.layer.5.output.LayerNorm.weight\"{...}, %\"encoder.layer.5.output.LayerNorm.bias\"{...}) {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}\n",
       "            259 |  # node_MatMul_324\n",
       "                   %\"val_350\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_12\", %\"val_349\"{...})\n",
       "            260 |  # node_linear_36\n",
       "                   %\"linear_36\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_350\", %\"encoder.layer.6.attention.attn.q.bias\"{...})\n",
       "            261 |  # node_view_24\n",
       "                   %\"view_24\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_36\", %\"val_79\") {allowzero=1}\n",
       "            262 |  # node_transpose_24\n",
       "                   %\"transpose_24\"<FLOAT16,[1,12,s53,64]> ⬅️ ::Transpose(%\"view_24\") {perm=(0, 2, 1, 3)}\n",
       "            263 |  # node_MatMul_332\n",
       "                   %\"val_358\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_12\", %\"val_357\"{...})\n",
       "            264 |  # node_linear_37\n",
       "                   %\"linear_37\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_358\", %\"encoder.layer.6.attention.attn.k.bias\"{...})\n",
       "            265 |  # node_view_25\n",
       "                   %\"view_25\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_37\", %\"val_79\") {allowzero=1}\n",
       "            266 |  # node_MatMul_340\n",
       "                   %\"val_366\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_12\", %\"val_365\"{...})\n",
       "            267 |  # node_linear_38\n",
       "                   %\"linear_38\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_366\", %\"encoder.layer.6.attention.attn.v.bias\"{...})\n",
       "            268 |  # node_view_26\n",
       "                   %\"view_26\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_38\", %\"val_79\") {allowzero=1}\n",
       "            269 |  # node_transpose_26\n",
       "                   %\"transpose_26\"<FLOAT16,[1,12,s53,64]> ⬅️ ::Transpose(%\"view_26\") {perm=(0, 2, 1, 3)}\n",
       "            270 |  # node_Transpose_71\n",
       "                   %\"transpose_27\"<FLOAT16,[1,12,64,s53]> ⬅️ ::Transpose(%\"view_25\") {perm=(0, 2, 3, 1)}\n",
       "            271 |  # node_matmul_12\n",
       "                   %\"matmul_12\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::MatMul(%\"transpose_24\", %\"transpose_27\")\n",
       "            272 |  # node_div_8\n",
       "                   %\"div_8\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::Div(%\"matmul_12\", %\"scalar_tensor_default_4\"{8.0})\n",
       "            273 |  # node_add_903\n",
       "                   %\"add_903\"<FLOAT16,[s43,12,s53,s53]> ⬅️ ::Add(%\"div_8\", %\"expand\")\n",
       "            274 |  # node_add_909\n",
       "                   %\"add_909\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::Add(%\"add_903\", %\"mul_22\")\n",
       "            275 |  # node_softmax_6\n",
       "                   %\"softmax_6\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::Softmax(%\"add_909\") {axis=-1}\n",
       "            276 |  # node_matmul_13\n",
       "                   %\"matmul_13\"<FLOAT16,[1,12,s53,64]> ⬅️ ::MatMul(%\"softmax_6\", %\"transpose_26\")\n",
       "            277 |  # node_permute_7\n",
       "                   %\"permute_7\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Transpose(%\"matmul_13\") {perm=(0, 2, 1, 3)}\n",
       "            278 |  # node_view_27\n",
       "                   %\"view_27\"<FLOAT16,[1,s53,768]> ⬅️ ::Reshape(%\"permute_7\", %\"val_101\") {allowzero=1}\n",
       "            279 |  # node_MatMul_353\n",
       "                   %\"val_379\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"view_27\", %\"val_378\"{...})\n",
       "            280 |  # node_linear_39\n",
       "                   %\"linear_39\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_379\", %\"encoder.layer.6.attention.attn.o.bias\"{...})\n",
       "            281 |  # node_add_943\n",
       "                   %\"add_943\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"linear_39\", %\"layer_norm_12\")\n",
       "            282 |  # node_layer_norm_13\n",
       "                   %\"layer_norm_13\"<FLOAT16,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_943\", %\"encoder.layer.6.attention.LayerNorm.weight\"{...}, %\"encoder.layer.6.attention.LayerNorm.bias\"{...}) {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}\n",
       "            283 |  # node_MatMul_355\n",
       "                   %\"val_383\"<FLOAT16,[1,s53,3072]> ⬅️ ::MatMul(%\"layer_norm_13\", %\"val_382\"{...})\n",
       "            284 |  # node_linear_40\n",
       "                   %\"linear_40\"<FLOAT16,[1,s53,3072]> ⬅️ ::Add(%\"val_383\", %\"encoder.layer.6.intermediate.dense.bias\"{...})\n",
       "            285 |  # node_Div_357\n",
       "                   %\"val_385\"<FLOAT16,[1,s53,3072]> ⬅️ ::Div(%\"linear_40\", %\"val_108\"{1.4140625})\n",
       "            286 |  # node_Erf_358\n",
       "                   %\"val_386\"<FLOAT16,[1,s53,3072]> ⬅️ ::Erf(%\"val_385\")\n",
       "            287 |  # node_Add_360\n",
       "                   %\"val_388\"<FLOAT16,[1,s53,3072]> ⬅️ ::Add(%\"val_386\", %\"scalar_tensor_default\"{1.0})\n",
       "            288 |  # node_Mul_362\n",
       "                   %\"val_390\"<FLOAT16,[1,s53,3072]> ⬅️ ::Mul(%\"val_113\"{0.5}, %\"val_388\")\n",
       "            289 |  # node_gelu_6\n",
       "                   %\"gelu_6\"<FLOAT16,[1,s53,3072]> ⬅️ ::Mul(%\"linear_40\", %\"val_390\")\n",
       "            290 |  # node_MatMul_364\n",
       "                   %\"val_392\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"gelu_6\", %\"val_391\"{...})\n",
       "            291 |  # node_linear_41\n",
       "                   %\"linear_41\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_392\", %\"encoder.layer.6.output.dense.bias\"{...})\n",
       "            292 |  # node_add_962\n",
       "                   %\"add_962\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"linear_41\", %\"layer_norm_13\")\n",
       "            293 |  # node_layer_norm_14\n",
       "                   %\"layer_norm_14\"<FLOAT16,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_962\", %\"encoder.layer.6.output.LayerNorm.weight\"{...}, %\"encoder.layer.6.output.LayerNorm.bias\"{...}) {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}\n",
       "            294 |  # node_MatMul_366\n",
       "                   %\"val_396\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_14\", %\"val_395\"{...})\n",
       "            295 |  # node_linear_42\n",
       "                   %\"linear_42\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_396\", %\"encoder.layer.7.attention.attn.q.bias\"{...})\n",
       "            296 |  # node_view_28\n",
       "                   %\"view_28\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_42\", %\"val_79\") {allowzero=1}\n",
       "            297 |  # node_transpose_28\n",
       "                   %\"transpose_28\"<FLOAT16,[1,12,s53,64]> ⬅️ ::Transpose(%\"view_28\") {perm=(0, 2, 1, 3)}\n",
       "            298 |  # node_MatMul_374\n",
       "                   %\"val_404\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_14\", %\"val_403\"{...})\n",
       "            299 |  # node_linear_43\n",
       "                   %\"linear_43\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_404\", %\"encoder.layer.7.attention.attn.k.bias\"{...})\n",
       "            300 |  # node_view_29\n",
       "                   %\"view_29\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_43\", %\"val_79\") {allowzero=1}\n",
       "            301 |  # node_MatMul_382\n",
       "                   %\"val_412\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_14\", %\"val_411\"{...})\n",
       "            302 |  # node_linear_44\n",
       "                   %\"linear_44\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_412\", %\"encoder.layer.7.attention.attn.v.bias\"{...})\n",
       "            303 |  # node_view_30\n",
       "                   %\"view_30\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_44\", %\"val_79\") {allowzero=1}\n",
       "            304 |  # node_transpose_30\n",
       "                   %\"transpose_30\"<FLOAT16,[1,12,s53,64]> ⬅️ ::Transpose(%\"view_30\") {perm=(0, 2, 1, 3)}\n",
       "            305 |  # node_Transpose_77\n",
       "                   %\"transpose_31\"<FLOAT16,[1,12,64,s53]> ⬅️ ::Transpose(%\"view_29\") {perm=(0, 2, 3, 1)}\n",
       "            306 |  # node_matmul_14\n",
       "                   %\"matmul_14\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::MatMul(%\"transpose_28\", %\"transpose_31\")\n",
       "            307 |  # node_div_9\n",
       "                   %\"div_9\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::Div(%\"matmul_14\", %\"scalar_tensor_default_4\"{8.0})\n",
       "            308 |  # node_add_1018\n",
       "                   %\"add_1018\"<FLOAT16,[s43,12,s53,s53]> ⬅️ ::Add(%\"div_9\", %\"expand\")\n",
       "            309 |  # node_add_1024\n",
       "                   %\"add_1024\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::Add(%\"add_1018\", %\"mul_22\")\n",
       "            310 |  # node_softmax_7\n",
       "                   %\"softmax_7\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::Softmax(%\"add_1024\") {axis=-1}\n",
       "            311 |  # node_matmul_15\n",
       "                   %\"matmul_15\"<FLOAT16,[1,12,s53,64]> ⬅️ ::MatMul(%\"softmax_7\", %\"transpose_30\")\n",
       "            312 |  # node_permute_8\n",
       "                   %\"permute_8\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Transpose(%\"matmul_15\") {perm=(0, 2, 1, 3)}\n",
       "            313 |  # node_view_31\n",
       "                   %\"view_31\"<FLOAT16,[1,s53,768]> ⬅️ ::Reshape(%\"permute_8\", %\"val_101\") {allowzero=1}\n",
       "            314 |  # node_MatMul_395\n",
       "                   %\"val_425\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"view_31\", %\"val_424\"{...})\n",
       "            315 |  # node_linear_45\n",
       "                   %\"linear_45\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_425\", %\"encoder.layer.7.attention.attn.o.bias\"{...})\n",
       "            316 |  # node_add_1058\n",
       "                   %\"add_1058\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"linear_45\", %\"layer_norm_14\")\n",
       "            317 |  # node_layer_norm_15\n",
       "                   %\"layer_norm_15\"<FLOAT16,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_1058\", %\"encoder.layer.7.attention.LayerNorm.weight\"{...}, %\"encoder.layer.7.attention.LayerNorm.bias\"{...}) {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}\n",
       "            318 |  # node_MatMul_397\n",
       "                   %\"val_429\"<FLOAT16,[1,s53,3072]> ⬅️ ::MatMul(%\"layer_norm_15\", %\"val_428\"{...})\n",
       "            319 |  # node_linear_46\n",
       "                   %\"linear_46\"<FLOAT16,[1,s53,3072]> ⬅️ ::Add(%\"val_429\", %\"encoder.layer.7.intermediate.dense.bias\"{...})\n",
       "            320 |  # node_Div_399\n",
       "                   %\"val_431\"<FLOAT16,[1,s53,3072]> ⬅️ ::Div(%\"linear_46\", %\"val_108\"{1.4140625})\n",
       "            321 |  # node_Erf_400\n",
       "                   %\"val_432\"<FLOAT16,[1,s53,3072]> ⬅️ ::Erf(%\"val_431\")\n",
       "            322 |  # node_Add_402\n",
       "                   %\"val_434\"<FLOAT16,[1,s53,3072]> ⬅️ ::Add(%\"val_432\", %\"scalar_tensor_default\"{1.0})\n",
       "            323 |  # node_Mul_404\n",
       "                   %\"val_436\"<FLOAT16,[1,s53,3072]> ⬅️ ::Mul(%\"val_113\"{0.5}, %\"val_434\")\n",
       "            324 |  # node_gelu_7\n",
       "                   %\"gelu_7\"<FLOAT16,[1,s53,3072]> ⬅️ ::Mul(%\"linear_46\", %\"val_436\")\n",
       "            325 |  # node_MatMul_406\n",
       "                   %\"val_438\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"gelu_7\", %\"val_437\"{...})\n",
       "            326 |  # node_linear_47\n",
       "                   %\"linear_47\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_438\", %\"encoder.layer.7.output.dense.bias\"{...})\n",
       "            327 |  # node_add_1077\n",
       "                   %\"add_1077\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"linear_47\", %\"layer_norm_15\")\n",
       "            328 |  # node_layer_norm_16\n",
       "                   %\"layer_norm_16\"<FLOAT16,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_1077\", %\"encoder.layer.7.output.LayerNorm.weight\"{...}, %\"encoder.layer.7.output.LayerNorm.bias\"{...}) {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}\n",
       "            329 |  # node_MatMul_408\n",
       "                   %\"val_442\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_16\", %\"val_441\"{...})\n",
       "            330 |  # node_linear_48\n",
       "                   %\"linear_48\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_442\", %\"encoder.layer.8.attention.attn.q.bias\"{...})\n",
       "            331 |  # node_view_32\n",
       "                   %\"view_32\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_48\", %\"val_79\") {allowzero=1}\n",
       "            332 |  # node_transpose_32\n",
       "                   %\"transpose_32\"<FLOAT16,[1,12,s53,64]> ⬅️ ::Transpose(%\"view_32\") {perm=(0, 2, 1, 3)}\n",
       "            333 |  # node_MatMul_416\n",
       "                   %\"val_450\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_16\", %\"val_449\"{...})\n",
       "            334 |  # node_linear_49\n",
       "                   %\"linear_49\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_450\", %\"encoder.layer.8.attention.attn.k.bias\"{...})\n",
       "            335 |  # node_view_33\n",
       "                   %\"view_33\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_49\", %\"val_79\") {allowzero=1}\n",
       "            336 |  # node_MatMul_424\n",
       "                   %\"val_458\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_16\", %\"val_457\"{...})\n",
       "            337 |  # node_linear_50\n",
       "                   %\"linear_50\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_458\", %\"encoder.layer.8.attention.attn.v.bias\"{...})\n",
       "            338 |  # node_view_34\n",
       "                   %\"view_34\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_50\", %\"val_79\") {allowzero=1}\n",
       "            339 |  # node_transpose_34\n",
       "                   %\"transpose_34\"<FLOAT16,[1,12,s53,64]> ⬅️ ::Transpose(%\"view_34\") {perm=(0, 2, 1, 3)}\n",
       "            340 |  # node_Transpose_83\n",
       "                   %\"transpose_35\"<FLOAT16,[1,12,64,s53]> ⬅️ ::Transpose(%\"view_33\") {perm=(0, 2, 3, 1)}\n",
       "            341 |  # node_matmul_16\n",
       "                   %\"matmul_16\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::MatMul(%\"transpose_32\", %\"transpose_35\")\n",
       "            342 |  # node_div_10\n",
       "                   %\"div_10\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::Div(%\"matmul_16\", %\"scalar_tensor_default_4\"{8.0})\n",
       "            343 |  # node_add_1133\n",
       "                   %\"add_1133\"<FLOAT16,[s43,12,s53,s53]> ⬅️ ::Add(%\"div_10\", %\"expand\")\n",
       "            344 |  # node_add_1139\n",
       "                   %\"add_1139\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::Add(%\"add_1133\", %\"mul_22\")\n",
       "            345 |  # node_softmax_8\n",
       "                   %\"softmax_8\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::Softmax(%\"add_1139\") {axis=-1}\n",
       "            346 |  # node_matmul_17\n",
       "                   %\"matmul_17\"<FLOAT16,[1,12,s53,64]> ⬅️ ::MatMul(%\"softmax_8\", %\"transpose_34\")\n",
       "            347 |  # node_permute_9\n",
       "                   %\"permute_9\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Transpose(%\"matmul_17\") {perm=(0, 2, 1, 3)}\n",
       "            348 |  # node_view_35\n",
       "                   %\"view_35\"<FLOAT16,[1,s53,768]> ⬅️ ::Reshape(%\"permute_9\", %\"val_101\") {allowzero=1}\n",
       "            349 |  # node_MatMul_437\n",
       "                   %\"val_471\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"view_35\", %\"val_470\"{...})\n",
       "            350 |  # node_linear_51\n",
       "                   %\"linear_51\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_471\", %\"encoder.layer.8.attention.attn.o.bias\"{...})\n",
       "            351 |  # node_add_1173\n",
       "                   %\"add_1173\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"linear_51\", %\"layer_norm_16\")\n",
       "            352 |  # node_layer_norm_17\n",
       "                   %\"layer_norm_17\"<FLOAT16,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_1173\", %\"encoder.layer.8.attention.LayerNorm.weight\"{...}, %\"encoder.layer.8.attention.LayerNorm.bias\"{...}) {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}\n",
       "            353 |  # node_MatMul_439\n",
       "                   %\"val_475\"<FLOAT16,[1,s53,3072]> ⬅️ ::MatMul(%\"layer_norm_17\", %\"val_474\"{...})\n",
       "            354 |  # node_linear_52\n",
       "                   %\"linear_52\"<FLOAT16,[1,s53,3072]> ⬅️ ::Add(%\"val_475\", %\"encoder.layer.8.intermediate.dense.bias\"{...})\n",
       "            355 |  # node_Div_441\n",
       "                   %\"val_477\"<FLOAT16,[1,s53,3072]> ⬅️ ::Div(%\"linear_52\", %\"val_108\"{1.4140625})\n",
       "            356 |  # node_Erf_442\n",
       "                   %\"val_478\"<FLOAT16,[1,s53,3072]> ⬅️ ::Erf(%\"val_477\")\n",
       "            357 |  # node_Add_444\n",
       "                   %\"val_480\"<FLOAT16,[1,s53,3072]> ⬅️ ::Add(%\"val_478\", %\"scalar_tensor_default\"{1.0})\n",
       "            358 |  # node_Mul_446\n",
       "                   %\"val_482\"<FLOAT16,[1,s53,3072]> ⬅️ ::Mul(%\"val_113\"{0.5}, %\"val_480\")\n",
       "            359 |  # node_gelu_8\n",
       "                   %\"gelu_8\"<FLOAT16,[1,s53,3072]> ⬅️ ::Mul(%\"linear_52\", %\"val_482\")\n",
       "            360 |  # node_MatMul_448\n",
       "                   %\"val_484\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"gelu_8\", %\"val_483\"{...})\n",
       "            361 |  # node_linear_53\n",
       "                   %\"linear_53\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_484\", %\"encoder.layer.8.output.dense.bias\"{...})\n",
       "            362 |  # node_add_1192\n",
       "                   %\"add_1192\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"linear_53\", %\"layer_norm_17\")\n",
       "            363 |  # node_layer_norm_18\n",
       "                   %\"layer_norm_18\"<FLOAT16,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_1192\", %\"encoder.layer.8.output.LayerNorm.weight\"{...}, %\"encoder.layer.8.output.LayerNorm.bias\"{...}) {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}\n",
       "            364 |  # node_MatMul_450\n",
       "                   %\"val_488\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_18\", %\"val_487\"{...})\n",
       "            365 |  # node_linear_54\n",
       "                   %\"linear_54\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_488\", %\"encoder.layer.9.attention.attn.q.bias\"{...})\n",
       "            366 |  # node_view_36\n",
       "                   %\"view_36\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_54\", %\"val_79\") {allowzero=1}\n",
       "            367 |  # node_transpose_36\n",
       "                   %\"transpose_36\"<FLOAT16,[1,12,s53,64]> ⬅️ ::Transpose(%\"view_36\") {perm=(0, 2, 1, 3)}\n",
       "            368 |  # node_MatMul_458\n",
       "                   %\"val_496\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_18\", %\"val_495\"{...})\n",
       "            369 |  # node_linear_55\n",
       "                   %\"linear_55\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_496\", %\"encoder.layer.9.attention.attn.k.bias\"{...})\n",
       "            370 |  # node_view_37\n",
       "                   %\"view_37\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_55\", %\"val_79\") {allowzero=1}\n",
       "            371 |  # node_MatMul_466\n",
       "                   %\"val_504\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_18\", %\"val_503\"{...})\n",
       "            372 |  # node_linear_56\n",
       "                   %\"linear_56\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_504\", %\"encoder.layer.9.attention.attn.v.bias\"{...})\n",
       "            373 |  # node_view_38\n",
       "                   %\"view_38\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_56\", %\"val_79\") {allowzero=1}\n",
       "            374 |  # node_transpose_38\n",
       "                   %\"transpose_38\"<FLOAT16,[1,12,s53,64]> ⬅️ ::Transpose(%\"view_38\") {perm=(0, 2, 1, 3)}\n",
       "            375 |  # node_Transpose_89\n",
       "                   %\"transpose_39\"<FLOAT16,[1,12,64,s53]> ⬅️ ::Transpose(%\"view_37\") {perm=(0, 2, 3, 1)}\n",
       "            376 |  # node_matmul_18\n",
       "                   %\"matmul_18\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::MatMul(%\"transpose_36\", %\"transpose_39\")\n",
       "            377 |  # node_div_11\n",
       "                   %\"div_11\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::Div(%\"matmul_18\", %\"scalar_tensor_default_4\"{8.0})\n",
       "            378 |  # node_add_1248\n",
       "                   %\"add_1248\"<FLOAT16,[s43,12,s53,s53]> ⬅️ ::Add(%\"div_11\", %\"expand\")\n",
       "            379 |  # node_add_1254\n",
       "                   %\"add_1254\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::Add(%\"add_1248\", %\"mul_22\")\n",
       "            380 |  # node_softmax_9\n",
       "                   %\"softmax_9\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::Softmax(%\"add_1254\") {axis=-1}\n",
       "            381 |  # node_matmul_19\n",
       "                   %\"matmul_19\"<FLOAT16,[1,12,s53,64]> ⬅️ ::MatMul(%\"softmax_9\", %\"transpose_38\")\n",
       "            382 |  # node_permute_10\n",
       "                   %\"permute_10\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Transpose(%\"matmul_19\") {perm=(0, 2, 1, 3)}\n",
       "            383 |  # node_view_39\n",
       "                   %\"view_39\"<FLOAT16,[1,s53,768]> ⬅️ ::Reshape(%\"permute_10\", %\"val_101\") {allowzero=1}\n",
       "            384 |  # node_MatMul_479\n",
       "                   %\"val_517\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"view_39\", %\"val_516\"{...})\n",
       "            385 |  # node_linear_57\n",
       "                   %\"linear_57\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_517\", %\"encoder.layer.9.attention.attn.o.bias\"{...})\n",
       "            386 |  # node_add_1288\n",
       "                   %\"add_1288\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"linear_57\", %\"layer_norm_18\")\n",
       "            387 |  # node_layer_norm_19\n",
       "                   %\"layer_norm_19\"<FLOAT16,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_1288\", %\"encoder.layer.9.attention.LayerNorm.weight\"{...}, %\"encoder.layer.9.attention.LayerNorm.bias\"{...}) {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}\n",
       "            388 |  # node_MatMul_481\n",
       "                   %\"val_521\"<FLOAT16,[1,s53,3072]> ⬅️ ::MatMul(%\"layer_norm_19\", %\"val_520\"{...})\n",
       "            389 |  # node_linear_58\n",
       "                   %\"linear_58\"<FLOAT16,[1,s53,3072]> ⬅️ ::Add(%\"val_521\", %\"encoder.layer.9.intermediate.dense.bias\"{...})\n",
       "            390 |  # node_Div_483\n",
       "                   %\"val_523\"<FLOAT16,[1,s53,3072]> ⬅️ ::Div(%\"linear_58\", %\"val_108\"{1.4140625})\n",
       "            391 |  # node_Erf_484\n",
       "                   %\"val_524\"<FLOAT16,[1,s53,3072]> ⬅️ ::Erf(%\"val_523\")\n",
       "            392 |  # node_Add_486\n",
       "                   %\"val_526\"<FLOAT16,[1,s53,3072]> ⬅️ ::Add(%\"val_524\", %\"scalar_tensor_default\"{1.0})\n",
       "            393 |  # node_Mul_488\n",
       "                   %\"val_528\"<FLOAT16,[1,s53,3072]> ⬅️ ::Mul(%\"val_113\"{0.5}, %\"val_526\")\n",
       "            394 |  # node_gelu_9\n",
       "                   %\"gelu_9\"<FLOAT16,[1,s53,3072]> ⬅️ ::Mul(%\"linear_58\", %\"val_528\")\n",
       "            395 |  # node_MatMul_490\n",
       "                   %\"val_530\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"gelu_9\", %\"val_529\"{...})\n",
       "            396 |  # node_linear_59\n",
       "                   %\"linear_59\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_530\", %\"encoder.layer.9.output.dense.bias\"{...})\n",
       "            397 |  # node_add_1307\n",
       "                   %\"add_1307\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"linear_59\", %\"layer_norm_19\")\n",
       "            398 |  # node_layer_norm_20\n",
       "                   %\"layer_norm_20\"<FLOAT16,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_1307\", %\"encoder.layer.9.output.LayerNorm.weight\"{...}, %\"encoder.layer.9.output.LayerNorm.bias\"{...}) {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}\n",
       "            399 |  # node_MatMul_492\n",
       "                   %\"val_534\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_20\", %\"val_533\"{...})\n",
       "            400 |  # node_linear_60\n",
       "                   %\"linear_60\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_534\", %\"encoder.layer.10.attention.attn.q.bias\"{...})\n",
       "            401 |  # node_view_40\n",
       "                   %\"view_40\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_60\", %\"val_79\") {allowzero=1}\n",
       "            402 |  # node_transpose_40\n",
       "                   %\"transpose_40\"<FLOAT16,[1,12,s53,64]> ⬅️ ::Transpose(%\"view_40\") {perm=(0, 2, 1, 3)}\n",
       "            403 |  # node_MatMul_500\n",
       "                   %\"val_542\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_20\", %\"val_541\"{...})\n",
       "            404 |  # node_linear_61\n",
       "                   %\"linear_61\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_542\", %\"encoder.layer.10.attention.attn.k.bias\"{...})\n",
       "            405 |  # node_view_41\n",
       "                   %\"view_41\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_61\", %\"val_79\") {allowzero=1}\n",
       "            406 |  # node_MatMul_508\n",
       "                   %\"val_550\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_20\", %\"val_549\"{...})\n",
       "            407 |  # node_linear_62\n",
       "                   %\"linear_62\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_550\", %\"encoder.layer.10.attention.attn.v.bias\"{...})\n",
       "            408 |  # node_view_42\n",
       "                   %\"view_42\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_62\", %\"val_79\") {allowzero=1}\n",
       "            409 |  # node_transpose_42\n",
       "                   %\"transpose_42\"<FLOAT16,[1,12,s53,64]> ⬅️ ::Transpose(%\"view_42\") {perm=(0, 2, 1, 3)}\n",
       "            410 |  # node_Transpose_95\n",
       "                   %\"transpose_43\"<FLOAT16,[1,12,64,s53]> ⬅️ ::Transpose(%\"view_41\") {perm=(0, 2, 3, 1)}\n",
       "            411 |  # node_matmul_20\n",
       "                   %\"matmul_20\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::MatMul(%\"transpose_40\", %\"transpose_43\")\n",
       "            412 |  # node_div_12\n",
       "                   %\"div_12\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::Div(%\"matmul_20\", %\"scalar_tensor_default_4\"{8.0})\n",
       "            413 |  # node_add_1363\n",
       "                   %\"add_1363\"<FLOAT16,[s43,12,s53,s53]> ⬅️ ::Add(%\"div_12\", %\"expand\")\n",
       "            414 |  # node_add_1369\n",
       "                   %\"add_1369\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::Add(%\"add_1363\", %\"mul_22\")\n",
       "            415 |  # node_softmax_10\n",
       "                   %\"softmax_10\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::Softmax(%\"add_1369\") {axis=-1}\n",
       "            416 |  # node_matmul_21\n",
       "                   %\"matmul_21\"<FLOAT16,[1,12,s53,64]> ⬅️ ::MatMul(%\"softmax_10\", %\"transpose_42\")\n",
       "            417 |  # node_permute_11\n",
       "                   %\"permute_11\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Transpose(%\"matmul_21\") {perm=(0, 2, 1, 3)}\n",
       "            418 |  # node_view_43\n",
       "                   %\"view_43\"<FLOAT16,[1,s53,768]> ⬅️ ::Reshape(%\"permute_11\", %\"val_101\") {allowzero=1}\n",
       "            419 |  # node_MatMul_521\n",
       "                   %\"val_563\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"view_43\", %\"val_562\"{...})\n",
       "            420 |  # node_linear_63\n",
       "                   %\"linear_63\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_563\", %\"encoder.layer.10.attention.attn.o.bias\"{...})\n",
       "            421 |  # node_add_1403\n",
       "                   %\"add_1403\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"linear_63\", %\"layer_norm_20\")\n",
       "            422 |  # node_layer_norm_21\n",
       "                   %\"layer_norm_21\"<FLOAT16,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_1403\", %\"encoder.layer.10.attention.LayerNorm.weight\"{...}, %\"encoder.layer.10.attention.LayerNorm.bias\"{...}) {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}\n",
       "            423 |  # node_MatMul_523\n",
       "                   %\"val_567\"<FLOAT16,[1,s53,3072]> ⬅️ ::MatMul(%\"layer_norm_21\", %\"val_566\"{...})\n",
       "            424 |  # node_linear_64\n",
       "                   %\"linear_64\"<FLOAT16,[1,s53,3072]> ⬅️ ::Add(%\"val_567\", %\"encoder.layer.10.intermediate.dense.bias\"{...})\n",
       "            425 |  # node_Div_525\n",
       "                   %\"val_569\"<FLOAT16,[1,s53,3072]> ⬅️ ::Div(%\"linear_64\", %\"val_108\"{1.4140625})\n",
       "            426 |  # node_Erf_526\n",
       "                   %\"val_570\"<FLOAT16,[1,s53,3072]> ⬅️ ::Erf(%\"val_569\")\n",
       "            427 |  # node_Add_528\n",
       "                   %\"val_572\"<FLOAT16,[1,s53,3072]> ⬅️ ::Add(%\"val_570\", %\"scalar_tensor_default\"{1.0})\n",
       "            428 |  # node_Mul_530\n",
       "                   %\"val_574\"<FLOAT16,[1,s53,3072]> ⬅️ ::Mul(%\"val_113\"{0.5}, %\"val_572\")\n",
       "            429 |  # node_gelu_10\n",
       "                   %\"gelu_10\"<FLOAT16,[1,s53,3072]> ⬅️ ::Mul(%\"linear_64\", %\"val_574\")\n",
       "            430 |  # node_MatMul_532\n",
       "                   %\"val_576\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"gelu_10\", %\"val_575\"{...})\n",
       "            431 |  # node_linear_65\n",
       "                   %\"linear_65\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_576\", %\"encoder.layer.10.output.dense.bias\"{...})\n",
       "            432 |  # node_add_1422\n",
       "                   %\"add_1422\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"linear_65\", %\"layer_norm_21\")\n",
       "            433 |  # node_layer_norm_22\n",
       "                   %\"layer_norm_22\"<FLOAT16,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_1422\", %\"encoder.layer.10.output.LayerNorm.weight\"{...}, %\"encoder.layer.10.output.LayerNorm.bias\"{...}) {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}\n",
       "            434 |  # node_MatMul_534\n",
       "                   %\"val_580\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_22\", %\"val_579\"{...})\n",
       "            435 |  # node_linear_66\n",
       "                   %\"linear_66\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_580\", %\"encoder.layer.11.attention.attn.q.bias\"{...})\n",
       "            436 |  # node_view_44\n",
       "                   %\"view_44\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_66\", %\"val_79\") {allowzero=1}\n",
       "            437 |  # node_transpose_44\n",
       "                   %\"transpose_44\"<FLOAT16,[1,12,s53,64]> ⬅️ ::Transpose(%\"view_44\") {perm=(0, 2, 1, 3)}\n",
       "            438 |  # node_MatMul_542\n",
       "                   %\"val_588\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_22\", %\"val_587\"{...})\n",
       "            439 |  # node_linear_67\n",
       "                   %\"linear_67\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_588\", %\"encoder.layer.11.attention.attn.k.bias\"{...})\n",
       "            440 |  # node_view_45\n",
       "                   %\"view_45\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_67\", %\"val_79\") {allowzero=1}\n",
       "            441 |  # node_MatMul_550\n",
       "                   %\"val_596\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_22\", %\"val_595\"{...})\n",
       "            442 |  # node_linear_68\n",
       "                   %\"linear_68\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_596\", %\"encoder.layer.11.attention.attn.v.bias\"{...})\n",
       "            443 |  # node_view_46\n",
       "                   %\"view_46\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_68\", %\"val_79\") {allowzero=1}\n",
       "            444 |  # node_transpose_46\n",
       "                   %\"transpose_46\"<FLOAT16,[1,12,s53,64]> ⬅️ ::Transpose(%\"view_46\") {perm=(0, 2, 1, 3)}\n",
       "            445 |  # node_Transpose_101\n",
       "                   %\"transpose_47\"<FLOAT16,[1,12,64,s53]> ⬅️ ::Transpose(%\"view_45\") {perm=(0, 2, 3, 1)}\n",
       "            446 |  # node_matmul_22\n",
       "                   %\"matmul_22\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::MatMul(%\"transpose_44\", %\"transpose_47\")\n",
       "            447 |  # node_div_13\n",
       "                   %\"div_13\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::Div(%\"matmul_22\", %\"scalar_tensor_default_4\"{8.0})\n",
       "            448 |  # node_add_1478\n",
       "                   %\"add_1478\"<FLOAT16,[s43,12,s53,s53]> ⬅️ ::Add(%\"div_13\", %\"expand\")\n",
       "            449 |  # node_add_1484\n",
       "                   %\"add_1484\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::Add(%\"add_1478\", %\"mul_22\")\n",
       "            450 |  # node_softmax_11\n",
       "                   %\"softmax_11\"<FLOAT16,[1,12,s53,s53]> ⬅️ ::Softmax(%\"add_1484\") {axis=-1}\n",
       "            451 |  # node_matmul_23\n",
       "                   %\"matmul_23\"<FLOAT16,[1,12,s53,64]> ⬅️ ::MatMul(%\"softmax_11\", %\"transpose_46\")\n",
       "            452 |  # node_permute_12\n",
       "                   %\"permute_12\"<FLOAT16,[1,s53,12,64]> ⬅️ ::Transpose(%\"matmul_23\") {perm=(0, 2, 1, 3)}\n",
       "            453 |  # node_view_47\n",
       "                   %\"view_47\"<FLOAT16,[1,s53,768]> ⬅️ ::Reshape(%\"permute_12\", %\"val_101\") {allowzero=1}\n",
       "            454 |  # node_MatMul_563\n",
       "                   %\"val_609\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"view_47\", %\"val_608\"{...})\n",
       "            455 |  # node_linear_69\n",
       "                   %\"linear_69\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_609\", %\"encoder.layer.11.attention.attn.o.bias\"{...})\n",
       "            456 |  # node_add_1518\n",
       "                   %\"add_1518\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"linear_69\", %\"layer_norm_22\")\n",
       "            457 |  # node_layer_norm_23\n",
       "                   %\"layer_norm_23\"<FLOAT16,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_1518\", %\"encoder.layer.11.attention.LayerNorm.weight\"{...}, %\"encoder.layer.11.attention.LayerNorm.bias\"{...}) {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}\n",
       "            458 |  # node_MatMul_565\n",
       "                   %\"val_613\"<FLOAT16,[1,s53,3072]> ⬅️ ::MatMul(%\"layer_norm_23\", %\"val_612\"{...})\n",
       "            459 |  # node_linear_70\n",
       "                   %\"linear_70\"<FLOAT16,[1,s53,3072]> ⬅️ ::Add(%\"val_613\", %\"encoder.layer.11.intermediate.dense.bias\"{...})\n",
       "            460 |  # node_Div_567\n",
       "                   %\"val_615\"<FLOAT16,[1,s53,3072]> ⬅️ ::Div(%\"linear_70\", %\"val_108\"{1.4140625})\n",
       "            461 |  # node_Erf_568\n",
       "                   %\"val_616\"<FLOAT16,[1,s53,3072]> ⬅️ ::Erf(%\"val_615\")\n",
       "            462 |  # node_Add_570\n",
       "                   %\"val_618\"<FLOAT16,[1,s53,3072]> ⬅️ ::Add(%\"val_616\", %\"scalar_tensor_default\"{1.0})\n",
       "            463 |  # node_Mul_572\n",
       "                   %\"val_620\"<FLOAT16,[1,s53,3072]> ⬅️ ::Mul(%\"val_113\"{0.5}, %\"val_618\")\n",
       "            464 |  # node_gelu_11\n",
       "                   %\"gelu_11\"<FLOAT16,[1,s53,3072]> ⬅️ ::Mul(%\"linear_70\", %\"val_620\")\n",
       "            465 |  # node_MatMul_574\n",
       "                   %\"val_622\"<FLOAT16,[1,s53,768]> ⬅️ ::MatMul(%\"gelu_11\", %\"val_621\"{...})\n",
       "            466 |  # node_linear_71\n",
       "                   %\"linear_71\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"val_622\", %\"encoder.layer.11.output.dense.bias\"{...})\n",
       "            467 |  # node_add_1537\n",
       "                   %\"add_1537\"<FLOAT16,[1,s53,768]> ⬅️ ::Add(%\"linear_71\", %\"layer_norm_23\")\n",
       "            468 |  # node_layer_norm_24\n",
       "                   %\"output\"<FLOAT16,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_1537\", %\"encoder.layer.11.output.LayerNorm.weight\"{...}, %\"encoder.layer.11.output.LayerNorm.bias\"{...}) {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}\n",
       "            469 |  # node_select\n",
       "                   %\"select\"<FLOAT16,[1,768]> ⬅️ ::Gather(%\"output\", %\"val_34\"{0}) {axis=1}\n",
       "            470 |  # node_Gemm_105\n",
       "                   %\"linear_72\"<FLOAT16,[1,768]> ⬅️ ::Gemm(%\"select\", %\"pooler.dense.weight\"{...}) {transA=0, transB=1, alpha=1.0, beta=1.0}\n",
       "            471 |  # node_tanh\n",
       "                   %\"tanh\"<FLOAT16,[1,768]> ⬅️ ::Tanh(%\"linear_72\")\n",
       "            return %\"output\"<FLOAT16,[1,s53,768]>, %\"tanh\"<FLOAT16,[1,768]>\n",
       "        }\n",
       "\n",
       "\n",
       "    ,\n",
       "    exported_program=\n",
       "        ExportedProgram:\n",
       "            class GraphModule(torch.nn.Module):\n",
       "                def forward(self, p_embeddings_word_embeddings_weight: \"f16[30527, 768]\", p_embeddings_position_embeddings_weight: \"f16[514, 768]\", p_embeddings_layernorm_weight: \"f16[768]\", p_embeddings_layernorm_bias: \"f16[768]\", p_encoder_layer_0_attention_attn_q_weight: \"f16[768, 768]\", p_encoder_layer_0_attention_attn_q_bias: \"f16[768]\", p_encoder_layer_0_attention_attn_k_weight: \"f16[768, 768]\", p_encoder_layer_0_attention_attn_k_bias: \"f16[768]\", p_encoder_layer_0_attention_attn_v_weight: \"f16[768, 768]\", p_encoder_layer_0_attention_attn_v_bias: \"f16[768]\", p_encoder_layer_0_attention_attn_o_weight: \"f16[768, 768]\", p_encoder_layer_0_attention_attn_o_bias: \"f16[768]\", p_encoder_layer_0_attention_layernorm_weight: \"f16[768]\", p_encoder_layer_0_attention_layernorm_bias: \"f16[768]\", p_encoder_layer_0_intermediate_dense_weight: \"f16[3072, 768]\", p_encoder_layer_0_intermediate_dense_bias: \"f16[3072]\", p_encoder_layer_0_output_dense_weight: \"f16[768, 3072]\", p_encoder_layer_0_output_dense_bias: \"f16[768]\", p_encoder_layer_0_output_layernorm_weight: \"f16[768]\", p_encoder_layer_0_output_layernorm_bias: \"f16[768]\", p_encoder_layer_1_attention_attn_q_weight: \"f16[768, 768]\", p_encoder_layer_1_attention_attn_q_bias: \"f16[768]\", p_encoder_layer_1_attention_attn_k_weight: \"f16[768, 768]\", p_encoder_layer_1_attention_attn_k_bias: \"f16[768]\", p_encoder_layer_1_attention_attn_v_weight: \"f16[768, 768]\", p_encoder_layer_1_attention_attn_v_bias: \"f16[768]\", p_encoder_layer_1_attention_attn_o_weight: \"f16[768, 768]\", p_encoder_layer_1_attention_attn_o_bias: \"f16[768]\", p_encoder_layer_1_attention_layernorm_weight: \"f16[768]\", p_encoder_layer_1_attention_layernorm_bias: \"f16[768]\", p_encoder_layer_1_intermediate_dense_weight: \"f16[3072, 768]\", p_encoder_layer_1_intermediate_dense_bias: \"f16[3072]\", p_encoder_layer_1_output_dense_weight: \"f16[768, 3072]\", p_encoder_layer_1_output_dense_bias: \"f16[768]\", p_encoder_layer_1_output_layernorm_weight: \"f16[768]\", p_encoder_layer_1_output_layernorm_bias: \"f16[768]\", p_encoder_layer_2_attention_attn_q_weight: \"f16[768, 768]\", p_encoder_layer_2_attention_attn_q_bias: \"f16[768]\", p_encoder_layer_2_attention_attn_k_weight: \"f16[768, 768]\", p_encoder_layer_2_attention_attn_k_bias: \"f16[768]\", p_encoder_layer_2_attention_attn_v_weight: \"f16[768, 768]\", p_encoder_layer_2_attention_attn_v_bias: \"f16[768]\", p_encoder_layer_2_attention_attn_o_weight: \"f16[768, 768]\", p_encoder_layer_2_attention_attn_o_bias: \"f16[768]\", p_encoder_layer_2_attention_layernorm_weight: \"f16[768]\", p_encoder_layer_2_attention_layernorm_bias: \"f16[768]\", p_encoder_layer_2_intermediate_dense_weight: \"f16[3072, 768]\", p_encoder_layer_2_intermediate_dense_bias: \"f16[3072]\", p_encoder_layer_2_output_dense_weight: \"f16[768, 3072]\", p_encoder_layer_2_output_dense_bias: \"f16[768]\", p_encoder_layer_2_output_layernorm_weight: \"f16[768]\", p_encoder_layer_2_output_layernorm_bias: \"f16[768]\", p_encoder_layer_3_attention_attn_q_weight: \"f16[768, 768]\", p_encoder_layer_3_attention_attn_q_bias: \"f16[768]\", p_encoder_layer_3_attention_attn_k_weight: \"f16[768, 768]\", p_encoder_layer_3_attention_attn_k_bias: \"f16[768]\", p_encoder_layer_3_attention_attn_v_weight: \"f16[768, 768]\", p_encoder_layer_3_attention_attn_v_bias: \"f16[768]\", p_encoder_layer_3_attention_attn_o_weight: \"f16[768, 768]\", p_encoder_layer_3_attention_attn_o_bias: \"f16[768]\", p_encoder_layer_3_attention_layernorm_weight: \"f16[768]\", p_encoder_layer_3_attention_layernorm_bias: \"f16[768]\", p_encoder_layer_3_intermediate_dense_weight: \"f16[3072, 768]\", p_encoder_layer_3_intermediate_dense_bias: \"f16[3072]\", p_encoder_layer_3_output_dense_weight: \"f16[768, 3072]\", p_encoder_layer_3_output_dense_bias: \"f16[768]\", p_encoder_layer_3_output_layernorm_weight: \"f16[768]\", p_encoder_layer_3_output_layernorm_bias: \"f16[768]\", p_encoder_layer_4_attention_attn_q_weight: \"f16[768, 768]\", p_encoder_layer_4_attention_attn_q_bias: \"f16[768]\", p_encoder_layer_4_attention_attn_k_weight: \"f16[768, 768]\", p_encoder_layer_4_attention_attn_k_bias: \"f16[768]\", p_encoder_layer_4_attention_attn_v_weight: \"f16[768, 768]\", p_encoder_layer_4_attention_attn_v_bias: \"f16[768]\", p_encoder_layer_4_attention_attn_o_weight: \"f16[768, 768]\", p_encoder_layer_4_attention_attn_o_bias: \"f16[768]\", p_encoder_layer_4_attention_layernorm_weight: \"f16[768]\", p_encoder_layer_4_attention_layernorm_bias: \"f16[768]\", p_encoder_layer_4_intermediate_dense_weight: \"f16[3072, 768]\", p_encoder_layer_4_intermediate_dense_bias: \"f16[3072]\", p_encoder_layer_4_output_dense_weight: \"f16[768, 3072]\", p_encoder_layer_4_output_dense_bias: \"f16[768]\", p_encoder_layer_4_output_layernorm_weight: \"f16[768]\", p_encoder_layer_4_output_layernorm_bias: \"f16[768]\", p_encoder_layer_5_attention_attn_q_weight: \"f16[768, 768]\", p_encoder_layer_5_attention_attn_q_bias: \"f16[768]\", p_encoder_layer_5_attention_attn_k_weight: \"f16[768, 768]\", p_encoder_layer_5_attention_attn_k_bias: \"f16[768]\", p_encoder_layer_5_attention_attn_v_weight: \"f16[768, 768]\", p_encoder_layer_5_attention_attn_v_bias: \"f16[768]\", p_encoder_layer_5_attention_attn_o_weight: \"f16[768, 768]\", p_encoder_layer_5_attention_attn_o_bias: \"f16[768]\", p_encoder_layer_5_attention_layernorm_weight: \"f16[768]\", p_encoder_layer_5_attention_layernorm_bias: \"f16[768]\", p_encoder_layer_5_intermediate_dense_weight: \"f16[3072, 768]\", p_encoder_layer_5_intermediate_dense_bias: \"f16[3072]\", p_encoder_layer_5_output_dense_weight: \"f16[768, 3072]\", p_encoder_layer_5_output_dense_bias: \"f16[768]\", p_encoder_layer_5_output_layernorm_weight: \"f16[768]\", p_encoder_layer_5_output_layernorm_bias: \"f16[768]\", p_encoder_layer_6_attention_attn_q_weight: \"f16[768, 768]\", p_encoder_layer_6_attention_attn_q_bias: \"f16[768]\", p_encoder_layer_6_attention_attn_k_weight: \"f16[768, 768]\", p_encoder_layer_6_attention_attn_k_bias: \"f16[768]\", p_encoder_layer_6_attention_attn_v_weight: \"f16[768, 768]\", p_encoder_layer_6_attention_attn_v_bias: \"f16[768]\", p_encoder_layer_6_attention_attn_o_weight: \"f16[768, 768]\", p_encoder_layer_6_attention_attn_o_bias: \"f16[768]\", p_encoder_layer_6_attention_layernorm_weight: \"f16[768]\", p_encoder_layer_6_attention_layernorm_bias: \"f16[768]\", p_encoder_layer_6_intermediate_dense_weight: \"f16[3072, 768]\", p_encoder_layer_6_intermediate_dense_bias: \"f16[3072]\", p_encoder_layer_6_output_dense_weight: \"f16[768, 3072]\", p_encoder_layer_6_output_dense_bias: \"f16[768]\", p_encoder_layer_6_output_layernorm_weight: \"f16[768]\", p_encoder_layer_6_output_layernorm_bias: \"f16[768]\", p_encoder_layer_7_attention_attn_q_weight: \"f16[768, 768]\", p_encoder_layer_7_attention_attn_q_bias: \"f16[768]\", p_encoder_layer_7_attention_attn_k_weight: \"f16[768, 768]\", p_encoder_layer_7_attention_attn_k_bias: \"f16[768]\", p_encoder_layer_7_attention_attn_v_weight: \"f16[768, 768]\", p_encoder_layer_7_attention_attn_v_bias: \"f16[768]\", p_encoder_layer_7_attention_attn_o_weight: \"f16[768, 768]\", p_encoder_layer_7_attention_attn_o_bias: \"f16[768]\", p_encoder_layer_7_attention_layernorm_weight: \"f16[768]\", p_encoder_layer_7_attention_layernorm_bias: \"f16[768]\", p_encoder_layer_7_intermediate_dense_weight: \"f16[3072, 768]\", p_encoder_layer_7_intermediate_dense_bias: \"f16[3072]\", p_encoder_layer_7_output_dense_weight: \"f16[768, 3072]\", p_encoder_layer_7_output_dense_bias: \"f16[768]\", p_encoder_layer_7_output_layernorm_weight: \"f16[768]\", p_encoder_layer_7_output_layernorm_bias: \"f16[768]\", p_encoder_layer_8_attention_attn_q_weight: \"f16[768, 768]\", p_encoder_layer_8_attention_attn_q_bias: \"f16[768]\", p_encoder_layer_8_attention_attn_k_weight: \"f16[768, 768]\", p_encoder_layer_8_attention_attn_k_bias: \"f16[768]\", p_encoder_layer_8_attention_attn_v_weight: \"f16[768, 768]\", p_encoder_layer_8_attention_attn_v_bias: \"f16[768]\", p_encoder_layer_8_attention_attn_o_weight: \"f16[768, 768]\", p_encoder_layer_8_attention_attn_o_bias: \"f16[768]\", p_encoder_layer_8_attention_layernorm_weight: \"f16[768]\", p_encoder_layer_8_attention_layernorm_bias: \"f16[768]\", p_encoder_layer_8_intermediate_dense_weight: \"f16[3072, 768]\", p_encoder_layer_8_intermediate_dense_bias: \"f16[3072]\", p_encoder_layer_8_output_dense_weight: \"f16[768, 3072]\", p_encoder_layer_8_output_dense_bias: \"f16[768]\", p_encoder_layer_8_output_layernorm_weight: \"f16[768]\", p_encoder_layer_8_output_layernorm_bias: \"f16[768]\", p_encoder_layer_9_attention_attn_q_weight: \"f16[768, 768]\", p_encoder_layer_9_attention_attn_q_bias: \"f16[768]\", p_encoder_layer_9_attention_attn_k_weight: \"f16[768, 768]\", p_encoder_layer_9_attention_attn_k_bias: \"f16[768]\", p_encoder_layer_9_attention_attn_v_weight: \"f16[768, 768]\", p_encoder_layer_9_attention_attn_v_bias: \"f16[768]\", p_encoder_layer_9_attention_attn_o_weight: \"f16[768, 768]\", p_encoder_layer_9_attention_attn_o_bias: \"f16[768]\", p_encoder_layer_9_attention_layernorm_weight: \"f16[768]\", p_encoder_layer_9_attention_layernorm_bias: \"f16[768]\", p_encoder_layer_9_intermediate_dense_weight: \"f16[3072, 768]\", p_encoder_layer_9_intermediate_dense_bias: \"f16[3072]\", p_encoder_layer_9_output_dense_weight: \"f16[768, 3072]\", p_encoder_layer_9_output_dense_bias: \"f16[768]\", p_encoder_layer_9_output_layernorm_weight: \"f16[768]\", p_encoder_layer_9_output_layernorm_bias: \"f16[768]\", p_encoder_layer_10_attention_attn_q_weight: \"f16[768, 768]\", p_encoder_layer_10_attention_attn_q_bias: \"f16[768]\", p_encoder_layer_10_attention_attn_k_weight: \"f16[768, 768]\", p_encoder_layer_10_attention_attn_k_bias: \"f16[768]\", p_encoder_layer_10_attention_attn_v_weight: \"f16[768, 768]\", p_encoder_layer_10_attention_attn_v_bias: \"f16[768]\", p_encoder_layer_10_attention_attn_o_weight: \"f16[768, 768]\", p_encoder_layer_10_attention_attn_o_bias: \"f16[768]\", p_encoder_layer_10_attention_layernorm_weight: \"f16[768]\", p_encoder_layer_10_attention_layernorm_bias: \"f16[768]\", p_encoder_layer_10_intermediate_dense_weight: \"f16[3072, 768]\", p_encoder_layer_10_intermediate_dense_bias: \"f16[3072]\", p_encoder_layer_10_output_dense_weight: \"f16[768, 3072]\", p_encoder_layer_10_output_dense_bias: \"f16[768]\", p_encoder_layer_10_output_layernorm_weight: \"f16[768]\", p_encoder_layer_10_output_layernorm_bias: \"f16[768]\", p_encoder_layer_11_attention_attn_q_weight: \"f16[768, 768]\", p_encoder_layer_11_attention_attn_q_bias: \"f16[768]\", p_encoder_layer_11_attention_attn_k_weight: \"f16[768, 768]\", p_encoder_layer_11_attention_attn_k_bias: \"f16[768]\", p_encoder_layer_11_attention_attn_v_weight: \"f16[768, 768]\", p_encoder_layer_11_attention_attn_v_bias: \"f16[768]\", p_encoder_layer_11_attention_attn_o_weight: \"f16[768, 768]\", p_encoder_layer_11_attention_attn_o_bias: \"f16[768]\", p_encoder_layer_11_attention_layernorm_weight: \"f16[768]\", p_encoder_layer_11_attention_layernorm_bias: \"f16[768]\", p_encoder_layer_11_intermediate_dense_weight: \"f16[3072, 768]\", p_encoder_layer_11_intermediate_dense_bias: \"f16[3072]\", p_encoder_layer_11_output_dense_weight: \"f16[768, 3072]\", p_encoder_layer_11_output_dense_bias: \"f16[768]\", p_encoder_layer_11_output_layernorm_weight: \"f16[768]\", p_encoder_layer_11_output_layernorm_bias: \"f16[768]\", p_encoder_relative_attention_bias_weight: \"f16[32, 12]\", p_pooler_dense_weight: \"f16[768, 768]\", p_pooler_dense_bias: \"f16[768]\", b_embeddings_position_ids: \"i64[1, 514]\", input_ids: \"i64[s43, s53]\", attention_mask: \"i64[s43, s53]\"):\n",
       "                     # \n",
       "                    sym_size_int_85: \"Sym(s43)\" = torch.ops.aten.sym_size.int(attention_mask, 0)\n",
       "                    sym_size_int_86: \"Sym(s53)\" = torch.ops.aten.sym_size.int(attention_mask, 1)\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:482 in forward, code: extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n",
       "                    slice_1: \"i64[s43, s53]\" = torch.ops.aten.slice.Tensor(attention_mask, 0, 0, 9223372036854775807);  attention_mask = None\n",
       "                    unsqueeze: \"i64[s43, 1, s53]\" = torch.ops.aten.unsqueeze.default(slice_1, 1);  slice_1 = None\n",
       "                    unsqueeze_1: \"i64[s43, 1, 1, s53]\" = torch.ops.aten.unsqueeze.default(unsqueeze, 2);  unsqueeze = None\n",
       "                    slice_2: \"i64[s43, 1, 1, s53]\" = torch.ops.aten.slice.Tensor(unsqueeze_1, 3, 0, 9223372036854775807);  unsqueeze_1 = None\n",
       "                    _to_copy: \"f16[s43, 1, 1, s53]\" = torch.ops.aten._to_copy.default(slice_2, dtype = torch.float16);  slice_2 = None\n",
       "                    scalar_tensor_default: \"f16[]\" = torch.ops.aten.scalar_tensor.default(1.0, dtype = torch.float16)\n",
       "                    sub_10: \"f16[s43, 1, 1, s53]\" = torch.ops.aten.sub.Tensor(scalar_tensor_default, _to_copy);  scalar_tensor_default = _to_copy = None\n",
       "                    scalar_tensor_default_1: \"f16[]\" = torch.ops.aten.scalar_tensor.default(-65504.0, dtype = torch.float16)\n",
       "                    mul_22: \"f16[1, 1, 1, s53]\" = torch.ops.aten.mul.Tensor(sub_10, scalar_tensor_default_1);  sub_10 = scalar_tensor_default_1 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:86 in forward, code: position_ids = create_position_ids_from_input_ids(input_ids, self.padding_idx)\n",
       "                    ne_3: \"b8[s43, s53]\" = torch.ops.aten.ne.Scalar(input_ids, 1)\n",
       "                    _to_copy_1: \"i32[s43, s53]\" = torch.ops.aten._to_copy.default(ne_3, dtype = torch.int32);  ne_3 = None\n",
       "                    convert_element_type_default: \"i64[s43, s53]\" = torch.ops.prims.convert_element_type.default(_to_copy_1, dtype = torch.int64)\n",
       "                    cumsum: \"i64[1, s53]\" = torch.ops.aten.cumsum.default(convert_element_type_default, 1);  convert_element_type_default = None\n",
       "                    type_as: \"i32[1, s53]\" = torch.ops.aten.type_as.default(cumsum, _to_copy_1);  cumsum = None\n",
       "                    mul_34: \"i32[s43, s53]\" = torch.ops.aten.mul.Tensor(type_as, _to_copy_1);  type_as = _to_copy_1 = None\n",
       "                    _to_copy_2: \"i64[s43, s53]\" = torch.ops.aten._to_copy.default(mul_34, dtype = torch.int64);  mul_34 = None\n",
       "                    add_45: \"i64[1, s53]\" = torch.ops.aten.add.Tensor(_to_copy_2, 1);  _to_copy_2 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:192 in forward, code: return F.embedding(\n",
       "                    embedding: \"f16[s43, s53, 768]\" = torch.ops.aten.embedding.default(p_embeddings_word_embeddings_weight, input_ids, 1);  p_embeddings_word_embeddings_weight = input_ids = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:192 in forward, code: return F.embedding(\n",
       "                    embedding_1: \"f16[1, s53, 768]\" = torch.ops.aten.embedding.default(p_embeddings_position_embeddings_weight, add_45, 1);  p_embeddings_position_embeddings_weight = add_45 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:104 in forward, code: embeddings = inputs_embeds + position_embeddings\n",
       "                    add_55: \"f16[1, s53, 768]\" = torch.ops.aten.add.Tensor(embedding, embedding_1);  embedding = embedding_1 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm: \"f16[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_55, [768], p_embeddings_layernorm_weight, p_embeddings_layernorm_bias);  add_55 = p_embeddings_layernorm_weight = p_embeddings_layernorm_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone: \"f16[1, s53, 768]\" = torch.ops.aten.clone.default(layer_norm);  layer_norm = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:331 in forward, code: position_bias = self.compute_position_bias(hidden_states)\n",
       "                    arange: \"i64[s53]\" = torch.ops.aten.arange.default(sym_size_int_86, dtype = torch.int64, device = device(type='cpu'), pin_memory = False)\n",
       "                    slice_3: \"i64[s53]\" = torch.ops.aten.slice.Tensor(arange, 0, 0, 9223372036854775807);  arange = None\n",
       "                    unsqueeze_2: \"i64[s53, 1]\" = torch.ops.aten.unsqueeze.default(slice_3, 1);  slice_3 = None\n",
       "                    arange_1: \"i64[s53]\" = torch.ops.aten.arange.default(sym_size_int_86, dtype = torch.int64, device = device(type='cpu'), pin_memory = False)\n",
       "                    unsqueeze_3: \"i64[1, s53]\" = torch.ops.aten.unsqueeze.default(arange_1, 0);  arange_1 = None\n",
       "                    slice_4: \"i64[1, s53]\" = torch.ops.aten.slice.Tensor(unsqueeze_3, 1, 0, 9223372036854775807);  unsqueeze_3 = None\n",
       "                    sub_37: \"i64[s53, s53]\" = torch.ops.aten.sub.Tensor(slice_4, unsqueeze_2);  slice_4 = unsqueeze_2 = None\n",
       "                    neg: \"i64[s53, s53]\" = torch.ops.aten.neg.default(sub_37);  sub_37 = None\n",
       "                    lt: \"b8[s53, s53]\" = torch.ops.aten.lt.Scalar(neg, 0)\n",
       "                    _to_copy_3: \"i64[s53, s53]\" = torch.ops.aten._to_copy.default(lt, dtype = torch.int64);  lt = None\n",
       "                    mul_71: \"i64[s53, s53]\" = torch.ops.aten.mul.Tensor(_to_copy_3, 16);  _to_copy_3 = None\n",
       "                    add_93: \"i64[s53, s53]\" = torch.ops.aten.add.Tensor(mul_71, 0);  mul_71 = None\n",
       "                    abs_1: \"i64[s53, s53]\" = torch.ops.aten.abs.default(neg);  neg = None\n",
       "                    lt_1: \"b8[s53, s53]\" = torch.ops.aten.lt.Scalar(abs_1, 8)\n",
       "                    _to_copy_4: \"f32[s53, s53]\" = torch.ops.aten._to_copy.default(abs_1, dtype = torch.float32)\n",
       "                    scalar_tensor_default_2: \"f32[]\" = torch.ops.aten.scalar_tensor.default(8, dtype = torch.float32)\n",
       "                    div: \"f32[s53, s53]\" = torch.ops.aten.div.Tensor(_to_copy_4, scalar_tensor_default_2);  _to_copy_4 = scalar_tensor_default_2 = None\n",
       "                    log: \"f32[s53, s53]\" = torch.ops.aten.log.default(div);  div = None\n",
       "                    div_1: \"f32[s53, s53]\" = torch.ops.aten.div.Tensor(log, 2.772588722239781);  log = None\n",
       "                    scalar_tensor_default_3: \"f32[]\" = torch.ops.aten.scalar_tensor.default(8, dtype = torch.float32)\n",
       "                    mul_87: \"f32[s53, s53]\" = torch.ops.aten.mul.Tensor(div_1, scalar_tensor_default_3);  div_1 = scalar_tensor_default_3 = None\n",
       "                    _to_copy_5: \"i64[s53, s53]\" = torch.ops.aten._to_copy.default(mul_87, dtype = torch.int64);  mul_87 = None\n",
       "                    add_121: \"i64[s53, s53]\" = torch.ops.aten.add.Tensor(_to_copy_5, 8);  _to_copy_5 = None\n",
       "                    full_like: \"i64[s53, s53]\" = torch.ops.aten.full_like.default(add_121, 15, pin_memory = False)\n",
       "                    minimum: \"i64[s53, s53]\" = torch.ops.aten.minimum.default(add_121, full_like);  add_121 = full_like = None\n",
       "                    where: \"i64[s53, s53]\" = torch.ops.aten.where.self(lt_1, abs_1, minimum);  lt_1 = abs_1 = minimum = None\n",
       "                    add_140: \"i64[s53, s53]\" = torch.ops.aten.add.Tensor(add_93, where);  add_93 = where = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:192 in forward, code: return F.embedding(\n",
       "                    embedding_2: \"f16[s53, s53, 12]\" = torch.ops.aten.embedding.default(p_encoder_relative_attention_bias_weight, add_140);  p_encoder_relative_attention_bias_weight = add_140 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:331 in forward, code: position_bias = self.compute_position_bias(hidden_states)\n",
       "                    permute: \"f16[12, s53, s53]\" = torch.ops.aten.permute.default(embedding_2, [2, 0, 1]);  embedding_2 = None\n",
       "                    unsqueeze_4: \"f16[1, 12, s53, s53]\" = torch.ops.aten.unsqueeze.default(permute, 0);  permute = None\n",
       "                    expand: \"f16[s43, 12, s53, s53]\" = torch.ops.aten.expand.default(unsqueeze_4, [sym_size_int_85, -1, sym_size_int_86, sym_size_int_86]);  unsqueeze_4 = None\n",
       "                    clone_1: \"f16[s43, 12, s53, s53]\" = torch.ops.aten.clone.default(expand, memory_format = torch.contiguous_format);  expand = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(clone, p_encoder_layer_0_attention_attn_q_weight, p_encoder_layer_0_attention_attn_q_bias);  p_encoder_layer_0_attention_attn_q_weight = p_encoder_layer_0_attention_attn_q_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:159 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view: \"f16[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear, [sym_size_int_85, -1, 12, 64]);  linear = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:160 in forward, code: .transpose(1, 2)\n",
       "                    transpose: \"f16[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view, 1, 2);  view = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_1: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(clone, p_encoder_layer_0_attention_attn_k_weight, p_encoder_layer_0_attention_attn_k_bias);  p_encoder_layer_0_attention_attn_k_weight = p_encoder_layer_0_attention_attn_k_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:164 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_1: \"f16[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_1, [sym_size_int_85, -1, 12, 64]);  linear_1 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:165 in forward, code: .transpose(1, 2)\n",
       "                    transpose_1: \"f16[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_1, 1, 2);  view_1 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_2: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(clone, p_encoder_layer_0_attention_attn_v_weight, p_encoder_layer_0_attention_attn_v_bias);  p_encoder_layer_0_attention_attn_v_weight = p_encoder_layer_0_attention_attn_v_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:169 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_2: \"f16[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_2, [sym_size_int_85, -1, 12, 64]);  linear_2 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:170 in forward, code: .transpose(1, 2)\n",
       "                    transpose_2: \"f16[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_2, 1, 2);  view_2 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:174 in forward, code: attention_scores = torch.matmul(q, k.transpose(-1, -2))\n",
       "                    transpose_3: \"f16[1, 12, 64, s53]\" = torch.ops.aten.transpose.int(transpose_1, -1, -2);  transpose_1 = None\n",
       "                    matmul: \"f16[1, 12, s53, s53]\" = torch.ops.aten.matmul.default(transpose, transpose_3);  transpose = transpose_3 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:175 in forward, code: attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
       "                    scalar_tensor_default_4: \"f16[]\" = torch.ops.aten.scalar_tensor.default(8.0, dtype = torch.float16)\n",
       "                    div_2: \"f16[1, 12, s53, s53]\" = torch.ops.aten.div.Tensor(matmul, scalar_tensor_default_4);  matmul = scalar_tensor_default_4 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:179 in forward, code: attention_scores += position_bias\n",
       "                    add_213: \"f16[s43, 12, s53, s53]\" = torch.ops.aten.add.Tensor(div_2, clone_1);  div_2 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:182 in forward, code: attention_scores = attention_scores + attention_mask\n",
       "                    add_219: \"f16[1, 12, s53, s53]\" = torch.ops.aten.add.Tensor(add_213, mul_22);  add_213 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:185 in forward, code: attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
       "                    softmax: \"f16[1, 12, s53, s53]\" = torch.ops.aten.softmax.int(add_219, -1);  add_219 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_2: \"f16[1, 12, s53, s53]\" = torch.ops.aten.clone.default(softmax);  softmax = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:192 in forward, code: c = torch.matmul(attention_probs, v)\n",
       "                    matmul_1: \"f16[1, 12, s53, 64]\" = torch.ops.aten.matmul.default(clone_2, transpose_2);  clone_2 = transpose_2 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:194 in forward, code: c = c.permute(0, 2, 1, 3).contiguous()\n",
       "                    permute_1: \"f16[1, s53, 12, 64]\" = torch.ops.aten.permute.default(matmul_1, [0, 2, 1, 3]);  matmul_1 = None\n",
       "                    clone_3: \"f16[1, s53, 12, 64]\" = torch.ops.aten.clone.default(permute_1, memory_format = torch.contiguous_format);  permute_1 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:196 in forward, code: c = c.view(*new_c_shape)\n",
       "                    view_3: \"f16[1, s53, 768]\" = torch.ops.aten.view.default(clone_3, [sym_size_int_85, sym_size_int_86, 768]);  clone_3 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_3: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(view_3, p_encoder_layer_0_attention_attn_o_weight, p_encoder_layer_0_attention_attn_o_bias);  view_3 = p_encoder_layer_0_attention_attn_o_weight = p_encoder_layer_0_attention_attn_o_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_4: \"f16[1, s53, 768]\" = torch.ops.aten.clone.default(linear_3);  linear_3 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:245 in forward, code: attention_output = self.LayerNorm(self.dropout(self_outputs[0]) + hidden_states)\n",
       "                    add_253: \"f16[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_4, clone);  clone_4 = clone = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_1: \"f16[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_253, [768], p_encoder_layer_0_attention_layernorm_weight, p_encoder_layer_0_attention_layernorm_bias);  add_253 = p_encoder_layer_0_attention_layernorm_weight = p_encoder_layer_0_attention_layernorm_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_4: \"f16[1, s53, 3072]\" = torch.ops.aten.linear.default(layer_norm_1, p_encoder_layer_0_intermediate_dense_weight, p_encoder_layer_0_intermediate_dense_bias);  p_encoder_layer_0_intermediate_dense_weight = p_encoder_layer_0_intermediate_dense_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\activations.py:85 in forward, code: return self.act(input)\n",
       "                    gelu: \"f16[1, s53, 3072]\" = torch.ops.aten.gelu.default(linear_4);  linear_4 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_5: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(gelu, p_encoder_layer_0_output_dense_weight, p_encoder_layer_0_output_dense_bias);  gelu = p_encoder_layer_0_output_dense_weight = p_encoder_layer_0_output_dense_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_5: \"f16[1, s53, 768]\" = torch.ops.aten.clone.default(linear_5);  linear_5 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:277 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
       "                    add_272: \"f16[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_5, layer_norm_1);  clone_5 = layer_norm_1 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_2: \"f16[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_272, [768], p_encoder_layer_0_output_layernorm_weight, p_encoder_layer_0_output_layernorm_bias);  add_272 = p_encoder_layer_0_output_layernorm_weight = p_encoder_layer_0_output_layernorm_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_6: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_2, p_encoder_layer_1_attention_attn_q_weight, p_encoder_layer_1_attention_attn_q_bias);  p_encoder_layer_1_attention_attn_q_weight = p_encoder_layer_1_attention_attn_q_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:159 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_4: \"f16[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_6, [sym_size_int_85, -1, 12, 64]);  linear_6 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:160 in forward, code: .transpose(1, 2)\n",
       "                    transpose_4: \"f16[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_4, 1, 2);  view_4 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_7: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_2, p_encoder_layer_1_attention_attn_k_weight, p_encoder_layer_1_attention_attn_k_bias);  p_encoder_layer_1_attention_attn_k_weight = p_encoder_layer_1_attention_attn_k_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:164 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_5: \"f16[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_7, [sym_size_int_85, -1, 12, 64]);  linear_7 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:165 in forward, code: .transpose(1, 2)\n",
       "                    transpose_5: \"f16[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_5, 1, 2);  view_5 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_8: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_2, p_encoder_layer_1_attention_attn_v_weight, p_encoder_layer_1_attention_attn_v_bias);  p_encoder_layer_1_attention_attn_v_weight = p_encoder_layer_1_attention_attn_v_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:169 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_6: \"f16[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_8, [sym_size_int_85, -1, 12, 64]);  linear_8 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:170 in forward, code: .transpose(1, 2)\n",
       "                    transpose_6: \"f16[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_6, 1, 2);  view_6 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:174 in forward, code: attention_scores = torch.matmul(q, k.transpose(-1, -2))\n",
       "                    transpose_7: \"f16[1, 12, 64, s53]\" = torch.ops.aten.transpose.int(transpose_5, -1, -2);  transpose_5 = None\n",
       "                    matmul_2: \"f16[1, 12, s53, s53]\" = torch.ops.aten.matmul.default(transpose_4, transpose_7);  transpose_4 = transpose_7 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:175 in forward, code: attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
       "                    scalar_tensor_default_5: \"f16[]\" = torch.ops.aten.scalar_tensor.default(8.0, dtype = torch.float16)\n",
       "                    div_3: \"f16[1, 12, s53, s53]\" = torch.ops.aten.div.Tensor(matmul_2, scalar_tensor_default_5);  matmul_2 = scalar_tensor_default_5 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:179 in forward, code: attention_scores += position_bias\n",
       "                    add_328: \"f16[s43, 12, s53, s53]\" = torch.ops.aten.add.Tensor(div_3, clone_1);  div_3 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:182 in forward, code: attention_scores = attention_scores + attention_mask\n",
       "                    add_334: \"f16[1, 12, s53, s53]\" = torch.ops.aten.add.Tensor(add_328, mul_22);  add_328 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:185 in forward, code: attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
       "                    softmax_1: \"f16[1, 12, s53, s53]\" = torch.ops.aten.softmax.int(add_334, -1);  add_334 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_6: \"f16[1, 12, s53, s53]\" = torch.ops.aten.clone.default(softmax_1);  softmax_1 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:192 in forward, code: c = torch.matmul(attention_probs, v)\n",
       "                    matmul_3: \"f16[1, 12, s53, 64]\" = torch.ops.aten.matmul.default(clone_6, transpose_6);  clone_6 = transpose_6 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:194 in forward, code: c = c.permute(0, 2, 1, 3).contiguous()\n",
       "                    permute_2: \"f16[1, s53, 12, 64]\" = torch.ops.aten.permute.default(matmul_3, [0, 2, 1, 3]);  matmul_3 = None\n",
       "                    clone_7: \"f16[1, s53, 12, 64]\" = torch.ops.aten.clone.default(permute_2, memory_format = torch.contiguous_format);  permute_2 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:196 in forward, code: c = c.view(*new_c_shape)\n",
       "                    view_7: \"f16[1, s53, 768]\" = torch.ops.aten.view.default(clone_7, [sym_size_int_85, sym_size_int_86, 768]);  clone_7 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_9: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(view_7, p_encoder_layer_1_attention_attn_o_weight, p_encoder_layer_1_attention_attn_o_bias);  view_7 = p_encoder_layer_1_attention_attn_o_weight = p_encoder_layer_1_attention_attn_o_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_8: \"f16[1, s53, 768]\" = torch.ops.aten.clone.default(linear_9);  linear_9 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:245 in forward, code: attention_output = self.LayerNorm(self.dropout(self_outputs[0]) + hidden_states)\n",
       "                    add_368: \"f16[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_8, layer_norm_2);  clone_8 = layer_norm_2 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_3: \"f16[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_368, [768], p_encoder_layer_1_attention_layernorm_weight, p_encoder_layer_1_attention_layernorm_bias);  add_368 = p_encoder_layer_1_attention_layernorm_weight = p_encoder_layer_1_attention_layernorm_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_10: \"f16[1, s53, 3072]\" = torch.ops.aten.linear.default(layer_norm_3, p_encoder_layer_1_intermediate_dense_weight, p_encoder_layer_1_intermediate_dense_bias);  p_encoder_layer_1_intermediate_dense_weight = p_encoder_layer_1_intermediate_dense_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\activations.py:85 in forward, code: return self.act(input)\n",
       "                    gelu_1: \"f16[1, s53, 3072]\" = torch.ops.aten.gelu.default(linear_10);  linear_10 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_11: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(gelu_1, p_encoder_layer_1_output_dense_weight, p_encoder_layer_1_output_dense_bias);  gelu_1 = p_encoder_layer_1_output_dense_weight = p_encoder_layer_1_output_dense_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_9: \"f16[1, s53, 768]\" = torch.ops.aten.clone.default(linear_11);  linear_11 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:277 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
       "                    add_387: \"f16[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_9, layer_norm_3);  clone_9 = layer_norm_3 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_4: \"f16[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_387, [768], p_encoder_layer_1_output_layernorm_weight, p_encoder_layer_1_output_layernorm_bias);  add_387 = p_encoder_layer_1_output_layernorm_weight = p_encoder_layer_1_output_layernorm_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_12: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_4, p_encoder_layer_2_attention_attn_q_weight, p_encoder_layer_2_attention_attn_q_bias);  p_encoder_layer_2_attention_attn_q_weight = p_encoder_layer_2_attention_attn_q_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:159 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_8: \"f16[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_12, [sym_size_int_85, -1, 12, 64]);  linear_12 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:160 in forward, code: .transpose(1, 2)\n",
       "                    transpose_8: \"f16[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_8, 1, 2);  view_8 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_13: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_4, p_encoder_layer_2_attention_attn_k_weight, p_encoder_layer_2_attention_attn_k_bias);  p_encoder_layer_2_attention_attn_k_weight = p_encoder_layer_2_attention_attn_k_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:164 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_9: \"f16[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_13, [sym_size_int_85, -1, 12, 64]);  linear_13 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:165 in forward, code: .transpose(1, 2)\n",
       "                    transpose_9: \"f16[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_9, 1, 2);  view_9 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_14: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_4, p_encoder_layer_2_attention_attn_v_weight, p_encoder_layer_2_attention_attn_v_bias);  p_encoder_layer_2_attention_attn_v_weight = p_encoder_layer_2_attention_attn_v_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:169 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_10: \"f16[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_14, [sym_size_int_85, -1, 12, 64]);  linear_14 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:170 in forward, code: .transpose(1, 2)\n",
       "                    transpose_10: \"f16[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_10, 1, 2);  view_10 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:174 in forward, code: attention_scores = torch.matmul(q, k.transpose(-1, -2))\n",
       "                    transpose_11: \"f16[1, 12, 64, s53]\" = torch.ops.aten.transpose.int(transpose_9, -1, -2);  transpose_9 = None\n",
       "                    matmul_4: \"f16[1, 12, s53, s53]\" = torch.ops.aten.matmul.default(transpose_8, transpose_11);  transpose_8 = transpose_11 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:175 in forward, code: attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
       "                    scalar_tensor_default_6: \"f16[]\" = torch.ops.aten.scalar_tensor.default(8.0, dtype = torch.float16)\n",
       "                    div_4: \"f16[1, 12, s53, s53]\" = torch.ops.aten.div.Tensor(matmul_4, scalar_tensor_default_6);  matmul_4 = scalar_tensor_default_6 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:179 in forward, code: attention_scores += position_bias\n",
       "                    add_443: \"f16[s43, 12, s53, s53]\" = torch.ops.aten.add.Tensor(div_4, clone_1);  div_4 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:182 in forward, code: attention_scores = attention_scores + attention_mask\n",
       "                    add_449: \"f16[1, 12, s53, s53]\" = torch.ops.aten.add.Tensor(add_443, mul_22);  add_443 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:185 in forward, code: attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
       "                    softmax_2: \"f16[1, 12, s53, s53]\" = torch.ops.aten.softmax.int(add_449, -1);  add_449 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_10: \"f16[1, 12, s53, s53]\" = torch.ops.aten.clone.default(softmax_2);  softmax_2 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:192 in forward, code: c = torch.matmul(attention_probs, v)\n",
       "                    matmul_5: \"f16[1, 12, s53, 64]\" = torch.ops.aten.matmul.default(clone_10, transpose_10);  clone_10 = transpose_10 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:194 in forward, code: c = c.permute(0, 2, 1, 3).contiguous()\n",
       "                    permute_3: \"f16[1, s53, 12, 64]\" = torch.ops.aten.permute.default(matmul_5, [0, 2, 1, 3]);  matmul_5 = None\n",
       "                    clone_11: \"f16[1, s53, 12, 64]\" = torch.ops.aten.clone.default(permute_3, memory_format = torch.contiguous_format);  permute_3 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:196 in forward, code: c = c.view(*new_c_shape)\n",
       "                    view_11: \"f16[1, s53, 768]\" = torch.ops.aten.view.default(clone_11, [sym_size_int_85, sym_size_int_86, 768]);  clone_11 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_15: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(view_11, p_encoder_layer_2_attention_attn_o_weight, p_encoder_layer_2_attention_attn_o_bias);  view_11 = p_encoder_layer_2_attention_attn_o_weight = p_encoder_layer_2_attention_attn_o_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_12: \"f16[1, s53, 768]\" = torch.ops.aten.clone.default(linear_15);  linear_15 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:245 in forward, code: attention_output = self.LayerNorm(self.dropout(self_outputs[0]) + hidden_states)\n",
       "                    add_483: \"f16[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_12, layer_norm_4);  clone_12 = layer_norm_4 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_5: \"f16[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_483, [768], p_encoder_layer_2_attention_layernorm_weight, p_encoder_layer_2_attention_layernorm_bias);  add_483 = p_encoder_layer_2_attention_layernorm_weight = p_encoder_layer_2_attention_layernorm_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_16: \"f16[1, s53, 3072]\" = torch.ops.aten.linear.default(layer_norm_5, p_encoder_layer_2_intermediate_dense_weight, p_encoder_layer_2_intermediate_dense_bias);  p_encoder_layer_2_intermediate_dense_weight = p_encoder_layer_2_intermediate_dense_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\activations.py:85 in forward, code: return self.act(input)\n",
       "                    gelu_2: \"f16[1, s53, 3072]\" = torch.ops.aten.gelu.default(linear_16);  linear_16 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_17: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(gelu_2, p_encoder_layer_2_output_dense_weight, p_encoder_layer_2_output_dense_bias);  gelu_2 = p_encoder_layer_2_output_dense_weight = p_encoder_layer_2_output_dense_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_13: \"f16[1, s53, 768]\" = torch.ops.aten.clone.default(linear_17);  linear_17 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:277 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
       "                    add_502: \"f16[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_13, layer_norm_5);  clone_13 = layer_norm_5 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_6: \"f16[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_502, [768], p_encoder_layer_2_output_layernorm_weight, p_encoder_layer_2_output_layernorm_bias);  add_502 = p_encoder_layer_2_output_layernorm_weight = p_encoder_layer_2_output_layernorm_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_18: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_6, p_encoder_layer_3_attention_attn_q_weight, p_encoder_layer_3_attention_attn_q_bias);  p_encoder_layer_3_attention_attn_q_weight = p_encoder_layer_3_attention_attn_q_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:159 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_12: \"f16[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_18, [sym_size_int_85, -1, 12, 64]);  linear_18 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:160 in forward, code: .transpose(1, 2)\n",
       "                    transpose_12: \"f16[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_12, 1, 2);  view_12 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_19: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_6, p_encoder_layer_3_attention_attn_k_weight, p_encoder_layer_3_attention_attn_k_bias);  p_encoder_layer_3_attention_attn_k_weight = p_encoder_layer_3_attention_attn_k_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:164 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_13: \"f16[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_19, [sym_size_int_85, -1, 12, 64]);  linear_19 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:165 in forward, code: .transpose(1, 2)\n",
       "                    transpose_13: \"f16[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_13, 1, 2);  view_13 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_20: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_6, p_encoder_layer_3_attention_attn_v_weight, p_encoder_layer_3_attention_attn_v_bias);  p_encoder_layer_3_attention_attn_v_weight = p_encoder_layer_3_attention_attn_v_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:169 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_14: \"f16[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_20, [sym_size_int_85, -1, 12, 64]);  linear_20 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:170 in forward, code: .transpose(1, 2)\n",
       "                    transpose_14: \"f16[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_14, 1, 2);  view_14 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:174 in forward, code: attention_scores = torch.matmul(q, k.transpose(-1, -2))\n",
       "                    transpose_15: \"f16[1, 12, 64, s53]\" = torch.ops.aten.transpose.int(transpose_13, -1, -2);  transpose_13 = None\n",
       "                    matmul_6: \"f16[1, 12, s53, s53]\" = torch.ops.aten.matmul.default(transpose_12, transpose_15);  transpose_12 = transpose_15 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:175 in forward, code: attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
       "                    scalar_tensor_default_7: \"f16[]\" = torch.ops.aten.scalar_tensor.default(8.0, dtype = torch.float16)\n",
       "                    div_5: \"f16[1, 12, s53, s53]\" = torch.ops.aten.div.Tensor(matmul_6, scalar_tensor_default_7);  matmul_6 = scalar_tensor_default_7 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:179 in forward, code: attention_scores += position_bias\n",
       "                    add_558: \"f16[s43, 12, s53, s53]\" = torch.ops.aten.add.Tensor(div_5, clone_1);  div_5 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:182 in forward, code: attention_scores = attention_scores + attention_mask\n",
       "                    add_564: \"f16[1, 12, s53, s53]\" = torch.ops.aten.add.Tensor(add_558, mul_22);  add_558 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:185 in forward, code: attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
       "                    softmax_3: \"f16[1, 12, s53, s53]\" = torch.ops.aten.softmax.int(add_564, -1);  add_564 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_14: \"f16[1, 12, s53, s53]\" = torch.ops.aten.clone.default(softmax_3);  softmax_3 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:192 in forward, code: c = torch.matmul(attention_probs, v)\n",
       "                    matmul_7: \"f16[1, 12, s53, 64]\" = torch.ops.aten.matmul.default(clone_14, transpose_14);  clone_14 = transpose_14 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:194 in forward, code: c = c.permute(0, 2, 1, 3).contiguous()\n",
       "                    permute_4: \"f16[1, s53, 12, 64]\" = torch.ops.aten.permute.default(matmul_7, [0, 2, 1, 3]);  matmul_7 = None\n",
       "                    clone_15: \"f16[1, s53, 12, 64]\" = torch.ops.aten.clone.default(permute_4, memory_format = torch.contiguous_format);  permute_4 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:196 in forward, code: c = c.view(*new_c_shape)\n",
       "                    view_15: \"f16[1, s53, 768]\" = torch.ops.aten.view.default(clone_15, [sym_size_int_85, sym_size_int_86, 768]);  clone_15 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_21: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(view_15, p_encoder_layer_3_attention_attn_o_weight, p_encoder_layer_3_attention_attn_o_bias);  view_15 = p_encoder_layer_3_attention_attn_o_weight = p_encoder_layer_3_attention_attn_o_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_16: \"f16[1, s53, 768]\" = torch.ops.aten.clone.default(linear_21);  linear_21 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:245 in forward, code: attention_output = self.LayerNorm(self.dropout(self_outputs[0]) + hidden_states)\n",
       "                    add_598: \"f16[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_16, layer_norm_6);  clone_16 = layer_norm_6 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_7: \"f16[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_598, [768], p_encoder_layer_3_attention_layernorm_weight, p_encoder_layer_3_attention_layernorm_bias);  add_598 = p_encoder_layer_3_attention_layernorm_weight = p_encoder_layer_3_attention_layernorm_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_22: \"f16[1, s53, 3072]\" = torch.ops.aten.linear.default(layer_norm_7, p_encoder_layer_3_intermediate_dense_weight, p_encoder_layer_3_intermediate_dense_bias);  p_encoder_layer_3_intermediate_dense_weight = p_encoder_layer_3_intermediate_dense_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\activations.py:85 in forward, code: return self.act(input)\n",
       "                    gelu_3: \"f16[1, s53, 3072]\" = torch.ops.aten.gelu.default(linear_22);  linear_22 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_23: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(gelu_3, p_encoder_layer_3_output_dense_weight, p_encoder_layer_3_output_dense_bias);  gelu_3 = p_encoder_layer_3_output_dense_weight = p_encoder_layer_3_output_dense_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_17: \"f16[1, s53, 768]\" = torch.ops.aten.clone.default(linear_23);  linear_23 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:277 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
       "                    add_617: \"f16[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_17, layer_norm_7);  clone_17 = layer_norm_7 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_8: \"f16[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_617, [768], p_encoder_layer_3_output_layernorm_weight, p_encoder_layer_3_output_layernorm_bias);  add_617 = p_encoder_layer_3_output_layernorm_weight = p_encoder_layer_3_output_layernorm_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_24: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_8, p_encoder_layer_4_attention_attn_q_weight, p_encoder_layer_4_attention_attn_q_bias);  p_encoder_layer_4_attention_attn_q_weight = p_encoder_layer_4_attention_attn_q_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:159 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_16: \"f16[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_24, [sym_size_int_85, -1, 12, 64]);  linear_24 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:160 in forward, code: .transpose(1, 2)\n",
       "                    transpose_16: \"f16[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_16, 1, 2);  view_16 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_25: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_8, p_encoder_layer_4_attention_attn_k_weight, p_encoder_layer_4_attention_attn_k_bias);  p_encoder_layer_4_attention_attn_k_weight = p_encoder_layer_4_attention_attn_k_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:164 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_17: \"f16[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_25, [sym_size_int_85, -1, 12, 64]);  linear_25 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:165 in forward, code: .transpose(1, 2)\n",
       "                    transpose_17: \"f16[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_17, 1, 2);  view_17 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_26: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_8, p_encoder_layer_4_attention_attn_v_weight, p_encoder_layer_4_attention_attn_v_bias);  p_encoder_layer_4_attention_attn_v_weight = p_encoder_layer_4_attention_attn_v_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:169 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_18: \"f16[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_26, [sym_size_int_85, -1, 12, 64]);  linear_26 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:170 in forward, code: .transpose(1, 2)\n",
       "                    transpose_18: \"f16[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_18, 1, 2);  view_18 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:174 in forward, code: attention_scores = torch.matmul(q, k.transpose(-1, -2))\n",
       "                    transpose_19: \"f16[1, 12, 64, s53]\" = torch.ops.aten.transpose.int(transpose_17, -1, -2);  transpose_17 = None\n",
       "                    matmul_8: \"f16[1, 12, s53, s53]\" = torch.ops.aten.matmul.default(transpose_16, transpose_19);  transpose_16 = transpose_19 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:175 in forward, code: attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
       "                    scalar_tensor_default_8: \"f16[]\" = torch.ops.aten.scalar_tensor.default(8.0, dtype = torch.float16)\n",
       "                    div_6: \"f16[1, 12, s53, s53]\" = torch.ops.aten.div.Tensor(matmul_8, scalar_tensor_default_8);  matmul_8 = scalar_tensor_default_8 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:179 in forward, code: attention_scores += position_bias\n",
       "                    add_673: \"f16[s43, 12, s53, s53]\" = torch.ops.aten.add.Tensor(div_6, clone_1);  div_6 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:182 in forward, code: attention_scores = attention_scores + attention_mask\n",
       "                    add_679: \"f16[1, 12, s53, s53]\" = torch.ops.aten.add.Tensor(add_673, mul_22);  add_673 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:185 in forward, code: attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
       "                    softmax_4: \"f16[1, 12, s53, s53]\" = torch.ops.aten.softmax.int(add_679, -1);  add_679 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_18: \"f16[1, 12, s53, s53]\" = torch.ops.aten.clone.default(softmax_4);  softmax_4 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:192 in forward, code: c = torch.matmul(attention_probs, v)\n",
       "                    matmul_9: \"f16[1, 12, s53, 64]\" = torch.ops.aten.matmul.default(clone_18, transpose_18);  clone_18 = transpose_18 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:194 in forward, code: c = c.permute(0, 2, 1, 3).contiguous()\n",
       "                    permute_5: \"f16[1, s53, 12, 64]\" = torch.ops.aten.permute.default(matmul_9, [0, 2, 1, 3]);  matmul_9 = None\n",
       "                    clone_19: \"f16[1, s53, 12, 64]\" = torch.ops.aten.clone.default(permute_5, memory_format = torch.contiguous_format);  permute_5 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:196 in forward, code: c = c.view(*new_c_shape)\n",
       "                    view_19: \"f16[1, s53, 768]\" = torch.ops.aten.view.default(clone_19, [sym_size_int_85, sym_size_int_86, 768]);  clone_19 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_27: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(view_19, p_encoder_layer_4_attention_attn_o_weight, p_encoder_layer_4_attention_attn_o_bias);  view_19 = p_encoder_layer_4_attention_attn_o_weight = p_encoder_layer_4_attention_attn_o_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_20: \"f16[1, s53, 768]\" = torch.ops.aten.clone.default(linear_27);  linear_27 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:245 in forward, code: attention_output = self.LayerNorm(self.dropout(self_outputs[0]) + hidden_states)\n",
       "                    add_713: \"f16[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_20, layer_norm_8);  clone_20 = layer_norm_8 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_9: \"f16[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_713, [768], p_encoder_layer_4_attention_layernorm_weight, p_encoder_layer_4_attention_layernorm_bias);  add_713 = p_encoder_layer_4_attention_layernorm_weight = p_encoder_layer_4_attention_layernorm_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_28: \"f16[1, s53, 3072]\" = torch.ops.aten.linear.default(layer_norm_9, p_encoder_layer_4_intermediate_dense_weight, p_encoder_layer_4_intermediate_dense_bias);  p_encoder_layer_4_intermediate_dense_weight = p_encoder_layer_4_intermediate_dense_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\activations.py:85 in forward, code: return self.act(input)\n",
       "                    gelu_4: \"f16[1, s53, 3072]\" = torch.ops.aten.gelu.default(linear_28);  linear_28 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_29: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(gelu_4, p_encoder_layer_4_output_dense_weight, p_encoder_layer_4_output_dense_bias);  gelu_4 = p_encoder_layer_4_output_dense_weight = p_encoder_layer_4_output_dense_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_21: \"f16[1, s53, 768]\" = torch.ops.aten.clone.default(linear_29);  linear_29 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:277 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
       "                    add_732: \"f16[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_21, layer_norm_9);  clone_21 = layer_norm_9 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_10: \"f16[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_732, [768], p_encoder_layer_4_output_layernorm_weight, p_encoder_layer_4_output_layernorm_bias);  add_732 = p_encoder_layer_4_output_layernorm_weight = p_encoder_layer_4_output_layernorm_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_30: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_10, p_encoder_layer_5_attention_attn_q_weight, p_encoder_layer_5_attention_attn_q_bias);  p_encoder_layer_5_attention_attn_q_weight = p_encoder_layer_5_attention_attn_q_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:159 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_20: \"f16[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_30, [sym_size_int_85, -1, 12, 64]);  linear_30 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:160 in forward, code: .transpose(1, 2)\n",
       "                    transpose_20: \"f16[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_20, 1, 2);  view_20 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_31: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_10, p_encoder_layer_5_attention_attn_k_weight, p_encoder_layer_5_attention_attn_k_bias);  p_encoder_layer_5_attention_attn_k_weight = p_encoder_layer_5_attention_attn_k_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:164 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_21: \"f16[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_31, [sym_size_int_85, -1, 12, 64]);  linear_31 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:165 in forward, code: .transpose(1, 2)\n",
       "                    transpose_21: \"f16[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_21, 1, 2);  view_21 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_32: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_10, p_encoder_layer_5_attention_attn_v_weight, p_encoder_layer_5_attention_attn_v_bias);  p_encoder_layer_5_attention_attn_v_weight = p_encoder_layer_5_attention_attn_v_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:169 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_22: \"f16[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_32, [sym_size_int_85, -1, 12, 64]);  linear_32 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:170 in forward, code: .transpose(1, 2)\n",
       "                    transpose_22: \"f16[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_22, 1, 2);  view_22 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:174 in forward, code: attention_scores = torch.matmul(q, k.transpose(-1, -2))\n",
       "                    transpose_23: \"f16[1, 12, 64, s53]\" = torch.ops.aten.transpose.int(transpose_21, -1, -2);  transpose_21 = None\n",
       "                    matmul_10: \"f16[1, 12, s53, s53]\" = torch.ops.aten.matmul.default(transpose_20, transpose_23);  transpose_20 = transpose_23 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:175 in forward, code: attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
       "                    scalar_tensor_default_9: \"f16[]\" = torch.ops.aten.scalar_tensor.default(8.0, dtype = torch.float16)\n",
       "                    div_7: \"f16[1, 12, s53, s53]\" = torch.ops.aten.div.Tensor(matmul_10, scalar_tensor_default_9);  matmul_10 = scalar_tensor_default_9 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:179 in forward, code: attention_scores += position_bias\n",
       "                    add_788: \"f16[s43, 12, s53, s53]\" = torch.ops.aten.add.Tensor(div_7, clone_1);  div_7 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:182 in forward, code: attention_scores = attention_scores + attention_mask\n",
       "                    add_794: \"f16[1, 12, s53, s53]\" = torch.ops.aten.add.Tensor(add_788, mul_22);  add_788 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:185 in forward, code: attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
       "                    softmax_5: \"f16[1, 12, s53, s53]\" = torch.ops.aten.softmax.int(add_794, -1);  add_794 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_22: \"f16[1, 12, s53, s53]\" = torch.ops.aten.clone.default(softmax_5);  softmax_5 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:192 in forward, code: c = torch.matmul(attention_probs, v)\n",
       "                    matmul_11: \"f16[1, 12, s53, 64]\" = torch.ops.aten.matmul.default(clone_22, transpose_22);  clone_22 = transpose_22 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:194 in forward, code: c = c.permute(0, 2, 1, 3).contiguous()\n",
       "                    permute_6: \"f16[1, s53, 12, 64]\" = torch.ops.aten.permute.default(matmul_11, [0, 2, 1, 3]);  matmul_11 = None\n",
       "                    clone_23: \"f16[1, s53, 12, 64]\" = torch.ops.aten.clone.default(permute_6, memory_format = torch.contiguous_format);  permute_6 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:196 in forward, code: c = c.view(*new_c_shape)\n",
       "                    view_23: \"f16[1, s53, 768]\" = torch.ops.aten.view.default(clone_23, [sym_size_int_85, sym_size_int_86, 768]);  clone_23 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_33: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(view_23, p_encoder_layer_5_attention_attn_o_weight, p_encoder_layer_5_attention_attn_o_bias);  view_23 = p_encoder_layer_5_attention_attn_o_weight = p_encoder_layer_5_attention_attn_o_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_24: \"f16[1, s53, 768]\" = torch.ops.aten.clone.default(linear_33);  linear_33 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:245 in forward, code: attention_output = self.LayerNorm(self.dropout(self_outputs[0]) + hidden_states)\n",
       "                    add_828: \"f16[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_24, layer_norm_10);  clone_24 = layer_norm_10 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_11: \"f16[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_828, [768], p_encoder_layer_5_attention_layernorm_weight, p_encoder_layer_5_attention_layernorm_bias);  add_828 = p_encoder_layer_5_attention_layernorm_weight = p_encoder_layer_5_attention_layernorm_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_34: \"f16[1, s53, 3072]\" = torch.ops.aten.linear.default(layer_norm_11, p_encoder_layer_5_intermediate_dense_weight, p_encoder_layer_5_intermediate_dense_bias);  p_encoder_layer_5_intermediate_dense_weight = p_encoder_layer_5_intermediate_dense_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\activations.py:85 in forward, code: return self.act(input)\n",
       "                    gelu_5: \"f16[1, s53, 3072]\" = torch.ops.aten.gelu.default(linear_34);  linear_34 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_35: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(gelu_5, p_encoder_layer_5_output_dense_weight, p_encoder_layer_5_output_dense_bias);  gelu_5 = p_encoder_layer_5_output_dense_weight = p_encoder_layer_5_output_dense_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_25: \"f16[1, s53, 768]\" = torch.ops.aten.clone.default(linear_35);  linear_35 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:277 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
       "                    add_847: \"f16[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_25, layer_norm_11);  clone_25 = layer_norm_11 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_12: \"f16[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_847, [768], p_encoder_layer_5_output_layernorm_weight, p_encoder_layer_5_output_layernorm_bias);  add_847 = p_encoder_layer_5_output_layernorm_weight = p_encoder_layer_5_output_layernorm_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_36: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_12, p_encoder_layer_6_attention_attn_q_weight, p_encoder_layer_6_attention_attn_q_bias);  p_encoder_layer_6_attention_attn_q_weight = p_encoder_layer_6_attention_attn_q_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:159 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_24: \"f16[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_36, [sym_size_int_85, -1, 12, 64]);  linear_36 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:160 in forward, code: .transpose(1, 2)\n",
       "                    transpose_24: \"f16[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_24, 1, 2);  view_24 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_37: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_12, p_encoder_layer_6_attention_attn_k_weight, p_encoder_layer_6_attention_attn_k_bias);  p_encoder_layer_6_attention_attn_k_weight = p_encoder_layer_6_attention_attn_k_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:164 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_25: \"f16[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_37, [sym_size_int_85, -1, 12, 64]);  linear_37 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:165 in forward, code: .transpose(1, 2)\n",
       "                    transpose_25: \"f16[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_25, 1, 2);  view_25 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_38: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_12, p_encoder_layer_6_attention_attn_v_weight, p_encoder_layer_6_attention_attn_v_bias);  p_encoder_layer_6_attention_attn_v_weight = p_encoder_layer_6_attention_attn_v_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:169 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_26: \"f16[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_38, [sym_size_int_85, -1, 12, 64]);  linear_38 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:170 in forward, code: .transpose(1, 2)\n",
       "                    transpose_26: \"f16[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_26, 1, 2);  view_26 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:174 in forward, code: attention_scores = torch.matmul(q, k.transpose(-1, -2))\n",
       "                    transpose_27: \"f16[1, 12, 64, s53]\" = torch.ops.aten.transpose.int(transpose_25, -1, -2);  transpose_25 = None\n",
       "                    matmul_12: \"f16[1, 12, s53, s53]\" = torch.ops.aten.matmul.default(transpose_24, transpose_27);  transpose_24 = transpose_27 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:175 in forward, code: attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
       "                    scalar_tensor_default_10: \"f16[]\" = torch.ops.aten.scalar_tensor.default(8.0, dtype = torch.float16)\n",
       "                    div_8: \"f16[1, 12, s53, s53]\" = torch.ops.aten.div.Tensor(matmul_12, scalar_tensor_default_10);  matmul_12 = scalar_tensor_default_10 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:179 in forward, code: attention_scores += position_bias\n",
       "                    add_903: \"f16[s43, 12, s53, s53]\" = torch.ops.aten.add.Tensor(div_8, clone_1);  div_8 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:182 in forward, code: attention_scores = attention_scores + attention_mask\n",
       "                    add_909: \"f16[1, 12, s53, s53]\" = torch.ops.aten.add.Tensor(add_903, mul_22);  add_903 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:185 in forward, code: attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
       "                    softmax_6: \"f16[1, 12, s53, s53]\" = torch.ops.aten.softmax.int(add_909, -1);  add_909 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_26: \"f16[1, 12, s53, s53]\" = torch.ops.aten.clone.default(softmax_6);  softmax_6 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:192 in forward, code: c = torch.matmul(attention_probs, v)\n",
       "                    matmul_13: \"f16[1, 12, s53, 64]\" = torch.ops.aten.matmul.default(clone_26, transpose_26);  clone_26 = transpose_26 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:194 in forward, code: c = c.permute(0, 2, 1, 3).contiguous()\n",
       "                    permute_7: \"f16[1, s53, 12, 64]\" = torch.ops.aten.permute.default(matmul_13, [0, 2, 1, 3]);  matmul_13 = None\n",
       "                    clone_27: \"f16[1, s53, 12, 64]\" = torch.ops.aten.clone.default(permute_7, memory_format = torch.contiguous_format);  permute_7 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:196 in forward, code: c = c.view(*new_c_shape)\n",
       "                    view_27: \"f16[1, s53, 768]\" = torch.ops.aten.view.default(clone_27, [sym_size_int_85, sym_size_int_86, 768]);  clone_27 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_39: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(view_27, p_encoder_layer_6_attention_attn_o_weight, p_encoder_layer_6_attention_attn_o_bias);  view_27 = p_encoder_layer_6_attention_attn_o_weight = p_encoder_layer_6_attention_attn_o_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_28: \"f16[1, s53, 768]\" = torch.ops.aten.clone.default(linear_39);  linear_39 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:245 in forward, code: attention_output = self.LayerNorm(self.dropout(self_outputs[0]) + hidden_states)\n",
       "                    add_943: \"f16[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_28, layer_norm_12);  clone_28 = layer_norm_12 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_13: \"f16[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_943, [768], p_encoder_layer_6_attention_layernorm_weight, p_encoder_layer_6_attention_layernorm_bias);  add_943 = p_encoder_layer_6_attention_layernorm_weight = p_encoder_layer_6_attention_layernorm_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_40: \"f16[1, s53, 3072]\" = torch.ops.aten.linear.default(layer_norm_13, p_encoder_layer_6_intermediate_dense_weight, p_encoder_layer_6_intermediate_dense_bias);  p_encoder_layer_6_intermediate_dense_weight = p_encoder_layer_6_intermediate_dense_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\activations.py:85 in forward, code: return self.act(input)\n",
       "                    gelu_6: \"f16[1, s53, 3072]\" = torch.ops.aten.gelu.default(linear_40);  linear_40 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_41: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(gelu_6, p_encoder_layer_6_output_dense_weight, p_encoder_layer_6_output_dense_bias);  gelu_6 = p_encoder_layer_6_output_dense_weight = p_encoder_layer_6_output_dense_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_29: \"f16[1, s53, 768]\" = torch.ops.aten.clone.default(linear_41);  linear_41 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:277 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
       "                    add_962: \"f16[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_29, layer_norm_13);  clone_29 = layer_norm_13 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_14: \"f16[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_962, [768], p_encoder_layer_6_output_layernorm_weight, p_encoder_layer_6_output_layernorm_bias);  add_962 = p_encoder_layer_6_output_layernorm_weight = p_encoder_layer_6_output_layernorm_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_42: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_14, p_encoder_layer_7_attention_attn_q_weight, p_encoder_layer_7_attention_attn_q_bias);  p_encoder_layer_7_attention_attn_q_weight = p_encoder_layer_7_attention_attn_q_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:159 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_28: \"f16[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_42, [sym_size_int_85, -1, 12, 64]);  linear_42 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:160 in forward, code: .transpose(1, 2)\n",
       "                    transpose_28: \"f16[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_28, 1, 2);  view_28 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_43: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_14, p_encoder_layer_7_attention_attn_k_weight, p_encoder_layer_7_attention_attn_k_bias);  p_encoder_layer_7_attention_attn_k_weight = p_encoder_layer_7_attention_attn_k_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:164 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_29: \"f16[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_43, [sym_size_int_85, -1, 12, 64]);  linear_43 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:165 in forward, code: .transpose(1, 2)\n",
       "                    transpose_29: \"f16[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_29, 1, 2);  view_29 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_44: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_14, p_encoder_layer_7_attention_attn_v_weight, p_encoder_layer_7_attention_attn_v_bias);  p_encoder_layer_7_attention_attn_v_weight = p_encoder_layer_7_attention_attn_v_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:169 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_30: \"f16[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_44, [sym_size_int_85, -1, 12, 64]);  linear_44 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:170 in forward, code: .transpose(1, 2)\n",
       "                    transpose_30: \"f16[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_30, 1, 2);  view_30 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:174 in forward, code: attention_scores = torch.matmul(q, k.transpose(-1, -2))\n",
       "                    transpose_31: \"f16[1, 12, 64, s53]\" = torch.ops.aten.transpose.int(transpose_29, -1, -2);  transpose_29 = None\n",
       "                    matmul_14: \"f16[1, 12, s53, s53]\" = torch.ops.aten.matmul.default(transpose_28, transpose_31);  transpose_28 = transpose_31 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:175 in forward, code: attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
       "                    scalar_tensor_default_11: \"f16[]\" = torch.ops.aten.scalar_tensor.default(8.0, dtype = torch.float16)\n",
       "                    div_9: \"f16[1, 12, s53, s53]\" = torch.ops.aten.div.Tensor(matmul_14, scalar_tensor_default_11);  matmul_14 = scalar_tensor_default_11 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:179 in forward, code: attention_scores += position_bias\n",
       "                    add_1018: \"f16[s43, 12, s53, s53]\" = torch.ops.aten.add.Tensor(div_9, clone_1);  div_9 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:182 in forward, code: attention_scores = attention_scores + attention_mask\n",
       "                    add_1024: \"f16[1, 12, s53, s53]\" = torch.ops.aten.add.Tensor(add_1018, mul_22);  add_1018 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:185 in forward, code: attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
       "                    softmax_7: \"f16[1, 12, s53, s53]\" = torch.ops.aten.softmax.int(add_1024, -1);  add_1024 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_30: \"f16[1, 12, s53, s53]\" = torch.ops.aten.clone.default(softmax_7);  softmax_7 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:192 in forward, code: c = torch.matmul(attention_probs, v)\n",
       "                    matmul_15: \"f16[1, 12, s53, 64]\" = torch.ops.aten.matmul.default(clone_30, transpose_30);  clone_30 = transpose_30 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:194 in forward, code: c = c.permute(0, 2, 1, 3).contiguous()\n",
       "                    permute_8: \"f16[1, s53, 12, 64]\" = torch.ops.aten.permute.default(matmul_15, [0, 2, 1, 3]);  matmul_15 = None\n",
       "                    clone_31: \"f16[1, s53, 12, 64]\" = torch.ops.aten.clone.default(permute_8, memory_format = torch.contiguous_format);  permute_8 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:196 in forward, code: c = c.view(*new_c_shape)\n",
       "                    view_31: \"f16[1, s53, 768]\" = torch.ops.aten.view.default(clone_31, [sym_size_int_85, sym_size_int_86, 768]);  clone_31 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_45: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(view_31, p_encoder_layer_7_attention_attn_o_weight, p_encoder_layer_7_attention_attn_o_bias);  view_31 = p_encoder_layer_7_attention_attn_o_weight = p_encoder_layer_7_attention_attn_o_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_32: \"f16[1, s53, 768]\" = torch.ops.aten.clone.default(linear_45);  linear_45 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:245 in forward, code: attention_output = self.LayerNorm(self.dropout(self_outputs[0]) + hidden_states)\n",
       "                    add_1058: \"f16[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_32, layer_norm_14);  clone_32 = layer_norm_14 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_15: \"f16[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_1058, [768], p_encoder_layer_7_attention_layernorm_weight, p_encoder_layer_7_attention_layernorm_bias);  add_1058 = p_encoder_layer_7_attention_layernorm_weight = p_encoder_layer_7_attention_layernorm_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_46: \"f16[1, s53, 3072]\" = torch.ops.aten.linear.default(layer_norm_15, p_encoder_layer_7_intermediate_dense_weight, p_encoder_layer_7_intermediate_dense_bias);  p_encoder_layer_7_intermediate_dense_weight = p_encoder_layer_7_intermediate_dense_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\activations.py:85 in forward, code: return self.act(input)\n",
       "                    gelu_7: \"f16[1, s53, 3072]\" = torch.ops.aten.gelu.default(linear_46);  linear_46 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_47: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(gelu_7, p_encoder_layer_7_output_dense_weight, p_encoder_layer_7_output_dense_bias);  gelu_7 = p_encoder_layer_7_output_dense_weight = p_encoder_layer_7_output_dense_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_33: \"f16[1, s53, 768]\" = torch.ops.aten.clone.default(linear_47);  linear_47 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:277 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
       "                    add_1077: \"f16[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_33, layer_norm_15);  clone_33 = layer_norm_15 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_16: \"f16[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_1077, [768], p_encoder_layer_7_output_layernorm_weight, p_encoder_layer_7_output_layernorm_bias);  add_1077 = p_encoder_layer_7_output_layernorm_weight = p_encoder_layer_7_output_layernorm_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_48: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_16, p_encoder_layer_8_attention_attn_q_weight, p_encoder_layer_8_attention_attn_q_bias);  p_encoder_layer_8_attention_attn_q_weight = p_encoder_layer_8_attention_attn_q_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:159 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_32: \"f16[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_48, [sym_size_int_85, -1, 12, 64]);  linear_48 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:160 in forward, code: .transpose(1, 2)\n",
       "                    transpose_32: \"f16[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_32, 1, 2);  view_32 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_49: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_16, p_encoder_layer_8_attention_attn_k_weight, p_encoder_layer_8_attention_attn_k_bias);  p_encoder_layer_8_attention_attn_k_weight = p_encoder_layer_8_attention_attn_k_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:164 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_33: \"f16[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_49, [sym_size_int_85, -1, 12, 64]);  linear_49 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:165 in forward, code: .transpose(1, 2)\n",
       "                    transpose_33: \"f16[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_33, 1, 2);  view_33 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_50: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_16, p_encoder_layer_8_attention_attn_v_weight, p_encoder_layer_8_attention_attn_v_bias);  p_encoder_layer_8_attention_attn_v_weight = p_encoder_layer_8_attention_attn_v_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:169 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_34: \"f16[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_50, [sym_size_int_85, -1, 12, 64]);  linear_50 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:170 in forward, code: .transpose(1, 2)\n",
       "                    transpose_34: \"f16[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_34, 1, 2);  view_34 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:174 in forward, code: attention_scores = torch.matmul(q, k.transpose(-1, -2))\n",
       "                    transpose_35: \"f16[1, 12, 64, s53]\" = torch.ops.aten.transpose.int(transpose_33, -1, -2);  transpose_33 = None\n",
       "                    matmul_16: \"f16[1, 12, s53, s53]\" = torch.ops.aten.matmul.default(transpose_32, transpose_35);  transpose_32 = transpose_35 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:175 in forward, code: attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
       "                    scalar_tensor_default_12: \"f16[]\" = torch.ops.aten.scalar_tensor.default(8.0, dtype = torch.float16)\n",
       "                    div_10: \"f16[1, 12, s53, s53]\" = torch.ops.aten.div.Tensor(matmul_16, scalar_tensor_default_12);  matmul_16 = scalar_tensor_default_12 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:179 in forward, code: attention_scores += position_bias\n",
       "                    add_1133: \"f16[s43, 12, s53, s53]\" = torch.ops.aten.add.Tensor(div_10, clone_1);  div_10 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:182 in forward, code: attention_scores = attention_scores + attention_mask\n",
       "                    add_1139: \"f16[1, 12, s53, s53]\" = torch.ops.aten.add.Tensor(add_1133, mul_22);  add_1133 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:185 in forward, code: attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
       "                    softmax_8: \"f16[1, 12, s53, s53]\" = torch.ops.aten.softmax.int(add_1139, -1);  add_1139 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_34: \"f16[1, 12, s53, s53]\" = torch.ops.aten.clone.default(softmax_8);  softmax_8 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:192 in forward, code: c = torch.matmul(attention_probs, v)\n",
       "                    matmul_17: \"f16[1, 12, s53, 64]\" = torch.ops.aten.matmul.default(clone_34, transpose_34);  clone_34 = transpose_34 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:194 in forward, code: c = c.permute(0, 2, 1, 3).contiguous()\n",
       "                    permute_9: \"f16[1, s53, 12, 64]\" = torch.ops.aten.permute.default(matmul_17, [0, 2, 1, 3]);  matmul_17 = None\n",
       "                    clone_35: \"f16[1, s53, 12, 64]\" = torch.ops.aten.clone.default(permute_9, memory_format = torch.contiguous_format);  permute_9 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:196 in forward, code: c = c.view(*new_c_shape)\n",
       "                    view_35: \"f16[1, s53, 768]\" = torch.ops.aten.view.default(clone_35, [sym_size_int_85, sym_size_int_86, 768]);  clone_35 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_51: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(view_35, p_encoder_layer_8_attention_attn_o_weight, p_encoder_layer_8_attention_attn_o_bias);  view_35 = p_encoder_layer_8_attention_attn_o_weight = p_encoder_layer_8_attention_attn_o_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_36: \"f16[1, s53, 768]\" = torch.ops.aten.clone.default(linear_51);  linear_51 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:245 in forward, code: attention_output = self.LayerNorm(self.dropout(self_outputs[0]) + hidden_states)\n",
       "                    add_1173: \"f16[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_36, layer_norm_16);  clone_36 = layer_norm_16 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_17: \"f16[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_1173, [768], p_encoder_layer_8_attention_layernorm_weight, p_encoder_layer_8_attention_layernorm_bias);  add_1173 = p_encoder_layer_8_attention_layernorm_weight = p_encoder_layer_8_attention_layernorm_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_52: \"f16[1, s53, 3072]\" = torch.ops.aten.linear.default(layer_norm_17, p_encoder_layer_8_intermediate_dense_weight, p_encoder_layer_8_intermediate_dense_bias);  p_encoder_layer_8_intermediate_dense_weight = p_encoder_layer_8_intermediate_dense_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\activations.py:85 in forward, code: return self.act(input)\n",
       "                    gelu_8: \"f16[1, s53, 3072]\" = torch.ops.aten.gelu.default(linear_52);  linear_52 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_53: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(gelu_8, p_encoder_layer_8_output_dense_weight, p_encoder_layer_8_output_dense_bias);  gelu_8 = p_encoder_layer_8_output_dense_weight = p_encoder_layer_8_output_dense_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_37: \"f16[1, s53, 768]\" = torch.ops.aten.clone.default(linear_53);  linear_53 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:277 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
       "                    add_1192: \"f16[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_37, layer_norm_17);  clone_37 = layer_norm_17 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_18: \"f16[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_1192, [768], p_encoder_layer_8_output_layernorm_weight, p_encoder_layer_8_output_layernorm_bias);  add_1192 = p_encoder_layer_8_output_layernorm_weight = p_encoder_layer_8_output_layernorm_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_54: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_18, p_encoder_layer_9_attention_attn_q_weight, p_encoder_layer_9_attention_attn_q_bias);  p_encoder_layer_9_attention_attn_q_weight = p_encoder_layer_9_attention_attn_q_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:159 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_36: \"f16[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_54, [sym_size_int_85, -1, 12, 64]);  linear_54 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:160 in forward, code: .transpose(1, 2)\n",
       "                    transpose_36: \"f16[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_36, 1, 2);  view_36 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_55: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_18, p_encoder_layer_9_attention_attn_k_weight, p_encoder_layer_9_attention_attn_k_bias);  p_encoder_layer_9_attention_attn_k_weight = p_encoder_layer_9_attention_attn_k_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:164 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_37: \"f16[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_55, [sym_size_int_85, -1, 12, 64]);  linear_55 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:165 in forward, code: .transpose(1, 2)\n",
       "                    transpose_37: \"f16[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_37, 1, 2);  view_37 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_56: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_18, p_encoder_layer_9_attention_attn_v_weight, p_encoder_layer_9_attention_attn_v_bias);  p_encoder_layer_9_attention_attn_v_weight = p_encoder_layer_9_attention_attn_v_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:169 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_38: \"f16[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_56, [sym_size_int_85, -1, 12, 64]);  linear_56 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:170 in forward, code: .transpose(1, 2)\n",
       "                    transpose_38: \"f16[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_38, 1, 2);  view_38 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:174 in forward, code: attention_scores = torch.matmul(q, k.transpose(-1, -2))\n",
       "                    transpose_39: \"f16[1, 12, 64, s53]\" = torch.ops.aten.transpose.int(transpose_37, -1, -2);  transpose_37 = None\n",
       "                    matmul_18: \"f16[1, 12, s53, s53]\" = torch.ops.aten.matmul.default(transpose_36, transpose_39);  transpose_36 = transpose_39 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:175 in forward, code: attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
       "                    scalar_tensor_default_13: \"f16[]\" = torch.ops.aten.scalar_tensor.default(8.0, dtype = torch.float16)\n",
       "                    div_11: \"f16[1, 12, s53, s53]\" = torch.ops.aten.div.Tensor(matmul_18, scalar_tensor_default_13);  matmul_18 = scalar_tensor_default_13 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:179 in forward, code: attention_scores += position_bias\n",
       "                    add_1248: \"f16[s43, 12, s53, s53]\" = torch.ops.aten.add.Tensor(div_11, clone_1);  div_11 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:182 in forward, code: attention_scores = attention_scores + attention_mask\n",
       "                    add_1254: \"f16[1, 12, s53, s53]\" = torch.ops.aten.add.Tensor(add_1248, mul_22);  add_1248 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:185 in forward, code: attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
       "                    softmax_9: \"f16[1, 12, s53, s53]\" = torch.ops.aten.softmax.int(add_1254, -1);  add_1254 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_38: \"f16[1, 12, s53, s53]\" = torch.ops.aten.clone.default(softmax_9);  softmax_9 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:192 in forward, code: c = torch.matmul(attention_probs, v)\n",
       "                    matmul_19: \"f16[1, 12, s53, 64]\" = torch.ops.aten.matmul.default(clone_38, transpose_38);  clone_38 = transpose_38 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:194 in forward, code: c = c.permute(0, 2, 1, 3).contiguous()\n",
       "                    permute_10: \"f16[1, s53, 12, 64]\" = torch.ops.aten.permute.default(matmul_19, [0, 2, 1, 3]);  matmul_19 = None\n",
       "                    clone_39: \"f16[1, s53, 12, 64]\" = torch.ops.aten.clone.default(permute_10, memory_format = torch.contiguous_format);  permute_10 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:196 in forward, code: c = c.view(*new_c_shape)\n",
       "                    view_39: \"f16[1, s53, 768]\" = torch.ops.aten.view.default(clone_39, [sym_size_int_85, sym_size_int_86, 768]);  clone_39 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_57: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(view_39, p_encoder_layer_9_attention_attn_o_weight, p_encoder_layer_9_attention_attn_o_bias);  view_39 = p_encoder_layer_9_attention_attn_o_weight = p_encoder_layer_9_attention_attn_o_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_40: \"f16[1, s53, 768]\" = torch.ops.aten.clone.default(linear_57);  linear_57 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:245 in forward, code: attention_output = self.LayerNorm(self.dropout(self_outputs[0]) + hidden_states)\n",
       "                    add_1288: \"f16[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_40, layer_norm_18);  clone_40 = layer_norm_18 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_19: \"f16[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_1288, [768], p_encoder_layer_9_attention_layernorm_weight, p_encoder_layer_9_attention_layernorm_bias);  add_1288 = p_encoder_layer_9_attention_layernorm_weight = p_encoder_layer_9_attention_layernorm_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_58: \"f16[1, s53, 3072]\" = torch.ops.aten.linear.default(layer_norm_19, p_encoder_layer_9_intermediate_dense_weight, p_encoder_layer_9_intermediate_dense_bias);  p_encoder_layer_9_intermediate_dense_weight = p_encoder_layer_9_intermediate_dense_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\activations.py:85 in forward, code: return self.act(input)\n",
       "                    gelu_9: \"f16[1, s53, 3072]\" = torch.ops.aten.gelu.default(linear_58);  linear_58 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_59: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(gelu_9, p_encoder_layer_9_output_dense_weight, p_encoder_layer_9_output_dense_bias);  gelu_9 = p_encoder_layer_9_output_dense_weight = p_encoder_layer_9_output_dense_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_41: \"f16[1, s53, 768]\" = torch.ops.aten.clone.default(linear_59);  linear_59 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:277 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
       "                    add_1307: \"f16[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_41, layer_norm_19);  clone_41 = layer_norm_19 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_20: \"f16[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_1307, [768], p_encoder_layer_9_output_layernorm_weight, p_encoder_layer_9_output_layernorm_bias);  add_1307 = p_encoder_layer_9_output_layernorm_weight = p_encoder_layer_9_output_layernorm_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_60: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_20, p_encoder_layer_10_attention_attn_q_weight, p_encoder_layer_10_attention_attn_q_bias);  p_encoder_layer_10_attention_attn_q_weight = p_encoder_layer_10_attention_attn_q_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:159 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_40: \"f16[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_60, [sym_size_int_85, -1, 12, 64]);  linear_60 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:160 in forward, code: .transpose(1, 2)\n",
       "                    transpose_40: \"f16[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_40, 1, 2);  view_40 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_61: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_20, p_encoder_layer_10_attention_attn_k_weight, p_encoder_layer_10_attention_attn_k_bias);  p_encoder_layer_10_attention_attn_k_weight = p_encoder_layer_10_attention_attn_k_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:164 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_41: \"f16[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_61, [sym_size_int_85, -1, 12, 64]);  linear_61 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:165 in forward, code: .transpose(1, 2)\n",
       "                    transpose_41: \"f16[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_41, 1, 2);  view_41 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_62: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_20, p_encoder_layer_10_attention_attn_v_weight, p_encoder_layer_10_attention_attn_v_bias);  p_encoder_layer_10_attention_attn_v_weight = p_encoder_layer_10_attention_attn_v_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:169 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_42: \"f16[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_62, [sym_size_int_85, -1, 12, 64]);  linear_62 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:170 in forward, code: .transpose(1, 2)\n",
       "                    transpose_42: \"f16[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_42, 1, 2);  view_42 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:174 in forward, code: attention_scores = torch.matmul(q, k.transpose(-1, -2))\n",
       "                    transpose_43: \"f16[1, 12, 64, s53]\" = torch.ops.aten.transpose.int(transpose_41, -1, -2);  transpose_41 = None\n",
       "                    matmul_20: \"f16[1, 12, s53, s53]\" = torch.ops.aten.matmul.default(transpose_40, transpose_43);  transpose_40 = transpose_43 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:175 in forward, code: attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
       "                    scalar_tensor_default_14: \"f16[]\" = torch.ops.aten.scalar_tensor.default(8.0, dtype = torch.float16)\n",
       "                    div_12: \"f16[1, 12, s53, s53]\" = torch.ops.aten.div.Tensor(matmul_20, scalar_tensor_default_14);  matmul_20 = scalar_tensor_default_14 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:179 in forward, code: attention_scores += position_bias\n",
       "                    add_1363: \"f16[s43, 12, s53, s53]\" = torch.ops.aten.add.Tensor(div_12, clone_1);  div_12 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:182 in forward, code: attention_scores = attention_scores + attention_mask\n",
       "                    add_1369: \"f16[1, 12, s53, s53]\" = torch.ops.aten.add.Tensor(add_1363, mul_22);  add_1363 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:185 in forward, code: attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
       "                    softmax_10: \"f16[1, 12, s53, s53]\" = torch.ops.aten.softmax.int(add_1369, -1);  add_1369 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_42: \"f16[1, 12, s53, s53]\" = torch.ops.aten.clone.default(softmax_10);  softmax_10 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:192 in forward, code: c = torch.matmul(attention_probs, v)\n",
       "                    matmul_21: \"f16[1, 12, s53, 64]\" = torch.ops.aten.matmul.default(clone_42, transpose_42);  clone_42 = transpose_42 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:194 in forward, code: c = c.permute(0, 2, 1, 3).contiguous()\n",
       "                    permute_11: \"f16[1, s53, 12, 64]\" = torch.ops.aten.permute.default(matmul_21, [0, 2, 1, 3]);  matmul_21 = None\n",
       "                    clone_43: \"f16[1, s53, 12, 64]\" = torch.ops.aten.clone.default(permute_11, memory_format = torch.contiguous_format);  permute_11 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:196 in forward, code: c = c.view(*new_c_shape)\n",
       "                    view_43: \"f16[1, s53, 768]\" = torch.ops.aten.view.default(clone_43, [sym_size_int_85, sym_size_int_86, 768]);  clone_43 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_63: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(view_43, p_encoder_layer_10_attention_attn_o_weight, p_encoder_layer_10_attention_attn_o_bias);  view_43 = p_encoder_layer_10_attention_attn_o_weight = p_encoder_layer_10_attention_attn_o_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_44: \"f16[1, s53, 768]\" = torch.ops.aten.clone.default(linear_63);  linear_63 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:245 in forward, code: attention_output = self.LayerNorm(self.dropout(self_outputs[0]) + hidden_states)\n",
       "                    add_1403: \"f16[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_44, layer_norm_20);  clone_44 = layer_norm_20 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_21: \"f16[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_1403, [768], p_encoder_layer_10_attention_layernorm_weight, p_encoder_layer_10_attention_layernorm_bias);  add_1403 = p_encoder_layer_10_attention_layernorm_weight = p_encoder_layer_10_attention_layernorm_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_64: \"f16[1, s53, 3072]\" = torch.ops.aten.linear.default(layer_norm_21, p_encoder_layer_10_intermediate_dense_weight, p_encoder_layer_10_intermediate_dense_bias);  p_encoder_layer_10_intermediate_dense_weight = p_encoder_layer_10_intermediate_dense_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\activations.py:85 in forward, code: return self.act(input)\n",
       "                    gelu_10: \"f16[1, s53, 3072]\" = torch.ops.aten.gelu.default(linear_64);  linear_64 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_65: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(gelu_10, p_encoder_layer_10_output_dense_weight, p_encoder_layer_10_output_dense_bias);  gelu_10 = p_encoder_layer_10_output_dense_weight = p_encoder_layer_10_output_dense_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_45: \"f16[1, s53, 768]\" = torch.ops.aten.clone.default(linear_65);  linear_65 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:277 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
       "                    add_1422: \"f16[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_45, layer_norm_21);  clone_45 = layer_norm_21 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_22: \"f16[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_1422, [768], p_encoder_layer_10_output_layernorm_weight, p_encoder_layer_10_output_layernorm_bias);  add_1422 = p_encoder_layer_10_output_layernorm_weight = p_encoder_layer_10_output_layernorm_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_66: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_22, p_encoder_layer_11_attention_attn_q_weight, p_encoder_layer_11_attention_attn_q_bias);  p_encoder_layer_11_attention_attn_q_weight = p_encoder_layer_11_attention_attn_q_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:159 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_44: \"f16[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_66, [sym_size_int_85, -1, 12, 64]);  linear_66 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:160 in forward, code: .transpose(1, 2)\n",
       "                    transpose_44: \"f16[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_44, 1, 2);  view_44 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_67: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_22, p_encoder_layer_11_attention_attn_k_weight, p_encoder_layer_11_attention_attn_k_bias);  p_encoder_layer_11_attention_attn_k_weight = p_encoder_layer_11_attention_attn_k_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:164 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_45: \"f16[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_67, [sym_size_int_85, -1, 12, 64]);  linear_67 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:165 in forward, code: .transpose(1, 2)\n",
       "                    transpose_45: \"f16[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_45, 1, 2);  view_45 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_68: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_22, p_encoder_layer_11_attention_attn_v_weight, p_encoder_layer_11_attention_attn_v_bias);  p_encoder_layer_11_attention_attn_v_weight = p_encoder_layer_11_attention_attn_v_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:169 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_46: \"f16[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_68, [sym_size_int_85, -1, 12, 64]);  linear_68 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:170 in forward, code: .transpose(1, 2)\n",
       "                    transpose_46: \"f16[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_46, 1, 2);  view_46 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:174 in forward, code: attention_scores = torch.matmul(q, k.transpose(-1, -2))\n",
       "                    transpose_47: \"f16[1, 12, 64, s53]\" = torch.ops.aten.transpose.int(transpose_45, -1, -2);  transpose_45 = None\n",
       "                    matmul_22: \"f16[1, 12, s53, s53]\" = torch.ops.aten.matmul.default(transpose_44, transpose_47);  transpose_44 = transpose_47 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:175 in forward, code: attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
       "                    scalar_tensor_default_15: \"f16[]\" = torch.ops.aten.scalar_tensor.default(8.0, dtype = torch.float16)\n",
       "                    div_13: \"f16[1, 12, s53, s53]\" = torch.ops.aten.div.Tensor(matmul_22, scalar_tensor_default_15);  matmul_22 = scalar_tensor_default_15 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:179 in forward, code: attention_scores += position_bias\n",
       "                    add_1478: \"f16[s43, 12, s53, s53]\" = torch.ops.aten.add.Tensor(div_13, clone_1);  div_13 = clone_1 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:182 in forward, code: attention_scores = attention_scores + attention_mask\n",
       "                    add_1484: \"f16[1, 12, s53, s53]\" = torch.ops.aten.add.Tensor(add_1478, mul_22);  add_1478 = mul_22 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:185 in forward, code: attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
       "                    softmax_11: \"f16[1, 12, s53, s53]\" = torch.ops.aten.softmax.int(add_1484, -1);  add_1484 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_46: \"f16[1, 12, s53, s53]\" = torch.ops.aten.clone.default(softmax_11);  softmax_11 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:192 in forward, code: c = torch.matmul(attention_probs, v)\n",
       "                    matmul_23: \"f16[1, 12, s53, 64]\" = torch.ops.aten.matmul.default(clone_46, transpose_46);  clone_46 = transpose_46 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:194 in forward, code: c = c.permute(0, 2, 1, 3).contiguous()\n",
       "                    permute_12: \"f16[1, s53, 12, 64]\" = torch.ops.aten.permute.default(matmul_23, [0, 2, 1, 3]);  matmul_23 = None\n",
       "                    clone_47: \"f16[1, s53, 12, 64]\" = torch.ops.aten.clone.default(permute_12, memory_format = torch.contiguous_format);  permute_12 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:196 in forward, code: c = c.view(*new_c_shape)\n",
       "                    view_47: \"f16[1, s53, 768]\" = torch.ops.aten.view.default(clone_47, [sym_size_int_85, sym_size_int_86, 768]);  clone_47 = sym_size_int_85 = sym_size_int_86 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_69: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(view_47, p_encoder_layer_11_attention_attn_o_weight, p_encoder_layer_11_attention_attn_o_bias);  view_47 = p_encoder_layer_11_attention_attn_o_weight = p_encoder_layer_11_attention_attn_o_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_48: \"f16[1, s53, 768]\" = torch.ops.aten.clone.default(linear_69);  linear_69 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:245 in forward, code: attention_output = self.LayerNorm(self.dropout(self_outputs[0]) + hidden_states)\n",
       "                    add_1518: \"f16[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_48, layer_norm_22);  clone_48 = layer_norm_22 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_23: \"f16[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_1518, [768], p_encoder_layer_11_attention_layernorm_weight, p_encoder_layer_11_attention_layernorm_bias);  add_1518 = p_encoder_layer_11_attention_layernorm_weight = p_encoder_layer_11_attention_layernorm_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_70: \"f16[1, s53, 3072]\" = torch.ops.aten.linear.default(layer_norm_23, p_encoder_layer_11_intermediate_dense_weight, p_encoder_layer_11_intermediate_dense_bias);  p_encoder_layer_11_intermediate_dense_weight = p_encoder_layer_11_intermediate_dense_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\activations.py:85 in forward, code: return self.act(input)\n",
       "                    gelu_11: \"f16[1, s53, 3072]\" = torch.ops.aten.gelu.default(linear_70);  linear_70 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_71: \"f16[1, s53, 768]\" = torch.ops.aten.linear.default(gelu_11, p_encoder_layer_11_output_dense_weight, p_encoder_layer_11_output_dense_bias);  gelu_11 = p_encoder_layer_11_output_dense_weight = p_encoder_layer_11_output_dense_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_49: \"f16[1, s53, 768]\" = torch.ops.aten.clone.default(linear_71);  linear_71 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:277 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
       "                    add_1537: \"f16[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_49, layer_norm_23);  clone_49 = layer_norm_23 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_24: \"f16[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_1537, [768], p_encoder_layer_11_output_layernorm_weight, p_encoder_layer_11_output_layernorm_bias);  add_1537 = p_encoder_layer_11_output_layernorm_weight = p_encoder_layer_11_output_layernorm_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:412 in forward, code: first_token_tensor = hidden_states[:, 0]\n",
       "                    slice_5: \"f16[1, s53, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_24, 0, 0, 9223372036854775807)\n",
       "                    select: \"f16[1, 768]\" = torch.ops.aten.select.int(slice_5, 1, 0);  slice_5 = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_72: \"f16[1, 768]\" = torch.ops.aten.linear.default(select, p_pooler_dense_weight, p_pooler_dense_bias);  select = p_pooler_dense_weight = p_pooler_dense_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\maps3\\Desktop\\msc2\\mlops2\\lab7\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:430 in forward, code: return torch.tanh(input)\n",
       "                    tanh: \"f16[1, 768]\" = torch.ops.aten.tanh.default(linear_72);  linear_72 = None\n",
       "                    return (layer_norm_24, tanh)\n",
       "            \n",
       "        Graph signature: \n",
       "            # inputs\n",
       "            p_embeddings_word_embeddings_weight: PARAMETER target='embeddings.word_embeddings.weight'\n",
       "            p_embeddings_position_embeddings_weight: PARAMETER target='embeddings.position_embeddings.weight'\n",
       "            p_embeddings_layernorm_weight: PARAMETER target='embeddings.LayerNorm.weight'\n",
       "            p_embeddings_layernorm_bias: PARAMETER target='embeddings.LayerNorm.bias'\n",
       "            p_encoder_layer_0_attention_attn_q_weight: PARAMETER target='encoder.layer.0.attention.attn.q.weight'\n",
       "            p_encoder_layer_0_attention_attn_q_bias: PARAMETER target='encoder.layer.0.attention.attn.q.bias'\n",
       "            p_encoder_layer_0_attention_attn_k_weight: PARAMETER target='encoder.layer.0.attention.attn.k.weight'\n",
       "            p_encoder_layer_0_attention_attn_k_bias: PARAMETER target='encoder.layer.0.attention.attn.k.bias'\n",
       "            p_encoder_layer_0_attention_attn_v_weight: PARAMETER target='encoder.layer.0.attention.attn.v.weight'\n",
       "            p_encoder_layer_0_attention_attn_v_bias: PARAMETER target='encoder.layer.0.attention.attn.v.bias'\n",
       "            p_encoder_layer_0_attention_attn_o_weight: PARAMETER target='encoder.layer.0.attention.attn.o.weight'\n",
       "            p_encoder_layer_0_attention_attn_o_bias: PARAMETER target='encoder.layer.0.attention.attn.o.bias'\n",
       "            p_encoder_layer_0_attention_layernorm_weight: PARAMETER target='encoder.layer.0.attention.LayerNorm.weight'\n",
       "            p_encoder_layer_0_attention_layernorm_bias: PARAMETER target='encoder.layer.0.attention.LayerNorm.bias'\n",
       "            p_encoder_layer_0_intermediate_dense_weight: PARAMETER target='encoder.layer.0.intermediate.dense.weight'\n",
       "            p_encoder_layer_0_intermediate_dense_bias: PARAMETER target='encoder.layer.0.intermediate.dense.bias'\n",
       "            p_encoder_layer_0_output_dense_weight: PARAMETER target='encoder.layer.0.output.dense.weight'\n",
       "            p_encoder_layer_0_output_dense_bias: PARAMETER target='encoder.layer.0.output.dense.bias'\n",
       "            p_encoder_layer_0_output_layernorm_weight: PARAMETER target='encoder.layer.0.output.LayerNorm.weight'\n",
       "            p_encoder_layer_0_output_layernorm_bias: PARAMETER target='encoder.layer.0.output.LayerNorm.bias'\n",
       "            p_encoder_layer_1_attention_attn_q_weight: PARAMETER target='encoder.layer.1.attention.attn.q.weight'\n",
       "            p_encoder_layer_1_attention_attn_q_bias: PARAMETER target='encoder.layer.1.attention.attn.q.bias'\n",
       "            p_encoder_layer_1_attention_attn_k_weight: PARAMETER target='encoder.layer.1.attention.attn.k.weight'\n",
       "            p_encoder_layer_1_attention_attn_k_bias: PARAMETER target='encoder.layer.1.attention.attn.k.bias'\n",
       "            p_encoder_layer_1_attention_attn_v_weight: PARAMETER target='encoder.layer.1.attention.attn.v.weight'\n",
       "            p_encoder_layer_1_attention_attn_v_bias: PARAMETER target='encoder.layer.1.attention.attn.v.bias'\n",
       "            p_encoder_layer_1_attention_attn_o_weight: PARAMETER target='encoder.layer.1.attention.attn.o.weight'\n",
       "            p_encoder_layer_1_attention_attn_o_bias: PARAMETER target='encoder.layer.1.attention.attn.o.bias'\n",
       "            p_encoder_layer_1_attention_layernorm_weight: PARAMETER target='encoder.layer.1.attention.LayerNorm.weight'\n",
       "            p_encoder_layer_1_attention_layernorm_bias: PARAMETER target='encoder.layer.1.attention.LayerNorm.bias'\n",
       "            p_encoder_layer_1_intermediate_dense_weight: PARAMETER target='encoder.layer.1.intermediate.dense.weight'\n",
       "            p_encoder_layer_1_intermediate_dense_bias: PARAMETER target='encoder.layer.1.intermediate.dense.bias'\n",
       "            p_encoder_layer_1_output_dense_weight: PARAMETER target='encoder.layer.1.output.dense.weight'\n",
       "            p_encoder_layer_1_output_dense_bias: PARAMETER target='encoder.layer.1.output.dense.bias'\n",
       "            p_encoder_layer_1_output_layernorm_weight: PARAMETER target='encoder.layer.1.output.LayerNorm.weight'\n",
       "            p_encoder_layer_1_output_layernorm_bias: PARAMETER target='encoder.layer.1.output.LayerNorm.bias'\n",
       "            p_encoder_layer_2_attention_attn_q_weight: PARAMETER target='encoder.layer.2.attention.attn.q.weight'\n",
       "            p_encoder_layer_2_attention_attn_q_bias: PARAMETER target='encoder.layer.2.attention.attn.q.bias'\n",
       "            p_encoder_layer_2_attention_attn_k_weight: PARAMETER target='encoder.layer.2.attention.attn.k.weight'\n",
       "            p_encoder_layer_2_attention_attn_k_bias: PARAMETER target='encoder.layer.2.attention.attn.k.bias'\n",
       "            p_encoder_layer_2_attention_attn_v_weight: PARAMETER target='encoder.layer.2.attention.attn.v.weight'\n",
       "            p_encoder_layer_2_attention_attn_v_bias: PARAMETER target='encoder.layer.2.attention.attn.v.bias'\n",
       "            p_encoder_layer_2_attention_attn_o_weight: PARAMETER target='encoder.layer.2.attention.attn.o.weight'\n",
       "            p_encoder_layer_2_attention_attn_o_bias: PARAMETER target='encoder.layer.2.attention.attn.o.bias'\n",
       "            p_encoder_layer_2_attention_layernorm_weight: PARAMETER target='encoder.layer.2.attention.LayerNorm.weight'\n",
       "            p_encoder_layer_2_attention_layernorm_bias: PARAMETER target='encoder.layer.2.attention.LayerNorm.bias'\n",
       "            p_encoder_layer_2_intermediate_dense_weight: PARAMETER target='encoder.layer.2.intermediate.dense.weight'\n",
       "            p_encoder_layer_2_intermediate_dense_bias: PARAMETER target='encoder.layer.2.intermediate.dense.bias'\n",
       "            p_encoder_layer_2_output_dense_weight: PARAMETER target='encoder.layer.2.output.dense.weight'\n",
       "            p_encoder_layer_2_output_dense_bias: PARAMETER target='encoder.layer.2.output.dense.bias'\n",
       "            p_encoder_layer_2_output_layernorm_weight: PARAMETER target='encoder.layer.2.output.LayerNorm.weight'\n",
       "            p_encoder_layer_2_output_layernorm_bias: PARAMETER target='encoder.layer.2.output.LayerNorm.bias'\n",
       "            p_encoder_layer_3_attention_attn_q_weight: PARAMETER target='encoder.layer.3.attention.attn.q.weight'\n",
       "            p_encoder_layer_3_attention_attn_q_bias: PARAMETER target='encoder.layer.3.attention.attn.q.bias'\n",
       "            p_encoder_layer_3_attention_attn_k_weight: PARAMETER target='encoder.layer.3.attention.attn.k.weight'\n",
       "            p_encoder_layer_3_attention_attn_k_bias: PARAMETER target='encoder.layer.3.attention.attn.k.bias'\n",
       "            p_encoder_layer_3_attention_attn_v_weight: PARAMETER target='encoder.layer.3.attention.attn.v.weight'\n",
       "            p_encoder_layer_3_attention_attn_v_bias: PARAMETER target='encoder.layer.3.attention.attn.v.bias'\n",
       "            p_encoder_layer_3_attention_attn_o_weight: PARAMETER target='encoder.layer.3.attention.attn.o.weight'\n",
       "            p_encoder_layer_3_attention_attn_o_bias: PARAMETER target='encoder.layer.3.attention.attn.o.bias'\n",
       "            p_encoder_layer_3_attention_layernorm_weight: PARAMETER target='encoder.layer.3.attention.LayerNorm.weight'\n",
       "            p_encoder_layer_3_attention_layernorm_bias: PARAMETER target='encoder.layer.3.attention.LayerNorm.bias'\n",
       "            p_encoder_layer_3_intermediate_dense_weight: PARAMETER target='encoder.layer.3.intermediate.dense.weight'\n",
       "            p_encoder_layer_3_intermediate_dense_bias: PARAMETER target='encoder.layer.3.intermediate.dense.bias'\n",
       "            p_encoder_layer_3_output_dense_weight: PARAMETER target='encoder.layer.3.output.dense.weight'\n",
       "            p_encoder_layer_3_output_dense_bias: PARAMETER target='encoder.layer.3.output.dense.bias'\n",
       "            p_encoder_layer_3_output_layernorm_weight: PARAMETER target='encoder.layer.3.output.LayerNorm.weight'\n",
       "            p_encoder_layer_3_output_layernorm_bias: PARAMETER target='encoder.layer.3.output.LayerNorm.bias'\n",
       "            p_encoder_layer_4_attention_attn_q_weight: PARAMETER target='encoder.layer.4.attention.attn.q.weight'\n",
       "            p_encoder_layer_4_attention_attn_q_bias: PARAMETER target='encoder.layer.4.attention.attn.q.bias'\n",
       "            p_encoder_layer_4_attention_attn_k_weight: PARAMETER target='encoder.layer.4.attention.attn.k.weight'\n",
       "            p_encoder_layer_4_attention_attn_k_bias: PARAMETER target='encoder.layer.4.attention.attn.k.bias'\n",
       "            p_encoder_layer_4_attention_attn_v_weight: PARAMETER target='encoder.layer.4.attention.attn.v.weight'\n",
       "            p_encoder_layer_4_attention_attn_v_bias: PARAMETER target='encoder.layer.4.attention.attn.v.bias'\n",
       "            p_encoder_layer_4_attention_attn_o_weight: PARAMETER target='encoder.layer.4.attention.attn.o.weight'\n",
       "            p_encoder_layer_4_attention_attn_o_bias: PARAMETER target='encoder.layer.4.attention.attn.o.bias'\n",
       "            p_encoder_layer_4_attention_layernorm_weight: PARAMETER target='encoder.layer.4.attention.LayerNorm.weight'\n",
       "            p_encoder_layer_4_attention_layernorm_bias: PARAMETER target='encoder.layer.4.attention.LayerNorm.bias'\n",
       "            p_encoder_layer_4_intermediate_dense_weight: PARAMETER target='encoder.layer.4.intermediate.dense.weight'\n",
       "            p_encoder_layer_4_intermediate_dense_bias: PARAMETER target='encoder.layer.4.intermediate.dense.bias'\n",
       "            p_encoder_layer_4_output_dense_weight: PARAMETER target='encoder.layer.4.output.dense.weight'\n",
       "            p_encoder_layer_4_output_dense_bias: PARAMETER target='encoder.layer.4.output.dense.bias'\n",
       "            p_encoder_layer_4_output_layernorm_weight: PARAMETER target='encoder.layer.4.output.LayerNorm.weight'\n",
       "            p_encoder_layer_4_output_layernorm_bias: PARAMETER target='encoder.layer.4.output.LayerNorm.bias'\n",
       "            p_encoder_layer_5_attention_attn_q_weight: PARAMETER target='encoder.layer.5.attention.attn.q.weight'\n",
       "            p_encoder_layer_5_attention_attn_q_bias: PARAMETER target='encoder.layer.5.attention.attn.q.bias'\n",
       "            p_encoder_layer_5_attention_attn_k_weight: PARAMETER target='encoder.layer.5.attention.attn.k.weight'\n",
       "            p_encoder_layer_5_attention_attn_k_bias: PARAMETER target='encoder.layer.5.attention.attn.k.bias'\n",
       "            p_encoder_layer_5_attention_attn_v_weight: PARAMETER target='encoder.layer.5.attention.attn.v.weight'\n",
       "            p_encoder_layer_5_attention_attn_v_bias: PARAMETER target='encoder.layer.5.attention.attn.v.bias'\n",
       "            p_encoder_layer_5_attention_attn_o_weight: PARAMETER target='encoder.layer.5.attention.attn.o.weight'\n",
       "            p_encoder_layer_5_attention_attn_o_bias: PARAMETER target='encoder.layer.5.attention.attn.o.bias'\n",
       "            p_encoder_layer_5_attention_layernorm_weight: PARAMETER target='encoder.layer.5.attention.LayerNorm.weight'\n",
       "            p_encoder_layer_5_attention_layernorm_bias: PARAMETER target='encoder.layer.5.attention.LayerNorm.bias'\n",
       "            p_encoder_layer_5_intermediate_dense_weight: PARAMETER target='encoder.layer.5.intermediate.dense.weight'\n",
       "            p_encoder_layer_5_intermediate_dense_bias: PARAMETER target='encoder.layer.5.intermediate.dense.bias'\n",
       "            p_encoder_layer_5_output_dense_weight: PARAMETER target='encoder.layer.5.output.dense.weight'\n",
       "            p_encoder_layer_5_output_dense_bias: PARAMETER target='encoder.layer.5.output.dense.bias'\n",
       "            p_encoder_layer_5_output_layernorm_weight: PARAMETER target='encoder.layer.5.output.LayerNorm.weight'\n",
       "            p_encoder_layer_5_output_layernorm_bias: PARAMETER target='encoder.layer.5.output.LayerNorm.bias'\n",
       "            p_encoder_layer_6_attention_attn_q_weight: PARAMETER target='encoder.layer.6.attention.attn.q.weight'\n",
       "            p_encoder_layer_6_attention_attn_q_bias: PARAMETER target='encoder.layer.6.attention.attn.q.bias'\n",
       "            p_encoder_layer_6_attention_attn_k_weight: PARAMETER target='encoder.layer.6.attention.attn.k.weight'\n",
       "            p_encoder_layer_6_attention_attn_k_bias: PARAMETER target='encoder.layer.6.attention.attn.k.bias'\n",
       "            p_encoder_layer_6_attention_attn_v_weight: PARAMETER target='encoder.layer.6.attention.attn.v.weight'\n",
       "            p_encoder_layer_6_attention_attn_v_bias: PARAMETER target='encoder.layer.6.attention.attn.v.bias'\n",
       "            p_encoder_layer_6_attention_attn_o_weight: PARAMETER target='encoder.layer.6.attention.attn.o.weight'\n",
       "            p_encoder_layer_6_attention_attn_o_bias: PARAMETER target='encoder.layer.6.attention.attn.o.bias'\n",
       "            p_encoder_layer_6_attention_layernorm_weight: PARAMETER target='encoder.layer.6.attention.LayerNorm.weight'\n",
       "            p_encoder_layer_6_attention_layernorm_bias: PARAMETER target='encoder.layer.6.attention.LayerNorm.bias'\n",
       "            p_encoder_layer_6_intermediate_dense_weight: PARAMETER target='encoder.layer.6.intermediate.dense.weight'\n",
       "            p_encoder_layer_6_intermediate_dense_bias: PARAMETER target='encoder.layer.6.intermediate.dense.bias'\n",
       "            p_encoder_layer_6_output_dense_weight: PARAMETER target='encoder.layer.6.output.dense.weight'\n",
       "            p_encoder_layer_6_output_dense_bias: PARAMETER target='encoder.layer.6.output.dense.bias'\n",
       "            p_encoder_layer_6_output_layernorm_weight: PARAMETER target='encoder.layer.6.output.LayerNorm.weight'\n",
       "            p_encoder_layer_6_output_layernorm_bias: PARAMETER target='encoder.layer.6.output.LayerNorm.bias'\n",
       "            p_encoder_layer_7_attention_attn_q_weight: PARAMETER target='encoder.layer.7.attention.attn.q.weight'\n",
       "            p_encoder_layer_7_attention_attn_q_bias: PARAMETER target='encoder.layer.7.attention.attn.q.bias'\n",
       "            p_encoder_layer_7_attention_attn_k_weight: PARAMETER target='encoder.layer.7.attention.attn.k.weight'\n",
       "            p_encoder_layer_7_attention_attn_k_bias: PARAMETER target='encoder.layer.7.attention.attn.k.bias'\n",
       "            p_encoder_layer_7_attention_attn_v_weight: PARAMETER target='encoder.layer.7.attention.attn.v.weight'\n",
       "            p_encoder_layer_7_attention_attn_v_bias: PARAMETER target='encoder.layer.7.attention.attn.v.bias'\n",
       "            p_encoder_layer_7_attention_attn_o_weight: PARAMETER target='encoder.layer.7.attention.attn.o.weight'\n",
       "            p_encoder_layer_7_attention_attn_o_bias: PARAMETER target='encoder.layer.7.attention.attn.o.bias'\n",
       "            p_encoder_layer_7_attention_layernorm_weight: PARAMETER target='encoder.layer.7.attention.LayerNorm.weight'\n",
       "            p_encoder_layer_7_attention_layernorm_bias: PARAMETER target='encoder.layer.7.attention.LayerNorm.bias'\n",
       "            p_encoder_layer_7_intermediate_dense_weight: PARAMETER target='encoder.layer.7.intermediate.dense.weight'\n",
       "            p_encoder_layer_7_intermediate_dense_bias: PARAMETER target='encoder.layer.7.intermediate.dense.bias'\n",
       "            p_encoder_layer_7_output_dense_weight: PARAMETER target='encoder.layer.7.output.dense.weight'\n",
       "            p_encoder_layer_7_output_dense_bias: PARAMETER target='encoder.layer.7.output.dense.bias'\n",
       "            p_encoder_layer_7_output_layernorm_weight: PARAMETER target='encoder.layer.7.output.LayerNorm.weight'\n",
       "            p_encoder_layer_7_output_layernorm_bias: PARAMETER target='encoder.layer.7.output.LayerNorm.bias'\n",
       "            p_encoder_layer_8_attention_attn_q_weight: PARAMETER target='encoder.layer.8.attention.attn.q.weight'\n",
       "            p_encoder_layer_8_attention_attn_q_bias: PARAMETER target='encoder.layer.8.attention.attn.q.bias'\n",
       "            p_encoder_layer_8_attention_attn_k_weight: PARAMETER target='encoder.layer.8.attention.attn.k.weight'\n",
       "            p_encoder_layer_8_attention_attn_k_bias: PARAMETER target='encoder.layer.8.attention.attn.k.bias'\n",
       "            p_encoder_layer_8_attention_attn_v_weight: PARAMETER target='encoder.layer.8.attention.attn.v.weight'\n",
       "            p_encoder_layer_8_attention_attn_v_bias: PARAMETER target='encoder.layer.8.attention.attn.v.bias'\n",
       "            p_encoder_layer_8_attention_attn_o_weight: PARAMETER target='encoder.layer.8.attention.attn.o.weight'\n",
       "            p_encoder_layer_8_attention_attn_o_bias: PARAMETER target='encoder.layer.8.attention.attn.o.bias'\n",
       "            p_encoder_layer_8_attention_layernorm_weight: PARAMETER target='encoder.layer.8.attention.LayerNorm.weight'\n",
       "            p_encoder_layer_8_attention_layernorm_bias: PARAMETER target='encoder.layer.8.attention.LayerNorm.bias'\n",
       "            p_encoder_layer_8_intermediate_dense_weight: PARAMETER target='encoder.layer.8.intermediate.dense.weight'\n",
       "            p_encoder_layer_8_intermediate_dense_bias: PARAMETER target='encoder.layer.8.intermediate.dense.bias'\n",
       "            p_encoder_layer_8_output_dense_weight: PARAMETER target='encoder.layer.8.output.dense.weight'\n",
       "            p_encoder_layer_8_output_dense_bias: PARAMETER target='encoder.layer.8.output.dense.bias'\n",
       "            p_encoder_layer_8_output_layernorm_weight: PARAMETER target='encoder.layer.8.output.LayerNorm.weight'\n",
       "            p_encoder_layer_8_output_layernorm_bias: PARAMETER target='encoder.layer.8.output.LayerNorm.bias'\n",
       "            p_encoder_layer_9_attention_attn_q_weight: PARAMETER target='encoder.layer.9.attention.attn.q.weight'\n",
       "            p_encoder_layer_9_attention_attn_q_bias: PARAMETER target='encoder.layer.9.attention.attn.q.bias'\n",
       "            p_encoder_layer_9_attention_attn_k_weight: PARAMETER target='encoder.layer.9.attention.attn.k.weight'\n",
       "            p_encoder_layer_9_attention_attn_k_bias: PARAMETER target='encoder.layer.9.attention.attn.k.bias'\n",
       "            p_encoder_layer_9_attention_attn_v_weight: PARAMETER target='encoder.layer.9.attention.attn.v.weight'\n",
       "            p_encoder_layer_9_attention_attn_v_bias: PARAMETER target='encoder.layer.9.attention.attn.v.bias'\n",
       "            p_encoder_layer_9_attention_attn_o_weight: PARAMETER target='encoder.layer.9.attention.attn.o.weight'\n",
       "            p_encoder_layer_9_attention_attn_o_bias: PARAMETER target='encoder.layer.9.attention.attn.o.bias'\n",
       "            p_encoder_layer_9_attention_layernorm_weight: PARAMETER target='encoder.layer.9.attention.LayerNorm.weight'\n",
       "            p_encoder_layer_9_attention_layernorm_bias: PARAMETER target='encoder.layer.9.attention.LayerNorm.bias'\n",
       "            p_encoder_layer_9_intermediate_dense_weight: PARAMETER target='encoder.layer.9.intermediate.dense.weight'\n",
       "            p_encoder_layer_9_intermediate_dense_bias: PARAMETER target='encoder.layer.9.intermediate.dense.bias'\n",
       "            p_encoder_layer_9_output_dense_weight: PARAMETER target='encoder.layer.9.output.dense.weight'\n",
       "            p_encoder_layer_9_output_dense_bias: PARAMETER target='encoder.layer.9.output.dense.bias'\n",
       "            p_encoder_layer_9_output_layernorm_weight: PARAMETER target='encoder.layer.9.output.LayerNorm.weight'\n",
       "            p_encoder_layer_9_output_layernorm_bias: PARAMETER target='encoder.layer.9.output.LayerNorm.bias'\n",
       "            p_encoder_layer_10_attention_attn_q_weight: PARAMETER target='encoder.layer.10.attention.attn.q.weight'\n",
       "            p_encoder_layer_10_attention_attn_q_bias: PARAMETER target='encoder.layer.10.attention.attn.q.bias'\n",
       "            p_encoder_layer_10_attention_attn_k_weight: PARAMETER target='encoder.layer.10.attention.attn.k.weight'\n",
       "            p_encoder_layer_10_attention_attn_k_bias: PARAMETER target='encoder.layer.10.attention.attn.k.bias'\n",
       "            p_encoder_layer_10_attention_attn_v_weight: PARAMETER target='encoder.layer.10.attention.attn.v.weight'\n",
       "            p_encoder_layer_10_attention_attn_v_bias: PARAMETER target='encoder.layer.10.attention.attn.v.bias'\n",
       "            p_encoder_layer_10_attention_attn_o_weight: PARAMETER target='encoder.layer.10.attention.attn.o.weight'\n",
       "            p_encoder_layer_10_attention_attn_o_bias: PARAMETER target='encoder.layer.10.attention.attn.o.bias'\n",
       "            p_encoder_layer_10_attention_layernorm_weight: PARAMETER target='encoder.layer.10.attention.LayerNorm.weight'\n",
       "            p_encoder_layer_10_attention_layernorm_bias: PARAMETER target='encoder.layer.10.attention.LayerNorm.bias'\n",
       "            p_encoder_layer_10_intermediate_dense_weight: PARAMETER target='encoder.layer.10.intermediate.dense.weight'\n",
       "            p_encoder_layer_10_intermediate_dense_bias: PARAMETER target='encoder.layer.10.intermediate.dense.bias'\n",
       "            p_encoder_layer_10_output_dense_weight: PARAMETER target='encoder.layer.10.output.dense.weight'\n",
       "            p_encoder_layer_10_output_dense_bias: PARAMETER target='encoder.layer.10.output.dense.bias'\n",
       "            p_encoder_layer_10_output_layernorm_weight: PARAMETER target='encoder.layer.10.output.LayerNorm.weight'\n",
       "            p_encoder_layer_10_output_layernorm_bias: PARAMETER target='encoder.layer.10.output.LayerNorm.bias'\n",
       "            p_encoder_layer_11_attention_attn_q_weight: PARAMETER target='encoder.layer.11.attention.attn.q.weight'\n",
       "            p_encoder_layer_11_attention_attn_q_bias: PARAMETER target='encoder.layer.11.attention.attn.q.bias'\n",
       "            p_encoder_layer_11_attention_attn_k_weight: PARAMETER target='encoder.layer.11.attention.attn.k.weight'\n",
       "            p_encoder_layer_11_attention_attn_k_bias: PARAMETER target='encoder.layer.11.attention.attn.k.bias'\n",
       "            p_encoder_layer_11_attention_attn_v_weight: PARAMETER target='encoder.layer.11.attention.attn.v.weight'\n",
       "            p_encoder_layer_11_attention_attn_v_bias: PARAMETER target='encoder.layer.11.attention.attn.v.bias'\n",
       "            p_encoder_layer_11_attention_attn_o_weight: PARAMETER target='encoder.layer.11.attention.attn.o.weight'\n",
       "            p_encoder_layer_11_attention_attn_o_bias: PARAMETER target='encoder.layer.11.attention.attn.o.bias'\n",
       "            p_encoder_layer_11_attention_layernorm_weight: PARAMETER target='encoder.layer.11.attention.LayerNorm.weight'\n",
       "            p_encoder_layer_11_attention_layernorm_bias: PARAMETER target='encoder.layer.11.attention.LayerNorm.bias'\n",
       "            p_encoder_layer_11_intermediate_dense_weight: PARAMETER target='encoder.layer.11.intermediate.dense.weight'\n",
       "            p_encoder_layer_11_intermediate_dense_bias: PARAMETER target='encoder.layer.11.intermediate.dense.bias'\n",
       "            p_encoder_layer_11_output_dense_weight: PARAMETER target='encoder.layer.11.output.dense.weight'\n",
       "            p_encoder_layer_11_output_dense_bias: PARAMETER target='encoder.layer.11.output.dense.bias'\n",
       "            p_encoder_layer_11_output_layernorm_weight: PARAMETER target='encoder.layer.11.output.LayerNorm.weight'\n",
       "            p_encoder_layer_11_output_layernorm_bias: PARAMETER target='encoder.layer.11.output.LayerNorm.bias'\n",
       "            p_encoder_relative_attention_bias_weight: PARAMETER target='encoder.relative_attention_bias.weight'\n",
       "            p_pooler_dense_weight: PARAMETER target='pooler.dense.weight'\n",
       "            p_pooler_dense_bias: PARAMETER target='pooler.dense.bias'\n",
       "            b_embeddings_position_ids: BUFFER target='embeddings.position_ids' persistent=False\n",
       "            input_ids: USER_INPUT\n",
       "            attention_mask: USER_INPUT\n",
       "    \n",
       "            # outputs\n",
       "            layer_norm_24: USER_OUTPUT\n",
       "            tanh: USER_OUTPUT\n",
       "    \n",
       "        Range constraints: {s43: VR[0, int_oo], s53: VR[0, int_oo]}\n",
       "\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.onnx\n",
    "\n",
    "# Put the model in eval mode and move to CPU\n",
    "model_cpu = model.eval().cpu()\n",
    "\n",
    "# Example input for tracking (for onnx export)\n",
    "sample_input = tokenizer(\n",
    "    \"This is a sample input text for ONNX export.\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "# Export to ONNX format\n",
    "torch.onnx.export(\n",
    "    model_cpu,\n",
    "    (sample_input[\"input_ids\"], sample_input[\"attention_mask\"]),\n",
    "    \"model.onnx\",\n",
    "    opset_version=17,\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\n",
    "        \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"output\": {0: \"batch_size\"},\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377016ff-8882-4c51-b0ce-60242bea07f7",
   "metadata": {},
   "source": [
    "- online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4714e6ac-ee3b-4289-9302-92e0cf2cdb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = tokenizer(\n",
    "    \"This is a sample input text for ONNX inference.\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"np\",\n",
    ")\n",
    "\n",
    "# Create input dictionary, in same format as during export\n",
    "inputs_onnx = {\n",
    "    \"input_ids\": sample_input[\"input_ids\"],\n",
    "    \"attention_mask\": sample_input[\"attention_mask\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8e013dfe-6cb2-40a3-97b4-f566822ddc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cold start time: 542.08 ms\n",
      "Inference time:  12.15 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import onnxruntime as ort\n",
    "\n",
    "t = time.perf_counter()\n",
    "\n",
    "options = ort.SessionOptions()\n",
    "options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "\n",
    "ort_session = ort.InferenceSession(\n",
    "    \"model.onnx\", sess_options=options, providers=[\"CPUExecutionProvider\"]\n",
    ")\n",
    "\n",
    "cold_start_time = time.perf_counter() - t\n",
    "print(f\"Cold start time: {cold_start_time * 1000:.2f} ms\")\n",
    "\n",
    "t = time.perf_counter()\n",
    "_ = ort_session.run(None, inputs_onnx)\n",
    "inference_time = time.perf_counter() - t\n",
    "print(f\"Inference time:  {inference_time * 1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d63b53-f893-491c-adfc-0e4b2402b4b7",
   "metadata": {},
   "source": [
    "- offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6ca0c2c2-786d-4821-a847-398639429449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cold Start Time: 49.27 ms\n",
      "Inference Time:  27.55 ms\n"
     ]
    }
   ],
   "source": [
    "sess_options = ort.SessionOptions()\n",
    "# Choose the optimization level for the offline pass\n",
    "sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_EXTENDED\n",
    "# Save the optimized model to this path\n",
    "sess_options.optimized_model_filepath = \"model_optimized.onnx\"\n",
    "# Create InferenceSession, which will perform offline optimization and save the optimized model\n",
    "ort.InferenceSession(\"model.onnx\", sess_options)\n",
    "\n",
    "t = time.perf_counter()\n",
    "\n",
    "sess_options = ort.SessionOptions()\n",
    "sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_DISABLE_ALL\n",
    "session = ort.InferenceSession(\n",
    "    \"model_optimized.onnx\", \n",
    "    sess_options=sess_options, \n",
    "    providers=['CPUExecutionProvider']\n",
    ")\n",
    "\n",
    "cold_start_time = time.perf_counter() - t\n",
    "print(f\"Cold Start Time: {cold_start_time * 1000:.2f} ms\")\n",
    "\n",
    "t = time.perf_counter()\n",
    "_ = session.run(None, inputs_onnx)\n",
    "inference_time = time.perf_counter() - t\n",
    "print(f\"Inference Time:  {inference_time * 1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d60698a-69b4-441d-8bd5-835422979c69",
   "metadata": {},
   "source": [
    "3. Prepare deployment Docker images:\n",
    "   - build two images, for a) compiled PyTorch model b) ONNX model with ONNX Runtime\n",
    "   - select the best model in both cases in terms of the inference time\n",
    "   - install a minimal set of requirements in both cases, e.g. do not install PyTorch for ONNX image\n",
    "4. Compare for those apps:\n",
    "   - Docker container sizes\n",
    "   - response time (average of 100 requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "af751dd0-c636-40d7-ab33-c6a73605c48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "url_torch = \"http://localhost:8001/predict\"\n",
    "url_onnx = \"http://localhost:8002/predict\"\n",
    "\n",
    "prompt = \"One two three four five six seven eight nine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5fea9be5-b552-43b2-b829-1ace4ec1e618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.7 ms ± 2.59 ms per loop (mean ± std. dev. of 10 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100 -r 10 -v torch_request\n",
    "requests.post(url_torch, json={\"text\": prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dbfc7719-3621-4437-8574-e155d6a4e4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.2 ms ± 2.33 ms per loop (mean ± std. dev. of 10 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100 -r 10 -v onnx_request\n",
    "requests.post(url_onnx, json={\"text\": prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0074ec0c-affb-4f57-8819-d25a20271ca5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
