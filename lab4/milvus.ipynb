{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92dd7914-09a2-4c54-afca-17059a9c240b",
   "metadata": {},
   "source": [
    "Retrieval-Augmented Generation (RAG) service (1 point)\n",
    "\n",
    "In this part of laboratory, we will build a RAG service. It enhances the LLM text generation capabilities with context and information drawn from a knowledge base. Relevant textual information is found with vector search and appended to the prompt, resulting in less hallucinations and more precise, relevant answers.\n",
    "\n",
    "In such cases, we don't relaly need any additional capabilities like attributes filtering, ACID, JOINs or other Postgres-related advantages. Thus, we will use Milvus, a typical example of vector database. To generate embeddings, we will use Silver Retriever model from Sentence Transformers. It is based on HerBERT model for Polish language, and finetuned for retrieval of similar vectors.\n",
    "\n",
    "1. Start by setting up Milvus by using its Docker image. Docker Compose file is also conveniently provided by its creators:\n",
    "2. Run the database with docker compose up -d.\n",
    "3. Next code sections are quite interactive and will probably be easier to run inside a Jupyter Notebook. Start it with jupyter notebook.\n",
    "4. Let's connect to the database. Milvus provides its own pymilvus library.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "716ce7df-d309-4103-96d7-66ff42f2ae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient\n",
    "\n",
    "host = \"localhost\"\n",
    "port = \"19530\"\n",
    "\n",
    "milvus_client = MilvusClient(\n",
    "    host=host,\n",
    "    port=port\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7c93d5-56c6-437f-8707-da2ee1484b8f",
   "metadata": {},
   "source": [
    "5. Vector databases work quite similarly to document databases like e.g. MongoDB. We define not a table, but a collection with specific schema, but conceptually it's a bit similar. For each element, we have an ID, text, and its embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e161ff8a-e50f-4d77-ab68-99a0c92feaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import FieldSchema, DataType, CollectionSchema\n",
    "\n",
    "VECTOR_LENGTH = 768  # check the dimensionality for Silver Retriever Base (v1.1) model\n",
    "\n",
    "id_field = FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, description=\"Primary id\")\n",
    "text = FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=4096, description=\"Page text\")\n",
    "embedding_text = FieldSchema(\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=VECTOR_LENGTH, description=\"Embedded text\")\n",
    "\n",
    "fields = [id_field, text, embedding_text]\n",
    "\n",
    "schema = CollectionSchema(fields=fields, auto_id=True, enable_dynamic_field=True, description=\"RAG Texts collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9619a0-72ff-48b8-94fb-5ab07c0f7477",
   "metadata": {},
   "source": [
    "6. To create a collection with the given schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc38005f-49c6-48d7-a61c-ff29ee7c0bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rag_texts_and_embeddings']\n",
      "{'collection_name': 'rag_texts_and_embeddings', 'auto_id': True, 'num_shards': 1, 'description': 'RAG Texts collection', 'fields': [{'field_id': 100, 'name': 'id', 'description': 'Primary id', 'type': <DataType.INT64: 5>, 'params': {}, 'auto_id': True, 'is_primary': True}, {'field_id': 101, 'name': 'text', 'description': 'Page text', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 4096}}, {'field_id': 102, 'name': 'embedding', 'description': 'Embedded text', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 768}}], 'functions': [], 'aliases': [], 'collection_id': 462149625083593581, 'consistency_level': 2, 'properties': {}, 'num_partitions': 1, 'enable_dynamic_field': True, 'created_timestamp': 462149670625869828}\n"
     ]
    }
   ],
   "source": [
    "COLLECTION_NAME = \"rag_texts_and_embeddings\"\n",
    "\n",
    "milvus_client.create_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    schema=schema\n",
    ")\n",
    "\n",
    "index_params = milvus_client.prepare_index_params()\n",
    "\n",
    "index_params.add_index(\n",
    "    field_name=\"embedding\", \n",
    "    index_type=\"HNSW\",\n",
    "    metric_type=\"L2\",\n",
    "    params={\"M\": 4, \"efConstruction\": 64}  # lower values for speed\n",
    ") \n",
    "\n",
    "milvus_client.create_index(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    index_params=index_params\n",
    ")\n",
    "\n",
    "# checkout our collection\n",
    "print(milvus_client.list_collections())\n",
    "\n",
    "# describe our collection\n",
    "print(milvus_client.describe_collection(COLLECTION_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea38b09-e7f2-4a59-b88c-1fe7f15a57c9",
   "metadata": {},
   "source": [
    "7. Now we are able to insert documents into put database. RAG is the most useful when information is very specialized, niche, or otherwise probably unknown to the model or less popular. Let's start with \"IAB POLSKA Przewodnik po sztucznej inteligencji\". This part is inspired by SpeakLeash and one of their projects Bielik-how-to-start - Bielik_2_(4_bit)_RAG example. Bielik is the first Polish LLM, and you can also explore other tutorials for its usage. Let's define some constants for a start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b29d2da4-3921-4ce3-a128-8ef0962b3908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data source and destination\n",
    "## the document origin destination from which document will be downloaded \n",
    "pdf_url = \"https://www.iab.org.pl/wp-content/uploads/2024/04/Przewodnik-po-sztucznej-inteligencji-2024_IAB-Polska.pdf\"\n",
    "\n",
    "## local destination of the document\n",
    "file_name = \"Przewodnik-po-sztucznej-inteligencji-2024_IAB-Polska.pdf\"\n",
    "\n",
    "## local destination of the processed document \n",
    "file_json = \"Przewodnik-po-sztucznej-inteligencji-2024_IAB-Polska.json\"\n",
    "\n",
    "## local destination of the embedded pages of the document\n",
    "embeddings_json = \"Przewodnik-po-sztucznej-inteligencji-2024_IAB-Polska-Embeddings.json\"\n",
    "\n",
    "## local destination of all above local required files\n",
    "data_dir = \"./data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b603054f-fe05-4f56-b954-0f64d6dd450c",
   "metadata": {},
   "source": [
    "8. Let's download the document into the data_dir directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cd2ddcb-c4f1-43cb-a306-5e99174747a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "import os\n",
    "import requests\n",
    "\n",
    "def download_pdf_data(pdf_url: str, file_name: str) -> None:\n",
    "    response = requests.get(pdf_url, stream=True)\n",
    "    with open(os.path.join(data_dir, file_name), \"wb\") as file:\n",
    "        for block in response.iter_content(chunk_size=1024):\n",
    "            if block:\n",
    "                file.write(block)\n",
    "\n",
    "download_pdf_data(pdf_url, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6ff0ae-d902-49ea-9837-d5a05778db60",
   "metadata": {},
   "source": [
    "9. This is a lot of text, and in RAG we need to add specific fragments to the prompt. To keep things simple, and number of vectors not too large, we will treat each page as a separate chunk to vectorize and search for. Below, we paginate document and save each page separately into a JSON file in format {\"page\": page_number, \"text\": text_of_the_page}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e25af7ec-588c-46e2-b33d-12b2773d64a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "\n",
    "import fitz\n",
    "import json\n",
    "\n",
    "\n",
    "def extract_pdf_text(file_name, file_json):\n",
    "    document = fitz.open(os.path.join(data_dir, file_name))\n",
    "    pages = []\n",
    "\n",
    "    for page_num in range(len(document)):\n",
    "        page = document.load_page(page_num)\n",
    "        page_text = page.get_text()\n",
    "        pages.append({\"page_num\": page_num, \"text\": page_text})\n",
    "\n",
    "    with open(os.path.join(data_dir, file_json), \"w\", encoding='utf-8') as file:\n",
    "        json.dump(pages, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "\n",
    "extract_pdf_text(file_name, file_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fba08e5-fb35-46b7-a00a-ee7089ac2a08",
   "metadata": {},
   "source": [
    "10. Now we have texts, but we need vectors. We will use the model to embed text from each page and save the result in out collection in Milvus. It's very easy if we first prepare a single JSON file with all data. Its format is {\"page\": page_num, \"embedding\": embedded_text}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "792cd873-941e-452c-8cf8-3f1e0a9ba2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize data\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "def generate_embeddings(file_json, embeddings_json, model):\n",
    "    pages = []\n",
    "    with open(os.path.join(data_dir, file_json), \"r\", encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    for page in data:\n",
    "        pages.append(page[\"text\"])\n",
    "\n",
    "    embeddings = model.encode(pages)\n",
    "\n",
    "    embeddings_paginated = []\n",
    "    for page_num in range(len(embeddings)):\n",
    "        embeddings_paginated.append({\"page_num\": page_num, \"embedding\": embeddings[page_num].tolist()})\n",
    "\n",
    "    with open(os.path.join(data_dir, embeddings_json), \"w\", encoding='utf-8') as file:\n",
    "        json.dump(embeddings_paginated, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "model_name = \"ipipan/silver-retriever-base-v1.1\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SentenceTransformer(model_name, device=device)\n",
    "generate_embeddings(file_json, embeddings_json, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b2331e1-8626-400b-9585-4280e2aae451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbc0bd2-a38d-49a0-96b6-82a41b50663d",
   "metadata": {},
   "source": [
    "11. Now we can easily insert the data into Milvus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f94fa05c-98bc-465b-a6ed-7faea9fe1b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_embeddings(file_json, embeddings_json, client=milvus_client):\n",
    "    rows = []\n",
    "    with open(os.path.join(data_dir, file_json), \"r\", encoding='utf-8') as t_f, open(os.path.join(data_dir, embeddings_json), \"r\", encoding='utf-8') as e_f:\n",
    "        text_data, embedding_data = json.load(t_f), json.load(e_f)\n",
    "        text_data =  list(map(lambda d: d[\"text\"], text_data))\n",
    "        embedding_data = list(map(lambda d: d[\"embedding\"], embedding_data))\n",
    "        \n",
    "        for page, (text, embedding) in enumerate(zip(text_data, embedding_data)):\n",
    "            rows.append({\"text\":text, \"embedding\": embedding})\n",
    "\n",
    "    client.insert(collection_name=\"rag_texts_and_embeddings\", data=rows)\n",
    "\n",
    "\n",
    "insert_embeddings(file_json, embeddings_json)\n",
    "\n",
    "# load inserted data into memory\n",
    "milvus_client.load_collection(\"rag_texts_and_embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942af51a-8482-4d03-8f38-c6b323181d6e",
   "metadata": {},
   "source": [
    "12. Now let's do some semantic search!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5e3eb17-1cc9-4728-ade1-df626e9ab749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: [[{'id': 462149625083598989, 'distance': 29.125164031982422, 'entity': {'text': 'Historia powstania\\nsztucznej inteligencji\\n7\\nW języku potocznym „sztuczny\" oznacza to, co\\njest \\nwytworem \\nmającym \\nnaśladować \\ncoś\\nnaturalnego. W takim znaczeniu używamy\\nterminu ,,sztuczny\\'\\', gdy mówimy o sztucznym\\nlodowisku lub oku. Sztuczna inteligencja byłaby\\nczymś (programem, maszyną) symulującym\\ninteligencję naturalną, ludzką.\\nSztuczna inteligencja (AI) to obszar informatyki,\\nktóry skupia się na tworzeniu programów\\nkomputerowych zdolnych do wykonywania\\nzadań, które wymagają ludzkiej inteligencji. \\nTe zadania obejmują rozpoznawanie wzorców,\\nrozumienie języka naturalnego, podejmowanie\\ndecyzji, uczenie się, planowanie i wiele innych.\\nGłównym celem AI jest stworzenie systemów,\\nktóre są zdolne do myślenia i podejmowania\\ndecyzji na sposób przypominający ludzki.\\nHistoria sztucznej inteligencji sięga lat 50. \\nXX wieku, kiedy to powstały pierwsze koncepcje\\ni modele tego, co mogłoby stać się sztuczną\\ninteligencją. Jednym z pionierów był Alan\\nTuring, który sformułował test Turinga, mający\\nna \\ncelu \\nocenę \\nzdolności \\nmaszyny \\ndo\\ninteligentnego \\nzachowania \\nna \\npoziomie\\nludzkim. Jednakże dopiero w latach 80. i 90.\\nnastąpił \\nprawdziwy \\nprzełom \\nw \\ndziedzinie\\nsztucznej \\ninteligencji \\ndzięki \\npostępowi \\nw\\ndziedzinie algorytmów uczenia maszynowego.\\nW wypadku sztucznej inteligencji mamy na\\nuwadze system, który realizowałby niektóre\\nfunkcje \\numysłu \\n– \\nczasami \\nw \\nsposób\\nprzewyższający funkcje naturalne (na przykład,\\naby był wolny od pomyłek przy liczeniu oraz\\ndefektów \\npamięci). \\nInteligencja \\njest \\nwła-\\nściwością umysłu. \\nSkłada się na nią szereg umiejętności, takich jak\\nzdolność do komunikowania, rozwiązywania\\nproblemów, uczenia się i dostosowywania do\\nsytuacji. \\nIstotna \\njest \\njednak \\numiejętność\\nrozumowania.\\nWspółczesne systemy sztucznej inteligencji są\\ninteligentne tylko w ograniczonym obszarze. \\nNa przykład komputer potrafi grać w szachy w\\ntaki \\nsposób, \\nże \\nwygrywa \\nz \\nszachowym\\narcymistrzem. W 1996 r. Deep Blue wygrał jedną\\npartię \\nszachów \\nz \\nGarry \\nKasparowem,\\nprzegrywając cały mecz wynikiem 4:2 (przy\\ndwóch remisach).\\nPóźniej Deep Blue został ulepszony i nie-\\noficjalnie \\nnazwany \\n„Deeper \\nBlue\". \\nZagrał\\nponownie z Kasparowem w maju 1997 roku.\\nMecz \\nskończył \\nsię \\nwynikiem \\n3½:2½ \\ndla\\nkomputera. W ten sposób Deep Blue stał się\\npierwszym systemem komputerowym, który\\nwygrał z aktualnym mistrzem świata w meczu\\nze standardową kontrolą czasu.\\nŹródło: Midjourney – obraz wygenerowany przez AI\\n'}}]]\n"
     ]
    }
   ],
   "source": [
    "# search\n",
    "def search(model, query, client=milvus_client):\n",
    "    embedded_query = model.encode(query).tolist()\n",
    "    result = client.search(\n",
    "        collection_name=\"rag_texts_and_embeddings\", \n",
    "        data=[embedded_query], \n",
    "        limit=1,\n",
    "        search_params={\"metric_type\": \"L2\"},\n",
    "        output_fields=[\"text\"]\n",
    "    )\n",
    "    return result\n",
    "\n",
    "\n",
    "result = search(model, query=\"Czym jest sztuczna inteligencja\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b02916-eaf7-4be6-acb5-7110d1b66e9c",
   "metadata": {},
   "source": [
    "However, this is not yet RAG!. This is just searching through our embeddings, without any LLM or generation. Many companies rely on external LLMs used via API, due to easy setup, good scalability, and low cost. We will follow this trend here and use Google Gemini API to generate answer with RAG.\n",
    "\n",
    "# Gemini API integration\n",
    "Gemini API is free to use, with rate limit for the free version of the API. We can use this LLM for intergration with our RAG system.\n",
    "1. Get the API key from the Gemini API. Go to model info, and click \"Try it in Google AI Studio\".\n",
    "2. You will be redirected to the Google AI Studio. Click \"Create API Key\", create a test project, and a test key.\n",
    "3. Copy the key and save it in the environment variable. This is a secret, like any other API key, and must never be shared!\n",
    "4. Let's prepare the function that will call Google API and generate our response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d16d8923-f039-4380-8563-981de77ef708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google import genai\n",
    "\n",
    "GEMINI_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "gemini_client = genai.Client(api_key=GEMINI_KEY)\n",
    "\n",
    "MODEL = \"gemini-2.0-flash\"\n",
    "\n",
    "def generate_response(prompt: str):\n",
    "    try:\n",
    "        # Send request to Gemini 2.0 Flash API and get the response\n",
    "        response = gemini_client.models.generate_content(\n",
    "            model=MODEL,\n",
    "            contents=prompt,\n",
    "        )\n",
    "        return response.text \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2990c8b1-cf6a-413a-a15c-1587c92c853e",
   "metadata": {},
   "source": [
    "5. Now we can fully integrate everything into a RAG system. Fill the function below that will augment the prompt with knowledge from Milvus, and then use the LLM to generate an answer based on that context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "729eb357-93ce-4ec4-8079-e58756702396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(context: str, query: str) -> str:\n",
    "    prompt = (\n",
    "        \"You are an intelligent assistant. Use the following context to answer the question.\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        f\"Question: {query}\\n\"\n",
    "        \"Answer in a clear and concise way.\"\n",
    "    )\n",
    "    return prompt\n",
    "    return prompt\n",
    "    \n",
    "\n",
    "def rag(model, query: str) -> str:\n",
    "    # having all prepared functions, you can combine them together and try to build your own RAG!\n",
    "    context = search(model, query)[0][0][\"entity\"][\"text\"]\n",
    "    prompt = build_prompt(context, query)\n",
    "    return generate_response(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790eadad-a798-4f25-870b-62b7d0ebe1ca",
   "metadata": {},
   "source": [
    "6. Test the RAG system with a few sample queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a32d79a-a2d5-477f-ad17-47f7c8e59f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sztuczna inteligencja (AI) to obszar informatyki, który skupia się na tworzeniu programów komputerowych zdolnych do wykonywania zadań, które wymagają ludzkiej inteligencji, takich jak rozpoznawanie wzorców, rozumienie języka naturalnego, podejmowanie decyzji, uczenie się i planowanie. Głównym celem AI jest stworzenie systemów zdolnych do myślenia i podejmowania decyzji w sposób przypominający ludzki.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(rag(model, \"Czym jest sztuczna inteligencja?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5bfb182-64b5-4a5b-b8cc-3ae90e18359f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uczenie głębokie to zaawansowany rodzaj uczenia maszynowego, wykorzystujący sieci algorytmów inspirowanych strukturą mózgu, zwane sieciami neuronowymi.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(rag(model, \"Czym jest uczenie głębokie?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "58a5ecbb-4cbc-4650-b873-a8d7b7190f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Przepraszam, ta informacja nie znajduje się w tekście.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(rag(model, \"Jak zrobić naleśniki?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815923ca-1677-4d7d-9400-485515527f94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
